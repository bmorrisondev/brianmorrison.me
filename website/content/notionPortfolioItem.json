[{"id":"120e2133-e6bd-4338-8a0e-27d63ee8861f","notion_id":"120e2133-e6bd-4338-8a0e-27d63ee8861f","date":"2013-01-01T00:00:00.000Z","tags":["ASP.NET MVC","Bootstrap","C#","HTML/CSS","JavaScript","MySQL"],"excerpt":"A simple mobile website that displayed active alerts for the MSP I was working for.","title":"Mobile RMM Dashboard","slug":"mobile-rmm-dashboard","html":"<div><figure><img src=\"/img/n/mobile-rmm-dashboard/168f5c33-a3cb-4d23-9b94-a6fdc7ed041b-image.png\" /><figcaption></figcaption></figure><h2>The Problem</h2><p>The Remote Monitoring & Management (RMM) tool used by the company did not have an easy way of quickly viewing active alerts without launching the Windows desktop application. The tech lead wanted a way to simply view all active alerts, specifically from a mobile device.</p><h2>The Solution</h2><p>I designed a responsive web app using ASP.NET MVC that queried data from the MySQL database of the RMM tool and presented any active alerts on the page. JavaScript was used to keep the views up to date by refreshing the data in the background every 30 seconds. Authentication was handled by Active Directory, and once the credentials were issued, a cookie was used for persistent authentication.</p><h2>The Results</h2><p>Technicians no longer needed to continue to monitor their computers when on-call. It freed them up to spend more time doing the things they enjoyed when responsible for being the initial point of contact during outages or emergencies.</p></div>","cachedOn":1681394635},{"id":"eb681861-3884-4caf-8aed-e415f0a65150","notion_id":"eb681861-3884-4caf-8aed-e415f0a65150","date":"2021-01-01T00:00:00.000Z","tags":["Bot Development","Docker","NodeJS","Open Source"],"excerpt":"A custom Discord bot built for the fullstack.chat community.","title":"fsc.bot (Walter)","slug":"fscbot-walter","html":"<div><figure><img src=\"/img/n/fscbot-walter/b9d2a497-d377-462c-9eb0-e1d24bf6f69e-image.png\" /><figcaption></figcaption></figure><p>fsc.bot is the open source, custom Discord bot written for the <a href=\"http://fullstack.chat/\" target=\"_blank\">fullstack.chat</a> community.</p><h2>Features</h2><p>Here are some key features of the bot at the time of this writing.</p><h3>XP Tracking</h3><p>I wrote an XP tracking system to automatically determine which users can be considered ‘active’. I wanted to be able to provide perks to users who actively participated in the conversation, one of which is a regular giveaway of some kind.</p><p>Each time a user sends a message to the server, they are granted XP. Continued activity will increase an XP multiplier to help active users gain XP faster. Likewise, inactive users are penalized after several days and XP will slowly start being decremented from their totals. This keeps a balance where active users, regardless of how recently they join, gain the biggest benefits of being on the server.</p><h3>Portfolios</h3><p>The <code>portfolio</code> command allows members to register their portfolio URL with the bot, and the bot will update a single message on the server with all member portfolios. It provides a nice way for everyone to connect with each other and see who works on what.</p><figure><img src=\"/img/n/fscbot-walter/e2d916ea-d3c6-459b-987a-74b4350895f6-image.png\" /><figcaption></figcaption></figure><h3>Help System</h3><p>I also built in a help system. When a user issues the command <code>!fsc help</code>, a private message will be sent to them with details on each command available to them. They can also append the name of the command if they are only interested in help for one command.</p><figure><img src=\"/img/n/fscbot-walter/e862d1df-0779-45de-bb24-a1f7c9d2c4a8-image.png\" /><figcaption></figcaption></figure><h2>Behind the Scenes</h2><p>Below are a few design considerations I took when building the bot.</p><h3>Dynamic Command Parsing</h3><p>When the bot starts, it will scan the <strong>commands</strong> directory and parse in JavaScript files dynamically provided the following fields are exported;</p><ul><li><code>command</code> - The actual command that is registered with the file.</li><li><code>fn</code> - The function that the message object is passed into for processing.</li></ul><p>The following fields are optional, but provide additional functionality;</p><ul><li><code>helpText</code> - The YAML formatted text that is associated with that command. I chose YAML because I could use syntax highlighting when DMing the user.</li><li><code>isEnabled</code> - If set to false, the init process will skip parsing that command file.</li><li><code>shouldCleanup</code> - If set to true, the bot will remove the message that the user sends to trigger the bot. This helps keep the server clean.</li></ul><h3>Security</h3><p>Certain commands should only be issues by moderators, so I implemented a method that checks the requester and confirms they are part of the moderator role before executing the code. If the user is not in the role, they will receive a DM from the bot stating they don't have access to that command.</p><h3>Scheduled Scripts</h3><p>A scheduler system exists within the bot where certain JavaScript functions are executed on a schedule based on cron expressions. One example of this is the XP system.</p><p>Gaining XP is event driven, specifically when the user sends a message to the server. But penalizing users for inactivity cannot be handled in the same way, so there is a script that runs every 24 hours to decrement XP from inactive users.</p><h3>Data Caching</h3><p>During the initialization process for the bot, all data is pulled into memory for quick access. Changes to each dataset are committed back to the database on an individual basis.</p><h2>Infrastructure</h2><p>Code for the bot is hosted on GitHub. The pipeline however managed by Azure DevOps. I inject environment variables & configuration flags into the build process and want to keep those private. The bot is run in a Docker container on my personal server at the moment, and data is stored in an Azure Storage Table.</p><p>Source for the bot can be found at <a href=\"https://github.com/bmorrisondev/fsc.bot\" target=\"_blank\">https://github.com/bmorrisondev/fsc.bot</a>.</p><p></p></div>","cachedOn":1681394637},{"id":"36afe066-973d-4e3e-9c86-2151e8af81e4","notion_id":"36afe066-973d-4e3e-9c86-2151e8af81e4","date":"2011-01-01T00:00:00.000Z","tags":["Android SDK","Eclipse","Java"],"excerpt":"An Android application I built and launched into the Play store in order to learn professional development.","title":"Contact Notebooks","slug":"contact-notebooks","html":"<div><figure><img src=\"/img/n/contact-notebooks/70f3cce4-627f-4b80-8473-bd1cddbd3db0-9d0bca83-9b32-417b-b651-cdded5319913.png\" /><figcaption></figcaption></figure><p>Contact Notebooks was my first attempt at creating a mobile app, and using a serious language. It was a fairly simple Android app that hooked into the phones contact database and created a \"notebook\" for each contact. The main view was simply a list of contacts and when the user clicked on one, it would bring them to a sub-view that contained a list of notes. Users were able to add a note for a contact to reference later. Notes were added via a custom dialog view. Once the app was in a functional state, it was published into the Android App Market (now the Google Play Store).</p><p>While the app was nothing spectacular, it was my first attempt building app and bringing a product to market. I essentially decided one day I wanted to build a mobile app since I could see it was definitely the future of computing. Once my normal responsibilities were done for the day, I would go to a local 24 hour chain restaurant and study/code until the early hours of the morning. I would generally start around 10pm and, depending on my schedule the following day, I'd stay as late (?) as 7am if I was on a roll. Since I was running my own business at the time, I had the luxury of creating my own schedule which helped tremendously.</p></div>","cachedOn":1681394640},{"id":"fd4782ee-c24c-4195-b254-32827291bb0d","notion_id":"fd4782ee-c24c-4195-b254-32827291bb0d","date":"2014-01-01T00:00:00.000Z","tags":["C#","WPF/XAML","Zebra Printers","ODBC"],"excerpt":"A label printing application that hooked into Sage100 for a manufacturer in Chicago.","title":"Sage 100 Label Printer","slug":"sage-100-label-printer","html":"<div><p>While working at Systech, we had a client that was mass upgrading their systems from Windows XP to Windows 8. We discovered that at some point many years ago, a custom label printing app was built that integrated into their Sage ERP to allow them to quickly print container labels for orders after they were packed and ready to ship. These labels listed what the contents of the package were and was critical to their operations. The app was built in 16 bit code and would not run on Windows 8, so I was asked to build a new version of the application and replicate the functionality.</p><p>I worked directly with the owner of the company, as well as warehouse personnel, to built a list of features that needed to be implemented in the new version. It was a simple app that allowed a user to click into a terminal field, enter an order number, list the order items, and print a label with those items on it. The app integrated directly into the ERP database (which I believe was Pervasive SQL) to pull the data to display.</p><p>I implemented the same functionality in the upgraded version of the app, but also enhanced it by redesigning the labels to work with industrial Zebra thermal printers, and implemented a number of keybindings as shortcuts to quickly navigate the app. This not only allowed them to continue with their upgrade, but also increased the speed at which they were able to generate labels.</p></div>","cachedOn":1681394641},{"id":"776703c6-aac9-428d-9a56-f6e866c3bc14","notion_id":"776703c6-aac9-428d-9a56-f6e866c3bc14","date":"2014-01-01T00:00:00.000Z","tags":["C#","SQL Server","SSIS","Reporting","WPF/XAML"],"excerpt":"A desktop application built to streamline the quoting process for a roofing company.","title":"Quoting Pro","slug":"quoting-pro","html":"<div><h2>The Problem</h2><p>A roofing company required a very customized quote to be created for their clients. The original quoting process was executed as follows;</p><ul><li>An admin employee would utilize an Access application that was built in-house to lookup customers and create a quote. While the customer was stored in the database, all of the line items on the quote were manually inputted. They would then use an Access Report to print a professional looking quote. They would then scan the quote to their email in PDF format.</li><li>The employee would generate a specifications document using a PDF form and export that to be part of the overall quote.</li><li>Any number of CAD drawings and text files provided by the engineers would then need to be combined along with the line item quote and specs document into a single PDF which would be the final quote.</li><li>Finally, this quote would be sent to the client via email.</li></ul><p>This overall process was very manual and time consuming. Much of this process was automated in a series of phases detailed below.</p><h2>The Solution</h2><h3>Phase 1</h3><p>The first thing we did is prevent the user from having to print and combine the PDFs. We setup the Access Report with a small amount of VBA to automate the Print to PDF process. An Accessory Command Line App was built using an open source PDF library to combine the PDFs. The VBA would call the accessory app, which would prompt the user to select any number of files to combine. Once the files were combined, the would be attached to an email template through Outlook. The specs document was still required to be completed manually at this point.</p><p>The result of Phase 1 was a dramatic decrease of time spent in the quoting process. One issue that remained however is that the Access App was not originally designed to be used by multiple users. An attempt was made to break out the backend database, but it was unsuccessful.</p><h3>Phase 2</h3><p>The next phase of this project involved a custom designed WPF application that was tailored to their specific quoting process. SQL Server was used as the back end datastore. In order to preserve historical data, I created an SSIS package which imported data from the existing Access database and mapped it into a table structure I designed for the application.</p><p>Custom modules were created to track customers, contacts, product info, quotes, and spec documents. During the quoting process, the application would prompt the user to import any number of CAD drawings or text files submitted by the engineers. The application would use the open source PDF library to combine all documents into a single PDF and attach it to an email.</p><p>I also designed a process by which the user could export a list of products from their primary Line of Business app into an Excel documents, and import it into the Quoting App so as to prevent manual entry. (There was no public API for the LOB App.)</p><p>The result of Phase 2 was a another dramatic increase in productivity as the application was more streamlined and more intuitively designed. I also designed it with multiple users in mind, which eliminated the restriction that the app could only by used by one person at a time.</p></div>","cachedOn":1681394642},{"id":"5ca1e211-c17a-4878-9af2-b0fd3a645e13","notion_id":"5ca1e211-c17a-4878-9af2-b0fd3a645e13","date":"2013-06-01T00:00:00.000Z","tags":["C#","JavaScript","SharePoint SDK"],"excerpt":"A SharePoint extension that saved over 100k of billable work hours for an MSP I was employed with.","title":"SharePoint Backup Tracker","slug":"sharepoint-backup-tracker","html":"<div><figure><img src=\"/img/n/sharepoint-backup-tracker/7ef1daeb-568d-48ab-a35e-a17596dfd844-14895539-3a65-48c4-b205-da438cc5eb68.png\" /><figcaption></figcaption></figure><h2>The Problem</h2><p>I built this while working for a Managed Services Provider. As part of our contractual agreement with our customers, we checked all customers' backups daily to ensure they were being successfully completed. Every day, an administrator would spend upwards of 3 hours manually inputting a list of backup sets that needed to be reviewed by a technician. Once the list was populated, the admin would notify the designated technician that the list was ready to be reviewed. The tech would then manually review the backups for all customers using a number of methods including; remoting directly to a server, checking email reports, or checking a cloud based panel.</p><p>Each backup to be checked was a SharePoint list item. This required the technician to open the item, change the necessary attributes, and save the item to return to the list. Any backup that had failed would require the tech to manually create a ticket in the ticketing system and add the ticket number as a reference to the backup item in SharePoint.</p><p>This entire process took a minimum of 8 hours of labor due to the volume of backups that needed to be tracked and the speed at which the process flowed.</p><h2>The Solution</h2><p>There were several improvements put in place to solve this problem.</p><h3>The Template List</h3><p>The first thing that was change is a new list that defined the following attributes;</p><ul><li>The customer name</li><li>The backup name</li><li>The set number (used to assign a set to a technician)</li><li>The method required for checking the backup</li><li>The days which the backup needed to be checked (as checkboxes)</li></ul><p>This list served as the template for backups to be checked and allowed us to create a programmatic way to populate the daily list of backups to check.</p><h3>The Daily List</h3><p>This list was used by the technicians to know which backups needed to be checked, and which were assigned to them. Every day, the techs would log into the SharePoint site and complete the list that was assigned to them. Each item would be populated as follows;</p><ul><li>The customer name</li><li>The backup name</li><li>The set number</li><li>The methods required for checking the backup</li><li>The status of the backup (success/failure)</li><li>Notes that pertain to the failure status</li></ul><h3>The Technician List</h3><p>A third list was created with the tech name and the backup set assigned as referenced in the Daily List. This list was used as a reference so that each tech would know which set to check for that day.</p><h3>Timer Jobs</h3><p>To automate the process of creating the daily list of backups to check, I created a SharePoint Timer Job that ran every day at 3am and referenced items from the template list to populate the daily list. This completely automate the manual entry required in the original process and free up the time of the admin.</p><p>Per the requirements of the lead tech, each tech needed to be assigned a different set on a daily basis. A second Timer Job was created for the purpose of randomizing the sets assigned based on the number of technicians in the Technician List.</p><h3>JavaScript Multiple Close Button</h3><p>To resolve the issue with closing multiple Daily List items simultaneously, I created a custom JavaScript button that would update the Status field in any number of selected backup items. This button was added to the ribbon of SharePoint.</p><h2>The Results</h2><p>At this point, a process that generally took a minimum of 8 hours between the admin and a single tech was distributed among 4-5 technicians taking a total of 30 minutes of total time. This ultimately saved the company over $100,000 of billable hours that the tech could use to resolve customer issues as opposed to handling this administrative task.</p><ul><li>* <em>This project was ultimately rolled into a multi-tiered custom application we named Unify.</em></li></div>","cachedOn":1681394643},{"id":"419a33e0-98ed-44b3-a817-62ab52e5fcdb","notion_id":"419a33e0-98ed-44b3-a817-62ab52e5fcdb","date":"2021-01-01T00:00:00.000Z","tags":["Gridsome","Netlify","Open Source","VueJS"],"excerpt":"A custom website built for my Discord server.","title":"fullstack.chat website","slug":"fullstackchat-website","html":"<div><p></p><figure><img src=\"/img/n/fullstackchat-website/11e53a72-fd65-4f57-b3dc-144deb62fb4e-812dca98-8ee2-42fe-854d-a525166e5102.png\" /><figcaption></figcaption></figure><h2>Design</h2><p>As most sites, I started with a basic color scheme using coolors.co. Below are the colors that I settled on with input from the rest of the community.</p><p>I also utilized <a href=\"http://fontpairs.co/\" target=\"_blank\">fontpairs.co</a> to select a set of fonts that were tested to work well together. Lato is used for the headers and Open Sans for body type.</p><h2>Implementation</h2><p>The site is built with Gridsome, a static site generator using VueJS. Much of the code on the landing page is hand written, but the Rules section references a markdown file that is stored along with the project. This makes for quick updating of the rules without modifying the code.</p><p>Bootstrap was used to quickly get some responsiveness, but no other components from that framework were used.</p><p>The site is hosted on Netlify and their automated deployment tools are leveraged by connecting the site to the GitHub repository.</p><p>The code for the site is open source and can be found at <a href=\"https://github.com/bmorrisondev/fullstack.chat\" target=\"_blank\">https://github.com/bmorrisondev/fullstack.chat</a>.</p></div>","cachedOn":1681394644},{"id":"6dc2d67c-0bf5-4b7d-aa19-80bafa7599fe","notion_id":"6dc2d67c-0bf5-4b7d-aa19-80bafa7599fe","date":"2014-01-01T00:00:00.000Z","tags":["C#","SQL Server","Reporting","WCF","WPF/XAML","XML API"],"excerpt":"The internal tool designed to consolidate many disparate functions into a single application for an MSP I worked for.","title":"Project Unify","slug":"project-unify","html":"<div><h2>The Problem</h2><p>Unify was built to become a platform to bridge multiple, manual processes into a single point of access to manage those processes in an automated fashion. That said, this project is broken into multiple sections. This section documents the general design of the overall solution.</p><h2>Front End Application</h2><p>The front end of Unify was a Windows WPF application that utilized the Telerik WPF controls. The overall feel of the desktop application was designed to mimic other Microsoft Offices with a contextually aware ribbon and an Outlook-style navigation on the left. I used ClickOnce for distributing any updates to the desktop application, and the installed client would check for updates upon launch.</p><h2>Reporting</h2><p>Report templates were stored in the Front End Application. This allowed the user to batch export reports per customer to a location on their computer. They would select the reports they would want to export, select a folder on their computer, and the application would export the requested reports to PDFs. These reports were previously built manually using Excel spreadsheets with connections to various datastores.</p><h2>Back End Service</h2><p>I created a WCF Service that contained all of the business logic of Unify. This included executing certain actions (specifically for the accessory applications specified below) and any CRUD operations. I designed and built out a SQL Server database to store any structured data. Unstructured data such as reports and firewall backups were stored on the file system of the server with a direct link stored in the database for quick reference. The WCF service was presented to the client using IIS. Updated were performed by a simple Deploy to a network path that was mapped through IIS.</p><h2>Accessory Applications</h2><p>While the business logic was stored in the WCF Service, I created several Windows Command Line Applications that could be used to execute those tasks. These were used in conjunction with the Windows Task Scheduler to execute any operations required on a regular basis.</p><h2>Authentication</h2><p>Authentication was tied into Active Directory via the WCF service. Usernames & passwords were hashed and sent to the WCF service using HTTPS where they were validated against Active Directory. The front end also checked the currently logged on Windows user and if it were a member of the domain, it would authenticate automatically.</p><h2>Backup Tracker</h2><p>The first problem that Unify solved was to further streamline the Backup Tracking process described in <a href=\"notion://www.notion.so/portfolio/sharepoint-backup-tracker\" target=\"_blank\">SharePoint Backup Tracker</a>. Additional functionality was included to further streamline the process of quarterly customer reporting.</p><p>One improvement gained by implementing a custom application was a significant performance gain. The responsiveness of a native application exceeded accessing SharePoint through a web app. This allowed the technicians to complete their backup review in under 15 minutes under most circumstances.</p><p>A second improvement that was made was deeper integration into our ticketing system, ConnectWise. I was able to directly access a list of customers to reference within Unify, and this was used to automatically create tickets based on the reported status of a backup. Upon creating a ticket using ConnectWise’s XML API, the API would respond with the generated ticket number. That was stored along with the backup for reporting purposes. Further integration as it pertains to the firewall backups is detailed below.</p><h2>Firewall Backup Automation</h2><p>The second problem was the automation of firewall backups. In order to preserve the configuration of our managed customers’ Fortinet firewalls, technicians were required to manually offload monthly backups. This process took between 4-5 hours to perform on a monthly basis.</p><p>Using the Renci.SSH library, logic was built into the Unify Service to send commands to the firewalls of our clients and issue commands. The firewalls had a command built in to send a configuration backup to an FTP server. We setup a dedicated FTP server for these backups and used these commands to execute weekly backups of these firewalls. The success or failures of the backup process, along with a direct link to the backup for quick access, was stored in the database. This was used for quarterly reporting between the client executives and the customer.</p><p>Considering the access URL and credentials were stored in the CMDB of ConnectWise, and the onboarding process had a task to add this information, it was decided that this would be the best source of truth for creating the firewall backups. We used the ConnectWise API to pull this data in at the start of the routine. This allowed integration of the firewall backups procedure into the existing business processes, reducing the possibility of error or oversight.</p><h2>Warranty Reporting</h2><p>The final component that was completed for Unify was the process of automatically storing warranty information on all endpoints covered under contract. As part of the onboarding process, a Remote Monitoring & Management (RMM) tool was deployed to endpoints. This allowed for remote support and management, but also gathered data regarding the system, notably the system serial number.</p><p>I utilized these values stored in the MySQL database of the RMM server, in conjunction with a third party warranty lookup service, to parse the warranty info of all 1000+ managed endpoints into Unify for reporting and tracking. A custom report was created to allow client executives to export reports for the quarterly review and plan with the customer to ensure their hardware was under warranty and compliance.</p></div>","cachedOn":1681394646},{"id":"89791dee-d1aa-4683-a66e-a6abeb2cbfbe","notion_id":"89791dee-d1aa-4683-a66e-a6abeb2cbfbe","date":"2020-09-01T00:00:00.000Z","tags":["After Effects","Premier Pro","Python","YouTube"],"excerpt":"A YouTube video I created for the team at MessengerX.io.","title":"MessengerX.io Python Bot YouTube Video","slug":"messengerxio-python-bot-youtube-video","html":"<div><figure><img src=\"/img/n/messengerxio-python-bot-youtube-video/4f3f329c-4447-47ea-897a-860c09fbe2c1-img_62b5f592f0dda.jpg\" /><figcaption></figcaption></figure><div class=\"callout\"><div class=\"callout-icon\">📽️</div><div class=\"callout-content\">You can view the video here: <a href=\"https://www.youtube.com/watch?v=nIp7QRNci2U\" target=\"_blank\">https://www.youtube.com/watch?v=nIp7QRNci2U</a></div></div><p><a href=\"http://messengerx.io/\" target=\"_blank\">MessengerX.io</a> is a developer marketplace for everyday chat apps also known as chatbots.</p><p>When I was in the process of releasing my <a href=\"https://www.youtube.com/playlist?list=PLwpjN-4DtVRbaxSY_dM0Ag0Y5SG6-RLu-\" target=\"_blank\">Coding Discord Bots</a> series on YouTube, I was advertising on a NodeJS Facebook group. The owner of the <a href=\"http://messengerx.io/\" target=\"_blank\">MessengerX.io</a> reached out to me and expressed interest in producing a video for their YouTube channel.</p><p>He sent me an existing article that someone on their internal team created outlining how to create a simple python bot using RASA, an open source ML framework. This was the foundation for the video I produced. I worked with their internal team over the course of a few weeks to better understand their platform, and to make revisions to the content as needed.</p><p>The video was recorded using Streamlabs OBS and edited using Adobe Premiere Pro. The intro animation was created using Adobe After Effects, and Photoshop was used to create the outro card. I leveraged Envato Elements for the music used in the intro & outro.</p><p><a href=\"http://messengerx.io/\" target=\"_blank\">MessengerX.io</a> is a developer marketplace for everyday chat apps also known as chatbots.</p><p>When I was in the process of releasing my <a href=\"https://www.youtube.com/playlist?list=PLwpjN-4DtVRbaxSY_dM0Ag0Y5SG6-RLu-\" target=\"_blank\">Coding Discord Bots</a> series on YouTube, I was advertising on a NodeJS Facebook group. The owner of the <a href=\"http://messengerx.io/\" target=\"_blank\">MessengerX.io</a> reached out to me and expressed interest in producing a video for their YouTube channel.</p><p>He sent me an existing article that someone on their internal team created outlining how to create a simple python bot using RASA, an open source ML framework. This was the foundation for the video I produced. I worked with their internal team over the course of a few weeks to better understand their platform, and to make revisions to the content as needed.</p><p>The video was recorded using Streamlabs OBS and edited using Adobe Premiere Pro. The intro animation was created using Adobe After Effects, and Photoshop was used to create the outro card. I leveraged Envato Elements for the music used in the intro & outro.</p><p></p></div>","cachedOn":1681394649},{"id":"b9ec4188-4abf-49b9-9a51-ea7ceaa442c6","notion_id":"b9ec4188-4abf-49b9-9a51-ea7ceaa442c6","date":"2010-01-01T00:00:00.000Z","tags":["MS Access","VBA"],"excerpt":"A Microsoft Access application that streamlined the yearly renewals for a landscaping client of mine.","title":"Proposals App","slug":"proposals-app","html":"<div><h2>The Problem</h2><p>A client of mine in the landscaping industry had a manual process defined to send out maintenance contract renewals every year. There were over 1000 customers that required the office employees to manually reference last year’s contracts and write out a proposal to mail to a customer. There were three possible forms that needed to be filled out for each of their customers, with certain customers requiring multiple forms to be completed. My client researched multiple canned solutions, but nothing existed that could be integrated into their workflow at the time.</p><h2>The Solution</h2><p>I worked with the client to build an application using Microsoft Access that automated much of this process. The three forms required for their proposals were digitized into Access Forms. The employees could fill out the form, save the data, and use a custom report that mirrored the previous physical form to perform one of the following operations;</p><ul><li>Print the document to mail if required. Each user is also able to customize the printer they wanted to send to.</li><li>Generate a PDF and attach to an Outlook Email using VBA integration. An email template was also configured to pre-populate the recipient and a standard message into the body. The message was then presented to the user to add or remove detail as necessary.</li></ul><p>The first year this was implemented required the employees to manually enter the information referenced from the previous year’s hand written proposals. Copy and Archive were added so that creating new proposals that referenced the previous year required a single button click, and historical data was kept for reference.</p><p>The Access Application was broken into a front end and back end so multiple users could use it at the same time.</p><h2>The Result</h2><p>We were able to take a process that originally required several weeks of manual work and reduce it to several days.</p></div>","cachedOn":1681394650}]