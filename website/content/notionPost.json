[{"id":"cffcce50-9f5a-4d43-a3db-b7aef28aee1f","notion_id":"cffcce50-9f5a-4d43-a3db-b7aef28aee1f","status":"Published","slug":"migrating-from-google-analytics-to-posthog","relation_series":[],"publishOn":"2022-09-14T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Migrating from Google Analytics to PostHog","html":"<div><p>A few weeks ago, Google Analytics broke on me for GuardianForge. Despite my best attempts, I haven’t been able to get it working even though I’ve followed every possible suggestion out there. So I asked around and someone on the <a href=\"https://www.learnbuildteach.com/\" target=\"_blank\">Learn Build Teach discord</a> recommended an alternative. That got me thinking “I bet there are a bunch of these services out there that do the same thing”. After exploring a few of the more cost-conscious options, I settled with PostHog and so far am pretty blown away by it.</p><h2>Installation and setup</h2><p>Installing the tool and setting it up was very straightforward. In my repo, I simply had to run the following command to install the library:</p><pre class=\"language-bash\"><code>npm install posthog-js</code></pre><p>Then I had to initialize the library with the following code somewhere in the repo:</p><pre class=\"language-javascript\"><code>import posthog from 'posthog-js'\n\nposthog.init(\"POSTHOG_KEY\", { api_host: 'https://app.posthog.com' };</code></pre><p>Now in GuardianForge, I initialize everything in a Context file instead of in the default <code>App.tsx</code>. I do this so I can pull down a <code>config.json</code> file from S3, which loads all of the public keys and configuration variables. The reason I do that is so my Azure Pipeline that's used to build and deploy the code can perform a transformation on that file before it goes out, with the values updated for the environment the code is going to (QA or Production).</p><p>Here is how that code looks, or you can view the source on the <a href=\"https://github.com/GuardianForge/guardianforge.net/blob/1c1ef714480dca49e133b199d111000c9503eed7/frontend/public/src/contexts/GlobalContext.tsx#L144\" target=\"_blank\">GitHub repo</a>.</p><pre class=\"language-javascript\"><code>posthog.init(config.posthogId, {\n  api_host: 'https://app.posthog.com',\n  capture_pageview: false,\n  autocapture: false\n})</code></pre><p>You'll also notice that I have two additional configs set when initializing PostHog.</p><p>The <code>capture_pageview</code> being set to false prevents the library from trying to automatically detect when a page changes. This is disabled because GuardianForge is a React Single Page App, so there is technically no loading of pages when links are clicked, the JS just renders the updated DOM. To overcome this, I have a new <code><LocationHandler></code> component that is in the <a href=\"https://github.com/GuardianForge/guardianforge.net/blob/master/frontend/public/src/Router.tsx\" target=\"_blank\">Router.tsx file</a> that can manually tell PostHog when a user navigates to another page.</p><pre class=\"language-javascript\"><code>function LocationHandler() {\n  const { isInitDone } = useContext(GlobalContext)\n  const location = useLocation()\n  const [curr, setCurr] = useState(\"\")\n\n  useEffect(() =&gt; {\n    const page_path = location.pathname + location.search\n    if(isInitDone && curr !== page_path) {\n      setCurr(page_path)\n      posthog.capture('$pageview')\n    }\n  }, [location])\n  return (&lt;&gt;&lt;/&gt;)\n}</code></pre><p>Additionally, with <code>autocapture</code> on, it was simply too noisy for what I needed it for. PostHog was trying to track every little detail the user was doing, such as clicking on buttons or typing into text fields. I don't need that level of detail, so that was turned off. Here's an example of what that looked like:</p><figure><img src=\"/img/n/migrating-from-google-analytics-to-posthog/5682a817-756b-4fda-a37d-f197cb21c2bd-image.png\" /><figcaption></figcaption></figure><h2>Things I like</h2><p>First and foremost, the default dashboard is something I ACTUALLY understand! Don't get me wrong, I think Google Analytics is a powerful tool when you have the correct understanding of the setup and terminology. I predominantly want to focus on building GuardianForge over learning how to slice and dice metrics, at least for now. Here is an example of the dashboard:</p><figure><img src=\"/img/n/migrating-from-google-analytics-to-posthog/befb0d38-3f43-40e7-ade8-7fe83982edb3-image.jpeg\" /><figcaption></figcaption></figure><p>Another thing I like is the ability to identify users with your own values. I'm not doing it in any kind of creepy way, but only for my own knowledge to match Destiny users to metrics on how they use the app. I'm hoping at some point I can identify power users to work more closely with and understand what features would be best to implement. Here is the code I'm using to tie users into PostHog:</p><pre class=\"language-javascript\"><code>posthog.identify(authData.membership_id)</code></pre><p>And then I can see them in the PostHog app and see what features they are using. This is my record in the QA environment, and the number is actually my unique ID in Destiny (nothing you couldn't get by searching on the forums and analyzing network traffic):</p><figure><img src=\"/img/n/migrating-from-google-analytics-to-posthog/7f27aece-345d-428b-8b4f-d63a5900df8b-image.jpeg\" /><figcaption></figcaption></figure><h2>Things I plan to explore more</h2><p>Since PostHog makes it easy to create custom events with custom data, I plan to expand on this to see what kinds of builds people are making without having to dive into the database. I also want to explore Annotations more, which are ways that I can tie external events to PostHog, and see if those actions (say, sharing a build on Twitter) have a positive effect on the usage of GuardianForge.</p></div>","featuredImage":"/img/n/migrating-from-google-analytics-to-posthog/520c2e24-ba02-4f9d-9e0a-c818aad1f8be-migrating-from-ga-to-posthog.jpg","icon":"/img/n/migrating-from-google-analytics-to-posthog/0e058da6-1b29-425a-adc1-c73f0c748d8f-84f60f9b33de4b4ea4b29bf30a0c7cc8.png.webp","excerpt":"A few weeks ago, Google Analytics broke on me for GuardianForge. Despite my best attempts, I haven’t been able to get...","cachedOn":1681394427},{"id":"419d73b5-63a7-4e00-9385-374c50973a7d","notion_id":"419d73b5-63a7-4e00-9385-374c50973a7d","status":"Published","slug":"querying-a-graphql-service-with-go","relation_series":[],"publishOn":"2022-09-12T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/go-graphql-client-demo","seriesOrder":null,"youTubeURL":"https://youtu.be/4UFDAEcZPgo","title":"Querying a GraphQL service with Go","html":"<div><p>So over this past weekend, I decided to build a sync client between my private WordPress instance that powers the content on this site, and my other hosted blogs on <a href=\"https://brianmmdev.hashnode.dev/\" target=\"_blank\">Hashnode</a> and <a href=\"https://dev.to/brianmmdev\" target=\"_blank\">dev.to</a>. Instead of using the standard REST API to get the data, I figure why not try to hit the GraphQL endpoint. Here are some of my findings.</p><p>For the demo code in this article, I’ll be using <a href=\"https://api.hashnode.com/graphql#\" target=\"_blank\">Hashnodes GraphQL API</a> since it’s open and pretty simple to use.</p><h2>Setting up the project</h2><p>I’m going to start a new project by running the following commands in the VSCode terminal. This will initialize the project, install the necessary module, and create the main.go file.</p><pre class=\"language-bash\"><code>go mod init go-graphql-demo\ngo get github.com/machinebox/graphql\ntouch main.go</code></pre><p>Next, I’ll update the main.go file with the following. This sets up the <code>main</code> func and creates the GraphQL client necessary to pull data.</p><pre class=\"language-go\"><code>package main\n\nimport \"github.com/machinebox/graphql\"\n\nfunc main() {\n    client := graphql.NewClient(\"https://api.hashnode.com\")\n}\n</code></pre><h2>Querying data</h2><p>Before we can pull data down, we’ll need to create a struct to hold the data. Running the following query in the Hashnode GraphQL UI shows us what the data looks like.</p><figure><img src=\"/img/n/querying-a-graphql-service-with-go/d54e387a-50fd-4150-981d-1e21ab48a591-image.png\" /><figcaption></figcaption></figure><p>I'll add this struct to the bottom of the file to hold the data. I'm defining the child structs inline for the sake of the demo, but creating them separately will do the same and would probably result in cleaner code.</p><pre class=\"language-go\"><code>type QueryUserResponse struct {\n    User struct {\n        Publication struct {\n            Posts []struct {\n                Id        stringjson:\"_id\"\n                Title     string\n                dateAdded string\n            }\n        }\n    }\n}\n</code></pre><p>Now let’s create a separate func to run the query. Note how you need to pass a reference of the struct into the <code>Run</code> method, similar to how you would unmarshal JSON. I’ll iterate through the results and log them out as well, just to see it working.</p><pre class=\"language-go\"><code>func queryData(client *graphql.Client) {\n    query := `\n    {\n        user(username:\"brianmmdev\") {\n            publication {\n                posts {\n                    _id\n                    title\n                    dateAdded\n                }\n            }\n        }\n    }\n    `\n    request := graphql.NewRequest(query)\n    var response QueryUserResponse\n    err := client.Run(context.Background(), request, &response)\n    if err != nil {\n        panic(err)\n    }\n\n    for _, el := range response.User.Publication.Posts {\n        log.Println(el.Title)\n    }\n}</code></pre><p>I updated the <code>main</code> method to call <code>queryData</code>. Running the code results in the following from the console.</p><figure><img src=\"/img/n/querying-a-graphql-service-with-go/c41270e3-a9ab-47de-baa6-a5c5ea24325f-image.png\" /><figcaption></figcaption></figure><p>You can find the source code for this post at <a href=\"https://github.com/bmorrisondev/go-graphql-client-demo\" target=\"_blank\">https://github.com/bmorrisondev/go-graphql-client-demo</a>.</p></div>","featuredImage":"/img/n/querying-a-graphql-service-with-go/41c8204f-4f6d-4ba6-b751-254a76f63685-background-6.jpg","icon":"/img/n/querying-a-graphql-service-with-go/0f229fa7-b8ab-4bbd-b273-d53515e4e21b-264-2641184_111-kb-png-golang-logo.png","excerpt":"So over this past weekend, I decided to build a sync client between my private WordPress instance that powers the con...","cachedOn":1681394430},{"id":"9471430e-87ef-40cb-99b4-206d31a015e4","notion_id":"9471430e-87ef-40cb-99b4-206d31a015e4","status":"Published","slug":"building-guardianforge-the-subclass-diamond","relation_series":["22756350-048e-4423-8948-233370c99f6e"],"publishOn":"2022-08-21T00:00:00.000Z","codeURL":"https://github.com/GuardianForge/guardianforge.net/blob/317c62a5f62dc4fbdebaf9c286fbed25295f7267/frontend/public/src/components/V2SuperTree.tsx","seriesOrder":4,"youTubeURL":null,"title":"Building GuardianForge: The Subclass Diamond","html":"<div><p>It’s hard to believe that in a few short days, the old Diamond system of subclasses in Destiny will be no more. While it’s a welcome update for sure, It’s a tad bittersweet for me. Recreating the Diamond in GuardianForge was one of my favorite React components I’ve built so far. That said, I figure it’s a great time to walk through exactly how I did it.</p><h2>Background</h2><p>When Destiny 2 dropped, a new subclass system was introduced with preset perks and abilities based on the node that you selected. Each of the tree nodes had 4 abilities associated with them that slightly modified the way your Guardian played.</p><figure><img src=\"/img/n/building-guardianforge-the-subclass-diamond/3b2fbcf0-57e8-4262-a4f4-e482f3681481-image.jpeg\" /><figcaption></figcaption></figure><h2>Building the component</h2><p>What I wanted to do from the start was recreate this UI in Forge, but in a dynamic way. When I pull data from the Destiny 2 API, I restructure it in a way so the data makes more sense in the context of the app. Here is what is used by GuardianForge:</p><pre class=\"language-json\"><code>\"superConfig\": {\n    \"bgUrl\": \"https://www.bungie.net/common/destiny2_content/screenshots/2958378809.jpg\",\n    \"characterIconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/18271c5f07ee2b6fee985ae3d8c98f8a.png\",\n    \"grenade\": {\n        \"iconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/3b515e2617bffe787c0246452153c105.png\",\n        \"name\": \"Pulse Grenade\"\n    },\n    \"movement\": {\n        \"iconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/20908fe8f75e22c51e671e9bc59b7429.png\",\n        \"name\": \"Strafe Lift\"\n    },\n    \"specialty\": {\n        \"iconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/340974e10bb9d66b62ed2b0c16c13377.png\",\n        \"name\": \"Towering Barricade\"\n    },\n    \"super\": {\n        \"iconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/685c6dfff805f96371186527487e8440.png\",\n        \"name\": \"Fists of Havoc\"\n    },\n    \"damageType\": 2,\n    \"tree\": 3,\n    \"treeNodes\": [\n        {\n            \"iconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/e830b703ce61734c0c30d76d300feede.png\",\n            \"name\": \"Thundercrash\"\n        },\n        {\n            \"iconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/8dbf5577e35a541d34b8c1c4f3a85a67.png\",\n            \"name\": \"Ballistic Slam\"\n        },\n        {\n            \"iconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/42d362d80b5c68ff248481e158b46371.png\",\n            \"name\": \"Impact Conversion\"\n        },\n        {\n            \"iconUrl\": \"https://www.bungie.net/common/destiny2_content/icons/f29e153cf76cd27ef74d3e4bae112b1c.png\",\n            \"name\": \"Inertia Override\"\n        }\n    ],\n    \"treeTitle\": \"Code of the Missile\"\n}</code></pre><p>And here is how it is rendered in the component. Hovering over an icon in the diamond shows its name, so you can see the leftmost perk in the diamond correlates with the first element of the <code>treeNodes</code> array, with each element being rendered in the order displayed in the image below.</p><figure><img src=\"/img/n/building-guardianforge-the-subclass-diamond/4fab6929-4dc9-4d8d-950f-1ef4c573fbb0-image.png\" /><figcaption></figcaption></figure><p>What you are seeing above is a CSS grid that is rotated 45 degrees counter-clockwise. If I remove the CSS that applies the rotation, it looks like this. You can also see that I apply a 45-degree clockwise rotation to the images that are rendered inside each node of the diamond. This makes the properly vertical when the rotation is applied to the surrounding grid.</p><figure><img src=\"/img/n/building-guardianforge-the-subclass-diamond/50f2f6d6-052b-49f2-af94-b5a1a5de32ef-image.png\" /><figcaption></figcaption></figure><pre class=\"language-css\"><code>.tree-diamond {\n    transform: rotate(-45deg);\n    display: grid;\n    grid-template-columns: repeat(2, 1fr);\n    grid-template-rows: repeat(2, 1fr);\n    grid-column-gap: 0px;\n    grid-row-gap: 0px;\n    height: 200px;\n    width: 200px;\n    border: 4px solid rgba(150,150,150,0.7);\n}</code></pre><p>Each box inside the diamond is actually a flexbox with a bit of padding on it. Now one thing I wanted to do is use a thicker border to denote which subclass tree (top, middle, or bottom) is selected. This is why node 4 has a thicker border to the bottom and right of the flexbox.</p><figure><img src=\"/img/n/building-guardianforge-the-subclass-diamond/e2c6a057-09a3-43c6-a606-98a759ed3706-image.png\" /><figcaption></figcaption></figure><p>When the rotation is applied, notice how the box points to the right to show that the middle tree Striker subclass is applied. Here is another example, the bottom tree Sunbreaker class. Notice how the thicker border points downward.</p><figure><img src=\"/img/n/building-guardianforge-the-subclass-diamond/58376da7-7fb9-46db-a8eb-2bfc9be89cb0-image.png\" /><figcaption></figcaption></figure><p>So by applying all these techniques using the code below, the <code>V2SuperTree</code> component dynamically renders the correct element type, selected tree, and perks by using the data pulled from the Destiny 2 API.</p><pre class=\"language-javascript\"><code>function V2SuperTree(props: Props) {\n  const { className, tree, affinity, hideName, onClick, selected } = props\n\n  return (\n    &lt;Wrapper className={className}&gt;\n      {!hideName && &lt;span&gt;{tree.name}&lt;/span&gt;}\n      &lt;div className={tree-diamond ${onClick ? 'tree-diamond-clickable' : \"\"} ${selected ? \"tree-diamond-selected\" : \"\"}}\n        onClick={onClick ? () =&gt; onClick(tree) : undefined}&gt;\n        {tree.perks && tree.perks.map((p: any, idx: number) =&gt; (\n          &lt;div key={tree-${tree.pos}-${idx}} className={img-wrapper-outer ${tree.pos === idx ?img-wrapper-outer-${idx} : \"\"}}&gt;\n            &lt;div className={img-wrapper img-wrapper-${affinity}}&gt;\n              &lt;OverlayTrigger\n                placement=\"bottom\"\n                delay={{ show: 250, hide: 400 }}\n                overlay={&lt;Tooltip&gt;{ p.name }&lt;/Tooltip&gt;}&gt;\n                &lt;img src={p.icon.startsWith(\"http\") ? p.icon :https://www.bungie.net${p.icon}} /&gt;\n              &lt;/OverlayTrigger&gt;\n            &lt;/div&gt;\n          &lt;/div&gt;\n        ))}\n      &lt;/div&gt;\n    &lt;/Wrapper&gt;\n  )\n}\n</code></pre><p>If you want to review the source you can find it in the <a href=\"https://github.com/GuardianForge/guardianforge.net/blob/317c62a5f62dc4fbdebaf9c286fbed25295f7267/frontend/public/src/components/V2SuperTree.tsx\" target=\"_blank\">public GuardianForge repo</a>.</p></div>","featuredImage":"/img/n/building-guardianforge-the-subclass-diamond/c99eda76-5b42-44fe-943c-bfbfc51bfb11-background-5.jpg","excerpt":"It’s hard to believe that in a few short days, the old Diamond system of subclasses in Destiny will be no more. While...","cachedOn":1681394431},{"id":"83a101eb-ea61-4457-a293-bd09e304b47e","notion_id":"83a101eb-ea61-4457-a293-bd09e304b47e","status":"Published","slug":"use-cognito-to-secure-a-serverless-go-api","relation_series":["c737786e-805c-4c6a-b356-10f6db2451b4"],"publishOn":"2022-08-19T00:00:00.000Z","codeURL":null,"seriesOrder":4,"youTubeURL":"https://youtu.be/Qvx3HRBFIGs","title":"Use Cognito to Secure a Serverless Go API","html":"<div><p>In the <a href=\"https://brianmorrison.me/blog/create-an-aws-cognito-user-pool\" target=\"_blank\">previous entry of the series</a>, we took a break from writing code to take a dive into AWS Cognito, how it works, how to set it up, and why you’d want to. We’re going to utilize the User Pool created in that article to see how to secure our API using API Gateway Authorizers and Cognito.</p><div class=\"callout\"><div class=\"callout-icon\">🔗</div><div class=\"callout-content\">This is a direct continuation from the previous entries in the series. If you haven’t followed along, start at the first entry on <a href=\"https://brianmorrison.me/blog/your-first-aws-lambda-function-in-go\" target=\"_blank\">building a Go Lambda</a>.</div></div><h2>What is an Authorizer?</h2><p>An Authorizer in API gateway is a way for API Gateway to check that the request coming in is being sent by someone who is authorized. When a request hits an API Gateway route configured with an Authorizer, it will check with the Authorizer before passing the request to any code or upstream system. There are two types of Authorizers available to HTTP APIs in API Gateway:</p><ul><li>JWT – Uses an OpenID JWKS endpoint to validate the incoming tokens.</li><li>Lambda – Passes the token to a Lambda for custom validation.</li></ul><p>Once an Authorizer is properly set up, any requests to those routes will be denied if the provided token is missing or invalid.</p><h2>Gather info from Cognito</h2><p>Before we move forward, there are a few pieces of information we need to grab from the Cognito user pool that was created in the previous setting. Search for ‘Cognito’ in the AWS console search and select it from the list.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/a7ee998f-2d14-4f88-9d46-4bcf82772f46-image.png\" /><figcaption></figcaption></figure><p>Select the user pool from the list.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/cd9448f0-6c23-4d56-8518-b6894c968d38-image.png\" /><figcaption></figcaption></figure><p>First thing to grab is the User pool ID.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/22909552-c3fb-4d71-8a3f-25abe20929b9-image.png\" /><figcaption></figcaption></figure><p>Next, click on the “App integration” tab and grab the Domain.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/941198c0-f96e-44bd-8904-64a16ee68522-image.png\" /><figcaption></figcaption></figure><p>Lastly, scroll down and grab the Client ID under App clients and analytics.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/df0700a8-9d70-4501-933d-fc6311607be3-image.png\" /><figcaption></figcaption></figure><h2>Set up the Authorizer in API Gateway</h2><p>Open API Gateway using the global search on the AWS Console.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/f233e686-89cf-4442-921a-cdb25e2d6499-image.png\" /><figcaption></figcaption></figure><p>Open your API, I’m using the First Go API created in the <a href=\"https://brianmorrison.me/blog/using-go-with-api-gateway\" target=\"_blank\">previous entry</a>.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/fbc99db9-85d8-4d83-8a66-ce6c28fbc255-image.png\" /><figcaption></figcaption></figure><p>Select “Authorization” from the left, then select the “Manage authorizers” tab, and then click the “Create” button.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/8f97cf75-6982-44dc-8739-b08b9a1fff17-image.png\" /><figcaption></figcaption></figure><p>You’ll have the option of a JWT authorizer or a Lambda authorizer, select the “JWT” option.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/5b717242-6f74-4d6d-9686-57f0dba7398f-image.png\" /><figcaption></figcaption></figure><p>In the form below, give the <strong>Authorizer </strong>a name. For the <strong>Issuer URL</strong>, use the format <code>https://cognito-idp.<AWS_REGION>.amazonaws.com/<USER_POOL_ID></code></p><p>and replace the variables taken from Cognito in the previous section. For <strong>Audience</strong>, add the <strong>Client ID </strong>taken from Cognito. Click “<strong>Create” </strong>once the form is completed.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/37a51bac-e7bc-4b8b-b46e-f4837d6d8d4b-image.png\" /><figcaption></figcaption></figure><p>Now select the “<strong>Attach authorizers</strong>” to the routes tab, select the route you want to secure, select the <strong>CognitoAuthorizer </strong>from the drop-down and then “<strong>Attach authorizer</strong>”.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/acd1bcf2-d30e-4558-9438-d44188a38213-image.png\" /><figcaption></figcaption></figure><p>The form should update to reflect the Authorizer being attached.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/da13a421-ad4c-4790-b1f5-b2daed0e3828-image.png\" /><figcaption></figcaption></figure><h2>Test the function</h2><p>Now that you’ve added the Authorizer to the API Gateway route, we can re-test the function using the exact same parameters as last time.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/c101ceb8-9ac0-4a06-ae81-d197366debf8-image.png\" /><figcaption></figcaption></figure><p>As you can see, the response is returning <strong>Unauthorized </strong>now, which is exactly what we want. API Gateway is expecting a JWT, specifically one issued by the Cognito user pool, to be in the <code>Authorization</code>header. Since that is missing, the Lambda doesn’t even execute before API Gateway sends back the 401 response. Now let’s generate a JWT from the user pool and try the request again. Use the same login URL that we built in the previous entry to get your login form.</p><pre class=\"language-plain text\"><code>&lt;COGNITO_DOMAIN&gt;/oauth2/authorize?response_type=token&client_id=&lt;CLIENT_ID&gt;&redirect_uri=https://jwt.io</code></pre><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/877d8193-b65a-4ac4-8643-8f3efa00a706-image.jpeg\" /><figcaption></figcaption></figure><p>Now login and grab the token from <a href=\"http://jwt.io/\" target=\"_blank\">jwt.io</a>.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/53668bd6-f4c4-491d-a6b3-f60797058b5b-image.jpeg\" /><figcaption></figcaption></figure><p>Update the request in the http file and add the <code>Authorization</code>header like so:</p><pre class=\"language-plain text\"><code>post &lt;https://dd064th21l.execute-api.us-east-1.amazonaws.com/hello-world-lambda&gt;\nContent-Type: application/json\nAuthorization: Bearer eyJraWQiOiJHa1JoRVc0TWlJUSs3QmZMbGYwXC9sN1ZyUEJhMjJhXC81clpmdWhhQjNyTGc9IiwiYWxnIjoiUlMyNTYifQ.eyJhdF9oYXNoIjoiZlVMSlE4UTd2akNZV25HOHYzYTNBdyIsInN1YiI6IjFlOGYwMGM5LTczYTAtNDZmYy04MjEzLWY0ZDUyM2E5ZjMzNiIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV9vU2R2OXFSWHUiLCJjb2duaXRvOnVzZXJuYW1lIjoiMWU4ZjAwYzktNzNhMC00NmZjLTgyMTMtZjRkNTIzYTlmMzM2IiwiYXVkIjoiYjl1N3Vsa3V2Zjk5cjZuaTc3NGwwbmVmNyIsImV2ZW50X2lkIjoiNTQxNGQ0NWQtZDM3NC00NzA4LTkxNTYtYjk2YjQ5YzZlMmM0IiwidG9rZW5fdXNlIjoiaWQiLCJhdXRoX3RpbWUiOjE2NjAzMzI1NzcsImV4cCI6MTY2MDMzNjE3NywiaWF0IjoxNjYwMzMyNTc3LCJqdGkiOiJjZmQ0ZjI2Yy0zMmM1LTQxMzMtOThlZC00ZWQ4MjE2ZWZkOGMiLCJlbWFpbCI6ImJyaWFuQGJyaWFubW9ycmlzb24ubWUifQ.ymsJIG4xhytHe8ZA3CMOPMZceQa1v4nkTG3PRef10YKnYsuYec5XUQtEdXnRRBTl8DmKGXuKL5DYJS_IglQ3bXUKEPyy3bgLelHjQVe62CuZ0Tv59METKbvOjyMj_KcJlDjxL9fZy-enOpU6UtatMUbaV6ftNtv1aU4t98-V-YHWYW2yIj9bNQKbKlhgqNF62dKmOTDraVTA2MTc0K8BWPxeRIU6LS_7oLO7JKiJ4JjXLBB1U-2CmReEE9dllN-uiyg4BxE69typ1MQZbLzLFVKFi6TN6a-MmEJ7WHvk92F-WazYpEeeKq6Mg55k3U-deMQjHd3IwGxGuevUBObS3A\n\n{\n    \"firstName\": \"Brian\",\n    \"lastName\": \"Morrison\"\n}\n</code></pre><p>Now run the test again and it should respond as expected!</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/7a54a35f-e8bc-4931-99b6-cd60d712aede-image.png\" /><figcaption></figcaption></figure><h2>Handling the token in code</h2><p>Now that the API endpoint is secure, let’s actually extract some of the claims from the token so we can identify the user. Update the <code>handle</code>func in the <strong>main.go </strong>file to look like the following code:</p><pre class=\"language-go\"><code>func handler(request events.APIGatewayV2HTTPRequest) (events.APIGatewayProxyResponse, error) {\n    jbytes, err := json.Marshal(request)\n    if err != nil {\n        return events.APIGatewayProxyResponse{}, err\n    }\n    log.Println(string(jbytes))\n\n    var person Person\n    err = json.Unmarshal([]byte(request.Body), &person)\n    if err != nil {\n        return events.APIGatewayProxyResponse{}, err\n    }\n\n    msg := fmt.Sprintf(\"Hello %v %v\", *person.FirstName, *person.LastName)\n    responseBody := ResponseBody{\n        Message: &msg,\n    }\n    jbytes, err = json.Marshal(responseBody)\n    if err != nil {\n        return events.APIGatewayProxyResponse{}, err\n    }\n\n    response := events.APIGatewayProxyResponse{\n        StatusCode: 200,\n        Body:       string(jbytes),\n    }\n    return response, nil\n}\n</code></pre><p>Now build & zip up the code so we can upload a fresh copy to AWS.</p><pre class=\"language-bash\"><code>GOOS=linux GOARCH=amd64 go build -o ./dist/main .</code></pre><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/b8305adb-f5e8-4cc9-9387-c2241bf8967f-image.png\" /><figcaption></figcaption></figure><p>Now upload it to the Lambda function in AWS.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/b6a0604a-878e-45dd-8671-3f9147857d5c-image.png\" /><figcaption></figcaption></figure><p>Run the test again, you shouldn’t see any difference in the response. Even though there is no change in the response, we can actually check the output of the function on AWS CloudWatch. Search for ‘CloudWatch’ in the AWS console search and select it from the list.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/f294ca01-7815-4d55-9979-96d38ca5c06f-image.png\" /><figcaption></figcaption></figure><p>In the left nav, click <strong>“Logs” </strong>> <strong>“Log groups”</strong>, and select the proper log group from the list that matches the name of your Lambda function.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/3d353d3f-92a3-4d47-9d34-f4a1eb06ebb9-image.png\" /><figcaption></figcaption></figure><p>Select the latest Log stream from the list.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/5fcfa8e0-85a3-46cd-b320-74c592aa8fdc-image.png\" /><figcaption></figcaption></figure><p>Expand the entry that looks at the start of some JSON and you can see that the claims are automatically extracted into the <code>request</code>object from the Go code. From all this data, we can use <code>sub</code>to uniquely identify the user accessing the API.</p><figure><img src=\"/img/n/use-cognito-to-secure-a-serverless-go-api/743c8a4f-2967-4682-b791-920114e2d8bd-image.png\" /><figcaption></figcaption></figure><p>The <code>claims</code>object is a string map, so to pull the user ID, you can add the following snippet to the <code>handler</code>function.</p><pre class=\"language-plain text\"><code>claims := request.RequestContext.Authorizer.JWT.Claims\nlog.Println(claims[\"sub\"])</code></pre><h2>What’s next?</h2><p>In the next entry of the series, we’ll explore how we can use everything we’ve learned to build a simple Tasks API, complete with user authentication and a database to store info.</p></div>","featuredImage":"/img/n/use-cognito-to-secure-a-serverless-go-api/13324729-93c7-42ed-9535-3a29ad49f244-background-2-3.jpg","excerpt":"In the  previous entry of the series , we took a break from writing code to take a dive into AWS Cognito, how it work...","cachedOn":1681394433},{"id":"c0fd6014-4d20-4db5-85f1-fa7b9334064b","notion_id":"c0fd6014-4d20-4db5-85f1-fa7b9334064b","status":"Published","slug":"create-an-aws-cognito-user-pool","relation_series":["c737786e-805c-4c6a-b356-10f6db2451b4"],"publishOn":"2022-07-21T00:00:00.000Z","codeURL":null,"seriesOrder":3,"youTubeURL":"https://youtu.be/n3br_TzJW28","title":"Create an AWS Cognito User Pool","html":"<div><p>I recently started a series on building a serverless API with AWS & Go. When I started drafting this article, I intended for it to be part of that series since it is a prerequisite for properly demonstrating how Cognito, API Gateway, and Lambda can be the perfect combination for authenticating users. There was just a lot to configure up front so I figure this could benefit anyone looking to get an understanding of Cognito & OAuth in general.</p><h2>What is OAuth?</h2><p>OAuth is an open standard for authentication. It allows users to securely sign in to an application, whether that's using a standard username & password set, or by using a trusted third party that also implements OAuth. I’m sure most of you reading this have used an app that allows something like “Sign in with Google”, that’s OAuth in action.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/8b95ce2b-c9b3-4fc2-b340-e1cb331c3ecd-image.png\" /><figcaption></figcaption></figure><p>An example of social login implemented.</p><h2>What are JWTs?</h2><p>JSON Web Tokens (or JWTs for short) are the result of the login process. In older (or less secure web apps), an authentication scheme called Basic Auth was used, which is essentially a base64 encoded version of a username & password. The downside of this approach is anyone who gets the encoded credential set can easily decode it and then they have your password.</p><p>JWTs are base64 encoded AND signed with a secret of some kind. They also have expiration built in, so if someone does somehow get one of your tokens, they are only valid for a short timeframe.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/332c0834-18e9-4971-950b-6665106bcd60-image.png\" /><figcaption></figcaption></figure><p>An example of the JWT we’ll create by the end of this article.</p><h2>What is Cognito?</h2><p>Cognito is a user management system in AWS designed to be used by applications and eliminate the need for developers to roll their own system. It allows you to add authentication & user management to your app without having to worry about being responsible for user credentials yourself. It also comes with a slew of nice features, including a sign-up & sign-in UI out of the box. Cognito uses OAuth to issue JWTs that you can use within your application to validate the identity of users. It also has excellent integrations built for all AWS services, so if you are using something like API Gateway, it can drastically cut the time it takes to implement auth in your app.</p><h2>Setting up Cognito</h2><p>Use the global search box to find “Cognito\" and open it up. Once there, click “<strong>Create user pool”.</strong></p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/610b7051-c2dd-457b-b86d-b84f92a6b431-image.png\" /><figcaption></figcaption></figure><p>Searching for Cognito in the AWS global search.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/36c23f78-fda8-495e-a15e-ef2d32a72079-image.png\" /><figcaption></figcaption></figure><p>The default Cognito landing page, with Create user pool button highlighted.</p><p>Next, you’ll be stepped through the process of creating a user pool. The first page lets you select the primary method by which you want to allow users to sign in. I’m a fan of using the email address, so select that and click “<strong>Next</strong>”.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/7ccbd7c5-ed94-4df3-9583-35bdb57636ab-image.png\" /><figcaption></figcaption></figure><p>The first view of the Cognito new user pool wizard.</p><p>On the next view, you can configure security requirements for your user. All of these options can be left as default, with the exception of the Multi-factor authentication section. I recommend leaving this on for additional security, but set it to No MFA for keeping this tutorial on the simpler end (MFA adds little value to the walkthrough). Uncheck that box and scroll down to the bottom and click “<strong>Next</strong>”.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/f66bf20a-c267-44ee-a6df-b81f8bf821f3-image.png\" /><figcaption></figcaption></figure><p>The Multi-factor authentication settings of creating a user pool.</p><p>The next view affects how the users will sign-up. Of all these settings, the first one is probably the most important. Self-registration allows ANYONE to create an account for your app. If you are creating an internal app, you definitely want to leave this unchecked. We can leave it enabled for now though. You can safely leave these settings as they are and click “<strong>Next</strong>”.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/0b7a27e3-357f-457a-a700-038cbe2c20cc-image.png\" /><figcaption></figcaption></figure><p>The Configure sign-up experience settings.</p><p>The next view allows you to customize how email messages are sent. While SES is recommended, it touches on services we haven’t covered so select “<strong>Send email with Cognito</strong>” and click “<strong>Next</strong>”.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/9f140355-277b-4cfc-b419-9d0e95fad679-image.png\" /><figcaption></figcaption></figure><p>The Configure message delivery settings of the new user pool wizard.</p><p>Now you get to name your user pool and configure a few more connectivity options. For the name, typically you’d name it something similar to your app, but it's arbitrary. I’ll name mine <strong>GoApiUserPool</strong>. Next, make sure to check the “<strong>Use the Cognito Hosted UI</strong>” button as it lets users use the default interfaces without having to build UIs yourself.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/20c76a1d-acf4-4a03-bed2-2f0d42194f3e-image.png\" /><figcaption></figcaption></figure><p>The User pool name and Hosted authentication pages sections of the new user pool wizard.</p><p>You’ll also notice a Domain section appear. Leave the “<strong>Use a Cognito domain</strong>” option set and select a domain prefix. It can be whatever you want it to be but has to be unique to the AWS region across ALL accounts, meaning nobody else can have selected that prefix. In my case, <strong>go-api-user-pool</strong> was available so I set mine to that.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/a43960bb-af43-4946-8843-6b3b9eb52092-image.png\" /><figcaption></figcaption></figure><p>The Domain settings of the new user pool wizard.</p><p>Under the Initial app client section, there are quite a few bits to configure, so let’s start by giving your app client a name. The name is unique to your user pool, so it can be whatever you want. I’ll name mine <strong>Default App Client</strong>. For client secret, ensure the second option is selected.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/e91b6080-f2b9-4ca4-b30c-4a87b462d7ff-image.png\" /><figcaption></figcaption></figure><p>The Initial app client settings of the new user pool wizard.</p><p>Under <strong>Allowed callback URLs</strong>, you’d typically add a URL that is part of your front end and is capable of handling OAuth callbacks. In this series, we won’t be building a UI of any kind, but one of my favorite sites to use to analyze the tokens that will be created by Cognito is <a href=\"https://jwt.io/\" target=\"_blank\"><code>https://jwt.io</code></a>, so put that in there. If you aren’t familiar with OAuth flows, it will make more sense when we see this all in action.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/884da61c-4f79-4665-a847-3a9f4121f18b-image.png\" /><figcaption></figcaption></figure><p>The Allowed callback URLs section of the new user pool wizard.</p><p>Now expand Advanced app client settings. Scroll down a bit until you see OAuth 2.0 Grant Types. “<strong>Authorization code grant</strong>” is selected by default, but you also want to make sure “<strong>Implicit grant</strong>” is selected as it will be easier to analyze the tokens in this demo. Once you’ve checked that, scroll to the bottom & click “<strong>Next</strong>”.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/fb7c1df2-d15e-4ee3-bf97-bc316492f3a2-image.png\" /><figcaption></figcaption></figure><p>The OAuth 2 grant types section of the new user pool wizard.</p><p>This is the last view of the setup wizard. Feel free to double-check your settings then click “<strong>Create user pool</strong>” at the bottom of the page.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/5a85032f-a1a4-4152-aa4a-e9f1efb93ff8-image.png\" /><figcaption></figcaption></figure><p>The review page of the new user pool wizard, with Create user pool highlighted.</p><p>You’ll be dropped into a list of your user pools. Select the new user pool you just created in the list.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/5d3994e6-53aa-41a1-9dcc-a2745ce94fb8-image.png\" /><figcaption></figcaption></figure><p>A list of your user pools.</p><p>Let’s create a new user to test the login portion with. Click the “<strong>Create user</strong>” button in the center of the page. You’ll be presented with a form to create a user. Let’s set up the form like so:</p><ul><li>Invitation message: Send an email invitation</li><li>Email address: Your email address</li><li>Mark email address as verified: Checked</li><li>Temporary password: Generate a password</li></ul><p>Click “<strong>Create user</strong>”.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/63439f3e-d26f-473a-a958-e266ec4cb5b9-image.png\" /><figcaption></figcaption></figure><p>The form to create a new user.</p><p>Back in the previous view, you now have a list that should show the new account that was created. Notice how the <strong>Confirmation status</strong> is set to <strong>Force change password</strong>.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/54e99222-9a21-48b4-8288-caf5b9b362aa-image.png\" /><figcaption></figcaption></figure><p>The users list of the user pool created.</p><p>Now check your email and you should have received an email to confirm your new account in Cognito, along with a temporary password.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/c249f564-8b2a-4c5d-99f8-e9bef4449099-image.png\" /><figcaption></figcaption></figure><p>The email a new user will receive with their temporary password.</p><p>To test your new account, you actually need to manually craft a URL to put into your browser. To create the URL, you’ll need to grab the Cognito domain from the “<strong>App integration</strong>” tab.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/e3ec343a-9531-45e0-ab86-39e851d371b7-image.png\" /><figcaption></figcaption></figure><p>The App integration tab for the user pool.</p><p>In the same tab, scroll to the bottom and grab the Client ID from the App client list.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/a6970994-48d0-4976-876a-12ffe7ae3ed9-image.png\" /><figcaption></figcaption></figure><p>The App clients list for the user pool.</p><p>The URL you’ll create will have the following format:</p><pre class=\"language-plain text\"><code>&lt;COGNITO_DOMAIN&gt;/oauth2/authorize?response_type=token&client_id=&lt;CLIENT_ID&gt;&redirect_uri=https://jwt.io</code></pre><p>Breaking this URL down, here are what each of the components of the query string are used for:</p><ul><li><strong>response_type</strong>: Tells Cognito which OAuth flow we’ll be using (token is Implicit Grant)</li><li><strong>client_id</strong>: Matches the request to the settings that were configured in the App client we created during the initial wizard.</li><li><strong>redirect_uri</strong>: Tells Cognito where to send the user after they’ve logged in. This MUST be added in the app client, otherwise the request will be rejected.</li></ul><p>Now enter the URL into your browser and you should be presented with a login form. Enter the info you received in your email to set a custom password.</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/ca6ef814-46a4-4ec1-8ab9-f036df558fe8-image.png\" /><figcaption></figcaption></figure><p>The default sign in form for Cognito user pools.</p><p>Once you set the password, you’ll be redirected to <a href=\"http://jwt.io/\" target=\"_blank\">jwt.io</a> with the Encoded & Decoded sections populated. The Encoded section contains the token that was generated from Congito, and the Decoded section shows information that the token contains. If you look in the Payload portion of the Decoded section, you can even see the email address and Cognito domain in there!</p><figure><img src=\"/img/n/create-an-aws-cognito-user-pool/f2dc4e3d-cc03-4998-aa23-209638054284-image.png\" /><figcaption></figcaption></figure><p>A screenshot of the token issued from Cognito in JWT.io.</p><p>So where did this come from? It’s actually in the URL. If you look at the hash values of the URL, you’ll have a few elements in there. Below is the URL that brought me to this page, with the various hash components on new lines for a better look.</p><ul><li><strong>id_token</strong>: Contains information about the user (this is what’s displayed on <a href=\"http://jwt.io/\" target=\"_blank\">jwt.io</a>).</li><li><strong>access_token</strong>: Contains information about what the user is allowed to access.</li><li><strong>expires_in</strong>: The number of seconds until the tokens expire and new ones need to be requested.</li><li><strong>token_type</strong>: The scheme that should be used in the Authorization header of requests (we’ll see this shortly).</li></ul><pre class=\"language-plain text\"><code>https://jwt.io/\n#id_token=eyJraWQiOiJHa1JoRVc0TWlJUSs3QmZMbGYwXC9sN1ZyUEJhMjJhXC81clpmdWhhQjNyTGc9IiwiYWxnIjoiUlMyNTYifQ.eyJhdF9oYXNoIjoiVjhMZG05blZkc3o0cmdUaEZuNURDUSIsInN1YiI6IjFlOGYwMGM5LTczYTAtNDZmYy04MjEzLWY0ZDUyM2E5ZjMzNiIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV9vU2R2OXFSWHUiLCJjb2duaXRvOnVzZXJuYW1lIjoiMWU4ZjAwYzktNzNhMC00NmZjLTgyMTMtZjRkNTIzYTlmMzM2IiwiYXVkIjoiYjl1N3Vsa3V2Zjk5cjZuaTc3NGwwbmVmNyIsImV2ZW50X2lkIjoiYTNhYWFjODYtMWU4MC00NmJhLWI5ZGEtMzVkNTc1Y2RjMmE1IiwidG9rZW5fdXNlIjoiaWQiLCJhdXRoX3RpbWUiOjE2NTgxNzY2MDgsImV4cCI6MTY1ODE4MDIwOCwiaWF0IjoxNjU4MTc2NjA4LCJqdGkiOiJkNWU1NjZiZS1lY2IyLTQxN2UtYThkMy0yNzFlNTRhMDEyMWMiLCJlbWFpbCI6ImJyaWFuQGJyaWFubW9ycmlzb24ubWUifQ.qgFVrdcekUzGesEUbplynFBM5239n-g4roGaHGq_7UlyGmGiLaeDBLziSUocTa2tpKNZ159pzSbmMajxLo_V4mRkhvWNldjA-EMkiBRxTaphnWkhq2HfwOmnw3UW1xGhVj51u8nm7bH96nu0AKf-lT7MEGv8hP09vNGtdFBTDJniN4xqqxYtbqppM70datgtjq_2SBVwqSwnqKbtx6s0eJj74UWijhy1JwBgxp6Tvodi-9T8WrSEj46QkO1LJF15fwdqEi38BrBRC64HCB2DK1jb_xhbQQJG3qklnc_ciu0fAzMPoeVCbqHcLy9RfLY2aaGThHCIKJpkCCAQoaPUgg\n&access_token=eyJraWQiOiJWQU5yeGMrQno5V1BRakxVXC9BTHozZzNsZXg5VHpmR0VXczJTbFhQTGhHST0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiIxZThmMDBjOS03M2EwLTQ2ZmMtODIxMy1mNGQ1MjNhOWYzMzYiLCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV9vU2R2OXFSWHUiLCJ2ZXJzaW9uIjoyLCJjbGllbnRfaWQiOiJiOXU3dWxrdXZmOTlyNm5pNzc0bDBuZWY3IiwiZXZlbnRfaWQiOiJhM2FhYWM4Ni0xZTgwLTQ2YmEtYjlkYS0zNWQ1NzVjZGMyYTUiLCJ0b2tlbl91c2UiOiJhY2Nlc3MiLCJzY29wZSI6InBob25lIG9wZW5pZCBlbWFpbCIsImF1dGhfdGltZSI6MTY1ODE3NjYwOCwiZXhwIjoxNjU4MTgwMjA4LCJpYXQiOjE2NTgxNzY2MDgsImp0aSI6ImVlNmQ4ZDU2LTY0MzctNGY4Zi1hYWY5LWEzYmQyYzhmOWU1OSIsInVzZXJuYW1lIjoiMWU4ZjAwYzktNzNhMC00NmZjLTgyMTMtZjRkNTIzYTlmMzM2In0.QBhNNT1JQ3CbpU7ZBe7Yptpo0piuSA7T_24fo5ItKRcTM8h7OP9Puj7f6JI8ziLH2Z2cRx-1pqhsIyzeSooqCe5yLX8OM_xFo6J-66o-K6I7YMkE0JKcsEvxVsOuW7vDCfr_iY0cVvS5zJPCgrqr7Svp0Il0j22EdJhGso8uxFeaH8Rzm9JlUsxWIY0OvbIJugABjTMVxcczh3wyeovzjFU0HMVoJsA6v0Cse-SlI2LOe01BBiHGGG6-AlP7Ch5L_OIxNeoa1kDkuJaaB31n9N0UOqmcOlU8rf1qK2BPlUpqfSJDpiuCzWeVheiN5diha9NfTUQPwPx08W3jU_Tn5w\n&expires_in=3600\n&token_type=Bearer\n</code></pre><h2>What's next?</h2><p>From here, I’d encourage you to explore how to validate these tokens using the language of your choice. My next entry in this series will cover exactly how to do this using API Gateway, and explore the claims using Go in AWS Lambda for building a serverless application.</p></div>","featuredImage":"/img/n/create-an-aws-cognito-user-pool/ccc3e119-a60e-4856-9c63-d03415211eb5-aws-cognito-user-pool.jpg","excerpt":"I recently started a series on building a serverless API with AWS & Go. When I started drafting this article, I inten...","cachedOn":1681394434},{"id":"106b879d-5288-4616-b409-87cce21a5790","notion_id":"106b879d-5288-4616-b409-87cce21a5790","status":"Published","slug":"first-experience-with-planetscale","relation_series":[],"publishOn":"2022-07-01T00:00:00.000Z","codeURL":"https://github.com/GuardianForge/guardianforge.net/tree/dev/planetscale-poc","seriesOrder":null,"youTubeURL":null,"title":"First Experience with PlanetScale","html":"<div><p>I just recently started a new job as Developer Educator at PlanetScale, a serverless, MySQL-compatible database platform. A big part of my role at the company is creating resources to help other developers understand the platform and more easily build stuff around it. I’ve worked with MySQL over my years as a developer but still need to get familiar with PlanetScale as a platform. So as a way to learn it better, I decided to port the backend of my Saas <a href=\"https://guardianforge.net/\" target=\"_blank\">GuardianForge</a> over to PlanetScale.</p><blockquote>At the end of this article, I’ll drop a link to the GitHub branch that all of this work is being done on for research purposes.</blockquote><h2>The Goal</h2><p>The end goal of this exercise is to cut over a few of the database operations from DynamoDB to PlanetScale, particularly around Builds which are the primary entities of GuardianForge. I’ll be updating the following operations:</p><ul><li>Creating a Build (Create)</li><li>Updating a Build (Update & Read One)</li><li>Archive a Build (Delete)</li><li>Getting the Latest Builds (Read Many)</li></ul><p>All of this currently happens in a single table, so no joins will be taken into consideration, but it tests all of the standard CRUD logic that applies to database platforms.</p><blockquote>As info, CRUD is an acronym in the data world that stands for Create, Read, Update, Delete.</blockquote><h2>PlanetScale Setup</h2><p>Although I am now employed with PlanetScale, my experience with the platform to this point has been pretty small, so I’m approaching this from the ground level. I setup a new organization in PlanetScale for testing and was presented with a pretty nice wizard walking me through some of the PlanetScale basics. Branching is one of the core features as I understand, so its pretty front & center in the wizard. At the end of this, I was prompted to create a database.</p><p>The view I'm presented with upon signing in for the first time.</p><figure><img src=\"/img/n/first-experience-with-planetscale/ddb4a51e-7825-4eaf-ab7a-98974b535ad7-capture_2022_06_2914_03_34.jpg\" /><figcaption></figcaption></figure><p>Details about branching & how it works.</p><figure><img src=\"/img/n/first-experience-with-planetscale/48aba801-18db-444f-9eac-932148e3d63a-capture_2022_06_2914_04_25.jpg\" /><figcaption></figcaption></figure><p>The last view of the wizard, prompting me to create a database.</p><figure><img src=\"/img/n/first-experience-with-planetscale/4ccd6359-7bdb-46ca-8213-3fdd5c66bfa0-capture_2022_06_2914_04_52.jpg\" /><figcaption></figcaption></figure><p>Once my DB was created, I was able to grab my connection string using the button in the first image below. It’s a standard MySQL connection string for Go.</p><p>Clicking the Connect button displays the connection string.</p><figure><img src=\"/img/n/first-experience-with-planetscale/60a8bf57-ecaa-459f-949e-32302e38d20a-capture_2022_06_2914_05_05.jpg\" /><figcaption></figcaption></figure><p>Once opening this view, I had to select the '.env' tab to see the connection string.</p><figure><img src=\"/img/n/first-experience-with-planetscale/fa68a00c-00c3-4c52-94f2-e36e9388001e-capture_2022_06_2914_05_32.jpg\" /><figcaption></figcaption></figure><p>And here is what would be used to connect to the database.</p><figure><img src=\"/img/n/first-experience-with-planetscale/f072b762-6244-424b-aa24-6c9cc81dcace-capture_2022_06_2914_05_49.png\" /><figcaption></figcaption></figure><p>The <code>main</code> branch is created by default, so in order to drop into a web console, I had to navigate to Branches, then select the <code>main</code> branch, then Console.</p><p>The Branches tab takes you to a view to manage branches for the database.</p><figure><img src=\"/img/n/first-experience-with-planetscale/6b87872e-8407-46ff-8940-65f1649f788b-capture_2022_06_2914_12_05.jpg\" /><figcaption></figcaption></figure><p>Selecting the main branch will drop you into those settings.</p><figure><img src=\"/img/n/first-experience-with-planetscale/98b542ab-573b-4a81-95db-bbc89c367d85-capture_2022_06_2914_12_50.jpg\" /><figcaption></figcaption></figure><p>Select Console gets you an in-browser terminal to manage the database/branch.</p><figure><img src=\"/img/n/first-experience-with-planetscale/89e6df6c-7b3c-48e6-9d86-ad49e0e1c766-capture_2022_06_2914_12_55.jpg\" /><figcaption></figcaption></figure><p>Now before I created my table, I wanted to review the existing <code>Build</code> struct in the code to determine what fields were necessary to store in PlanetScale. I store only meta data about a build in the database, whereas the bulk of the data is in S3. Below are the structs that make up a Build. The BuildSummary is included since its stored as JSON in Dynamo and I wanted to abstract some of the fields to store them directly in the table.</p><pre class=\"language-go\"><code>type Build struct {\n\t// DynamoDB Partition Key\n\tEntityType      string         `json:\"entityType\" dynamodbav:\"entityType\"`\n  // DynamoDB Sort Key\n\tId              string         `json:\"entityId\" dynamodbav:\"entityId\"`\n\tPublishedOn     int64          `json:\"publishedOn\" dynamodbav:\"publishedOn\"`\n\tCreatedById     string         `json:\"createdById\" dynamodbav:\"createdById\"`\n\tIsPrivate       bool           `json:\"isPrivate\" dynamodbav:\"isPrivate\"`\n\tSearchKey       string         `json:\"searchKey\" dynamodbav:\"searchKey\"`\n\tSummary         BuildSummary   `json:\"summary\" dynamodbav:\"summary\"`\n\tUpvotes         int            `json:\"upvotes\" dynamodbav:\"upvotes\"`\n\tSeasonalUpvotes map[string]int `json:\"seasonalUpvotes\" dynamodbav:\"seasonalUpvotes\"`\n}\n\ntype BuildSummary struct {\n\tUserId         string   `json:\"userId\" dynamodbav:\"userId\"`\n\tUsername       string   `json:\"username\" dynamodbav:\"username\"`\n\tHighlights     []string `json:\"highlights\" dynamodbav:\"highlights\"`\n\tName           string   `json:\"name\" dynamodbav:\"name\"`\n\tPrimaryIconSet string   `json:\"primaryIconSet\" dynamodbav:\"primaryIconSet\"`\n\n\t// Non DynamoDB Fields\n\tPublishedOn     *int64          `json:\"publishedOn\"`\n\tBuildId         *string         `json:\"id\"`\n\tActivity        *int            `json:\"activity\"`\n\tUpvotes         *int            `json:\"upvotes\"`\n\tSeasonalUpvotes *map[string]int `json:\"seasonalUpvotes\"`\n}\n</code></pre><p>So using the structs above, I came up with this script to create the Builds table.</p><figure><img src=\"/img/n/first-experience-with-planetscale/09fe09d2-f9d7-4442-ad18-673b1212e667-image-2.png\" /><figcaption></figcaption></figure><p>The initial script I used to create the Builds table.</p><p>The console took the script no problem, but when I tried to promote my <code>main</code> branch to be the production DB, an error was thrown since I didn’t flag my <code>Id</code> column with the <code>unique</code> keyword. PlanetScale doesn’t let you perform schema updates on the production branch, so I needed to create a new branch, make the changes there, and merge them into main. I also changed the <code>Id</code> column type to make room for more data (a mistake in my original script).</p><p>The New branch modal.</p><figure><img src=\"/img/n/first-experience-with-planetscale/11379064-c11b-4829-90d9-b6a2785d8583-capture_2022_06_2915_45_25.png\" /><figcaption></figcaption></figure><p>This was the script I used to fix the table before I could work with it.</p><figure><img src=\"/img/n/first-experience-with-planetscale/27c3c1c9-cda6-4a57-ae24-8414881da2a1-capture_2022_06_2915_47_12.png\" /><figcaption></figcaption></figure><p>This is the view to 'merge' branches in PlanetScale.</p><figure><img src=\"/img/n/first-experience-with-planetscale/3dd6f1b3-1926-4997-982b-a66fcda55833-capture_2022_06_2915_47_38.png\" /><figcaption></figcaption></figure><p>Now that my table has been created, I was ready to start building in my Go backend.</p><h2>Creating a Build</h2><p>Instead of seeing data from Dynamo, I decided to start with the logic around creating builds so I can insert data into the table as if it was coming from the UI. I started by creating a new file called <code>planetscale.go</code> to encapsulate all of my logic for working with the service. I added a simple <code>connect</code> method to the file that can be called from each function that runs. I’m using the connection string provided when I created the database and storing it as the <code>PS_CONN_STR</code> environment variable. Since this is all in a serverless environment (AWS Lambda) I don’t need to worry too much about checking whether or not a connection already exists.</p><pre class=\"language-go\"><code>// backend/core/services/planetscale.go\n\nvar psdb *sql.DB\n\nfunc connect() error {\n\t_psdb, err := sql.Open(\"mysql\", os.Getenv(\"PS_CONN_STR\"))\n\tif err == nil {\n\t\tpsdb = _psdb\n\t}\n\treturn err\n}\n</code></pre><p>Then I created a <code>CreateBuild</code> function, that will write data to PlanetScale as the API is hit. What’s important to note here is that this is all standard MySQL code in Go. There is nothing necessary native to PlanetScale going on here, which is pretty neat.</p><pre class=\"language-go\"><code>// backend/core/services/planetscale.go\n\nfunc CreateBuild(buildRecord dbModels.Build) error {\n\terr := connect()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(CreateBuild) connect\")\n\t}\n\n\t// This is just a method to flatten the Build &amp; BuildSummary structs\n\tpsbuild := buildRecord.ToPSBuild()\n\n\tquery := `INSERT INTO Builds(Id,\n\t\tPublishedOn,\n\t\tCreatedById,\n\t\tUpvotes,\n\t\tSeasonalUpvotes,\n\t\tUserId,\n\t\tUsername,\n\t\tHighlights,\n\t\tName,\n\t\tPrimaryIconSet) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`\n\n\tstmt, err := psdb.Prepare(query)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(CreateBuild) Prepare\")\n\t}\n\tdefer stmt.Close()\n\n\t_, err = stmt.Exec(\n\t\tpsbuild.Id,\n\t\tpsbuild.PublishedOn,\n\t\tpsbuild.CreatedById,\n\t\tpsbuild.Upvotes,\n\t\tpsbuild.SeasonalUpvotes,\n\t\tpsbuild.UserId,\n\t\tpsbuild.Username,\n\t\tpsbuild.Highlights,\n\t\tpsbuild.Name,\n\t\tpsbuild.PrimaryIconSet,\n\t)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(CreateBuild) Exec\")\n\t}\n}\n</code></pre><p>Now in the Lambda function that actually handles the <code>POST /builds</code> request, I just commented out the code that hit Dynamo and added the necessary code to hit the new function above.</p><pre class=\"language-go\"><code>// backend/stack/api/builds/_post/main.go\n\n// err = services.PutBuildToDynamo(sess, dbBuild)\n// if err != nil {\n// \treturn utils.ErrorResponse(err, \"(handler) put build to dynamo\")\n// }\n\nerr := services.CreateBuild(dbBuild)\nif err != nil {\n\treturn utils.ErrorResponse(err, \"(handler) CreateBuild\")\n}\n</code></pre><p>And we had a successful build created! Below is the UI from when the build was created followed by a select statement from within the PlanetScale web console on the <code>main</code> branch (this needed to be turned on in my DB settings, otherwise it’s disabled by default).</p><p>The view after a build was created.</p><figure><img src=\"/img/n/first-experience-with-planetscale/bfc5bf89-d19f-477b-b3e8-6a5a8f5bc5bc-capture_2022_06_2915_48_35.png\" /><figcaption></figcaption></figure><p>The data in PlanetScale.</p><figure><img src=\"/img/n/first-experience-with-planetscale/796df025-8d00-4b63-97b3-857fa320ba43-capture_2022_06_2915_49_32.png\" /><figcaption></figcaption></figure><h2>Updating a Build</h2><p>Certain fields in a Build are allowed to be updated at a future date. Most of that data is stored in the S3 document but the build name is stored in the database, so that’s the only field we’ll target here.</p><p>I updated my <strong>planetscale.go</strong> and added the following function. As you can see it’s very similar to the <code>CreateBuild</code>function, just with a different query and slightly different parameters. Honestly most of the functions written will look very similar, which is just the nature of working with a database.</p><pre class=\"language-go\"><code>// backend/core/services/planetscale.go\n\nfunc UpdateBuild(buildId string, buildRecord dbModels.Build) error {\n\terr := connect()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(UpdateBuild) connect\")\n\t}\n\n\tpsbuild := buildRecord.ToPSBuild()\n\n\tquery := `UPDATE Builds SET Name = ? WHERE Id = ?`\n\tstmt, err := psdb.Prepare(query)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(UpdateBuild) Prepare\")\n\t}\n\tdefer stmt.Close()\n\n\t_, err = stmt.Exec(\n\t\tpsbuild.Name,\n\t\tbuildId,\n\t)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(UpdateBuild) Exec\")\n\t}\n\treturn nil\n}\n</code></pre><p>I updated the code in the PUT function that performs updates on the builds. Notice there is also a call to grab the record from Dynamo first to ensure it exists before trying to update anything. Since I cut over the functionality to create builds to PlanetScale, that data won’t exist in Dynamo, so I needed to create the function to fetch a single build from PlanetScale.</p><pre class=\"language-go\"><code>// backend/core/services/planetscale.go\n\n func PSFetchBuildById(buildId string) (*dbModels.Build, error) {\n\terr := connect()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(PSFetchBuildById) connect\")\n\t}\n\n\tquery := \"SELECT * FROM Builds WHERE Id = ? LIMIT 1\"\n\tstmt, err := psdb.Prepare(query)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(PSFetchBuildById) Prepare\")\n\t}\n\tdefer stmt.Close()\n\n\tbuild := dbModels.Build{\n\t\tSummary: dbModels.BuildSummary{},\n\t}\n\n\tvar isPrivate *bool\n\tvar seasonalUpvotesString string\n\tvar highlightsString string\n\n\terr = stmt.QueryRow(buildId).Scan(\n\t\t&build.Id,\n\t\t&build.PublishedOn,\n\t\t&build.CreatedById,\n\t\t&isPrivate,\n\t\t&build.Upvotes,\n\t\t&seasonalUpvotesString,\n\t\t&build.Summary.UserId,\n\t\t&build.Summary.Username,\n\t\t&highlightsString,\n\t\t&build.Summary.Name,\n\t\t&build.Summary.PrimaryIconSet,\n\t)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(PSFetchBuildById) execQuery\")\n\t}\n\n\tif isPrivate != nil &amp;&amp; *isPrivate == true {\n\t\tbuild.IsPrivate = true\n\t}\n\n\tif seasonalUpvotesString != \"\" {\n\t\terr = json.Unmarshal([]byte(seasonalUpvotesString), &amp;build.Summary.Highlights)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"(PSFetchBuildId) unmarshal seasonal upvotes\")\n\t\t}\n\t}\n\n\tif highlightsString != \"\" {\n\t\terr = json.Unmarshal([]byte(highlightsString), &amp;build.Summary.Highlights)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"(PSFetchBuildId) unmarshal highlights\")\n\t\t}\n\t}\n\n\treturn &amp;build, err\n}\n</code></pre><p>Here is the updated code in the PUT function that does everything it needs to with PlanetScale over Dynamo.</p><pre class=\"language-go\"><code>// backend/stack/api/builds/_put/main.go\n\n// Fetch from Dynamo, confirm userId matches\n// record, err := services.FetchBuildById(buildId)\nrecord, err := services.PSFetchBuildById(buildId)\nif err != nil {\n\treturn utils.ErrorResponse(err, \"(updateBuild) FetchBuildsById\")\n}\nif record.CreatedById != *membershipId {\n\treturn utils.UnauthorizedResponse(nil)\n}\n\nif requestModel.Name != nil {\n\t// sess, err := session.NewSession()\n\t// if err != nil {\n\t// \treturn utils.ErrorResponse(err, \"(updateBuild) Creating AWS session to update Dynamo record\")\n\t// }\n\n\trecord.Summary.Name = *requestModel.Name\n\t// err = services.PutBuildToDynamo(sess, *record)\n\t// if err != nil {\n\t// \treturn utils.ErrorResponse(err, \"(updateBuild) Put build to Dynamo\")\n\t// }\n\n\terr = services.UpdateBuild(buildId, *record)\n\tif err != nil {\n\t\treturn utils.ErrorResponse(err, \"(updateBuild) Update build in PlanetScale\")\n\t}\n}\n</code></pre><p>And finally, testing the code all the way through.</p><p>Updating the Build in GuardianForge.</p><figure><img src=\"/img/n/first-experience-with-planetscale/13441e07-5bc9-4057-9978-63c41c1c1f56-capture_2022_06_3014_45_26.png\" /><figcaption></figcaption></figure><p>The updated build, note the UPDATED in the Name column.</p><figure><img src=\"/img/n/first-experience-with-planetscale/f11c63da-8674-4e59-923a-280e81227439-capture_2022_06_3015_26_34.png\" /><figcaption></figcaption></figure><h2>Archiving a Build</h2><p>Archiving is effectively a DELETE call in the API. It wipes the database records from Dynamo, so I’m doing the same in PlanetScale here. This is easily the simplest of all functions so far, note the simple change in the <code>query</code> variable over the <code>UpdateBuild</code> function.</p><pre class=\"language-go\"><code>// backend/core/services/planetscale.go\n\nfunc DeleteBuild(buildId string) error {\n\terr := connect()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(DeleteBuild) connect\")\n\t}\n\n\tquery := `DELETE FROM Builds WHERE Id = ?`\n\tstmt, err := psdb.Prepare(query)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(DeleteBuild) Prepare\")\n\t}\n\tdefer stmt.Close()\n\n\t_, err = stmt.Exec(buildId)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(DeleteBuild) Exec\")\n\t}\n\treturn nil\n}\n\n</code></pre><h2>Fetching the Latest Builds</h2><p>This is what is called whenever someone hits the GuardianForge home page or logs in for the first time. At the bottom of the page, there is a list of the latest 15 builds.</p><p>The current Latest Builds view.</p><figure><img src=\"/img/n/first-experience-with-planetscale/55c8e026-5eda-443a-99ee-1dd817217f33-capture_2022_06_3015_48_40.jpg\" /><figcaption></figcaption></figure><p>So let’s switch this over to work with PlanetScale now. I’ll add another function to fetch the latest builds, sorted in reverse by PublishedOn, which is the unix timestamp of when that build was created. Here is the query from from the PlanetScale console.</p><p>Latest builds in PlanetScale.</p><figure><img src=\"/img/n/first-experience-with-planetscale/b2411942-2bf4-485e-a36a-3918b594734a-capture_2022_06_3015_53_36.png\" /><figcaption></figcaption></figure><p>This function is a bit bigger than the ones we’ve seen so far, but that’s primarily due to the reason that you need to iterate over each row and append it to a slice before returning. I’m also only returning the summaries here, although that doesn’t change the code too much.</p><pre class=\"language-go\"><code>// backend/core/services/planetscale.go\n\nfunc PSFetchLatestBuilds() ([]dbModels.BuildSummary, error) {\n\terr := connect()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(PSFetchLatestBuilds) connect\")\n\t}\n\n\tquery := \"SELECT * FROM Builds ORDER BY PublishedOn DESC LIMIT 15\"\n\tstmt, err := psdb.Prepare(query)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(PSFetchLatestBuilds) Prepare\")\n\t}\n\tdefer stmt.Close()\n\n\trows, err := stmt.Query()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(PSFetchLatestBuilds) query\")\n\t}\n\tdefer rows.Close()\n\n\tsummaries := []dbModels.BuildSummary{}\n\n\tfor rows.Next() {\n\t\tbuild := dbModels.Build{\n\t\t\tSummary: dbModels.BuildSummary{},\n\t\t}\n\t\tvar isPrivate *bool\n\t\tvar seasonalUpvotesString string\n\t\tvar highlightsString string\n\n\t\terr = rows.Scan(\n\t\t\t&build.Id,\n\t\t\t&build.PublishedOn,\n\t\t\t&build.CreatedById,\n\t\t\t&isPrivate,\n\t\t\t&build.Upvotes,\n\t\t\t&seasonalUpvotesString,\n\t\t\t&build.Summary.UserId,\n\t\t\t&build.Summary.Username,\n\t\t\t&highlightsString,\n\t\t\t&build.Summary.Name,\n\t\t\t&build.Summary.PrimaryIconSet,\n\t\t)\n\n\t\tif isPrivate != nil &amp;&amp; *isPrivate == true {\n\t\t\tbuild.IsPrivate = true\n\t\t}\n\n\t\tif seasonalUpvotesString != \"\" {\n\t\t\terr = json.Unmarshal([]byte(seasonalUpvotesString), &amp;build.Summary.Highlights)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"(PSFetchLatestBuilds) unmarshal seasonal upvotes\")\n\t\t\t}\n\t\t}\n\n\t\tif highlightsString != \"\" {\n\t\t\terr = json.Unmarshal([]byte(highlightsString), &amp;build.Summary.Highlights)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"(PSFetchLatestBuilds) unmarshal highlights\")\n\t\t\t}\n\t\t}\n\t\tsummaries = append(summaries, build.Summary)\n\t}\n\n\treturn summaries, err\n}\n</code></pre><p>And after modifying the necessary handler, we have great success!</p><p>The Latest Builds view after switching the data to PlanetScale.</p><figure><img src=\"/img/n/first-experience-with-planetscale/52532283-36e6-4f54-b7ef-74c2b974a5e1-capture_2022_06_3016_19_01.png\" /><figcaption></figcaption></figure><h2>Summary</h2><p>So one of my first assigned tasks at PlanetScale has been to essentially explore the platform and get a feel for it. I work much better with real world examples, and since I host a real world product, it only makes sense to use it as a base for working with new services. In general, getting up and running with PlanetScale has been relevantly straightforward. Once the basic database is setup, its working with SQL as you would in any other Go project, which is pretty awesome that there isn’t any kind of fancy new syntax to learn.</p><blockquote>Feel free to review the source at https://github.com/GuardianForge/guardianforge.net/tree/dev/planetscale-poc.</blockquote></div>","featuredImage":"/img/n/first-experience-with-planetscale/67bda82c-240a-4a95-aa5e-75dceaa6dc99-background-2-2.jpg","icon":"/img/n/first-experience-with-planetscale/1b441663-147f-41cb-887c-ef608446b227-planetscale-logo-0EEA8CAEB4-seeklogo.com.png","excerpt":"I just recently started a new job as Developer Educator at PlanetScale, a serverless, MySQL-compatible database platf...","cachedOn":1681394436},{"id":"425c9b72-6eb8-4bdb-87ab-cbb305b74956","notion_id":"425c9b72-6eb8-4bdb-87ab-cbb305b74956","status":"Published","slug":"rearchitecting-the-front-end-of-guardianforge","relation_series":["22756350-048e-4423-8948-233370c99f6e"],"publishOn":"2022-06-28T00:00:00.000Z","codeURL":"https://github.com/guardianforge/guardianforge.net","seriesOrder":3,"youTubeURL":null,"title":"Rearchitecting the Front End of GuardianForge","html":"<div><p>Hey all. Recently completely overhauled the front end for GuardianForge and it was quite an interesting, educational, and fun project. No tutorial here, just me explaining the process and various challenges along the way!</p><h2>A Brief History</h2><p>So in early 2021 I started building the MVP for GuardianForge. Hell bent on using Vue because I like to go against the grain sometimes, that’s where I started. V1 of the SaaS was entirely a SPA in Vue hosted on AWS S3 and CloudFront. When I applied for AdSense, I quickly realized that their validation service didn’t like SPAs and that I’d need to create a more static site. I started with Gridsome but after some issues getting things up and running, decided to jump back to Gatsby since React has such a huge support system. It worked fine at the time but as the apps complexity grew, I realized shoving a dynamic app into an SSG was not the best approach. So that’s when I started planning to break things out.</p><h2>The New Structure</h2><p>So the plan was to break the app up into three separate apps: a core app, a blog app, and a documentation app. The first was generated using create-react-app and the latter two were generated using a Gatsby WordPress template (since I use WP as my headless CMS).</p><p>When I started this rebuild, luckily I had everything written in TS so rewriting the components up wasn’t terribly difficult due to the type checking in VSCode. I started with the main app, added in <strong>react-router-dom</strong> to handle routing and slowly moved my pages over. As the pages came over, VSCode would complain about missing components so I’d move those over too and test things as I went.</p><p>I essentially did the same with the Blog and Docs sites, but that was even less work since there is significantly less complexity. Then the fun part started: trying to get everything working together in a cohesive manner.</p><h2>Changes to the Architecture</h2><p>I use Azure DevOps to manage my CI/CD along with AWS SAM to manage my infrastructure in AWS. I wanted to break out my one pipeline into 4 (the three front end apps + the backend) to speed up deployments and not have me rebuild the whole damn thing over a simple typo, for example. If anyone has worked with static sites in AWS, you probably know that the files themselves are stored in S3 and served up via the built in Public Website feature, or placed behind a CloudFront CDN instance. Forge used the latter of these two so I can use my own domain & HTTPS cert. Since I was building three separate apps, I needed them to be in separate folders in S3 and that was the first challenge.</p><p>My S3 bucket structure.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/6f0881ca-7003-469b-9b75-0c7a83d9d134-capture_2022_06_2809_12_42.png\" /><figcaption></figcaption></figure><p>How could I serve up three separate static apps based on the URL Path, especially when one has literally no path? Here is what I needed to figure out:</p><ul><li>Any path that started with <strong>/blog</strong> should serve files in the <strong>blog</strong> folder in S3.</li><li>Any path that started with <strong>/docs</strong> should serve files in the <strong>docs</strong> filter in S3.</li><li>Any other scenarios should serve out of the <strong>app</strong> directory.</li></ul><p>Now CloudFront has two different ways to serve files from S3. The built in feature allows your CF instance to return files from S3 directly without having to mess with the security of the files. The alternate method is to enable the Public Website feature of the S3 bucket and instead use that URL as the origin for the files. Gatsby specifically needed to use the URL directly as the framework doesn’t play well with files directly from S3 (an implementation detail I had forgotten that made this take WAY longer than it should have 🤦‍♂️).</p><p>By using CloudFront behaviors and two separate origins (direct S3 URL for the Gatsby apps, and built in S3 integration for the CRA app), I was able to pull this off with the below configuration.</p><p>Here are my CF Origins. Note how the last one has an origin path set to /app, which is the catch-all for requests.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/47383a3f-ad0a-4f5c-9507-1334a8f1cd9f-capture_2022_06_2809_10_58.png\" /><figcaption></figcaption></figure><p>And here are the Behaviors that coorespond to the Origins based on path.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/1c9d1582-5bc4-486f-955e-f994261e0fcc-capture_2022_06_2809_11_55.png\" /><figcaption></figcaption></figure><p>Another benefit of this setup is that I can easily wipe the folders for each app as they are being deployed via Azure DevOps.</p><h2>Sharing Components Between the Apps</h2><p>One of the reasons I decided to use an SSG in the first place is because I didn’t want to copy/paste code between projects. Well through this process I finally took the time to figure out how to create a library of shared React components, which was actually much easier than I though it would be. It took me a bit to figure out the right import/export process, but once I figured that out I was able to pull this off.</p><p>In index.ts, I can export any TSX component from within my shared library.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/1a687770-9f91-42f9-9c43-49cc649c7883-capture_2022_06_2809_08_45.png\" /><figcaption></figcaption></figure><p>And its easy to import those components into any of the apps like so.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/f4e92631-c52c-4cf4-9e1b-e5fee2527039-capture_2022_06_2809_09_18.png\" /><figcaption></figcaption></figure><p>Since the shared library is just part of the mono repo, I don’t have to worry about hosting it anywhere. So I can install it in all three apps using the following command in the terminal:</p><pre class=\"language-bash\"><code>npm install ../shared</code></pre><p>Now one of the trickier things was figuring out how to create a Nav & Footer to share between the main app (created with CRA) and the blog/docs sites (using Gatsby). Gatsby uses <code>reach-router</code> whereas CRA uses <code>react-router-dom</code>. I knew that <code>react-bootstrap</code> had a way to somehow get a component to render as a different type, passed in via a prop. For example, if you want a <code>NavLink</code> to render as a div instead of the default anchor, you can set it via a prop, which is pretty awesome.</p><p>In the NavLink component, here is how they are able to set the component type dynamically,</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/80f7421d-97d1-4b31-8bd4-6e822e289a1d-capture_2022_06_2809_07_18.png\" /><figcaption></figcaption></figure><p>Turns out the trick to that is to add a prop that is a <code>React.ElementType</code> type and literally use the named variable as your element in the TSX. I discovered this after reviewing the source for <code>react-bootstrap</code>. Open source for the win!!!</p><p>Here is the code for the footer of all three apps. Note the linkComponent prop type on 43 and how its used on 55.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/aed8008c-4c96-444a-9463-3ac2e7a63297-capture_2022_06_2809_03_54.png\" /><figcaption></figcaption></figure><p>And here is how I'm able to pass in a component type as the linkComponent.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/33f21410-dbe5-4e6b-a3e3-052f09cce611-capture_2022_06_2809_05_05.png\" /><figcaption></figcaption></figure><h2>My Azure DevOps Pipelines</h2><p>Now the last bit to this entire process is the updated pipelines I had to setup in Azure DevOps. Recall that before, everything was setup in a single pipeline with two stages, QA and Production. I essentially had to pull out the necessary build steps from the pre-existing pipeline and create new pipelines for each of the new projects. The main app & backend still force me to deploy to QA first so I can check that everything is working properly when new functionality is added, but I decided to create separate pipelines for the blogs & docs apps, one each for QA & Prod so if I write an article, for example, I can deploy straight out instead of going through my standard QA process.</p><p>Here is a list of all the Release definitions I use in Azure DevOps.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/2ecdbc59-3638-4d9f-9a73-5fce415c0d85-capture_2022_06_2809_14_56.png\" /><figcaption></figcaption></figure><p>And here is essentially what each of the front end apps all do, with slight modifications based on app & environment.</p><figure><img src=\"/img/n/rearchitecting-the-front-end-of-guardianforge/28b3d5d4-ee60-4aec-a3eb-17474eb0f9a0-capture_2022_06_2809_16_03-2.png\" /><figcaption></figcaption></figure><h2>The Result</h2><p>Faster and more targeted deployments, and an easier to manage project that lets me add new feature & content MUCH faster. Plus I don’t have to fight against Gatsby for adding functionality to GuardianForge. I don’t fault the framework at all, I was trying to use it for something it really wasn’t designed to be used for.</p><blockquote>GuardianForge is entirely open sourced, feel free to explore the magic that powers it here.</blockquote></div>","featuredImage":"/img/n/rearchitecting-the-front-end-of-guardianforge/78070c4e-adf5-4c71-9d7b-026b0e179671-background-4.jpg","excerpt":"Hey all. Recently completely overhauled the front end for GuardianForge and it was quite an interesting, educational,...","cachedOn":1681394439},{"id":"9741d09d-6692-4101-a468-5bc46ee7a708","notion_id":"9741d09d-6692-4101-a468-5bc46ee7a708","status":"Published","slug":"porting-my-website-to-gatsby-wordpress","relation_series":[],"publishOn":"2022-06-27T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Porting my Website to Gatsby & WordPress","html":"<div><p>Over the past few days, I decided to rebuild my website for the fourth time. The goal was to make it use Gatsby so I can take advantage of React and all the stuff I’ve learned over the last year while building <a href=\"https://guardianforge.net/\" target=\"_blank\">GuardianForge</a>. I also wanted to go back to using WordPress as my content management system since Ive been using WordPress for over 10 years and quite enjoy it as a backend system. This article is largely about the stack in general and why I made some of the decisions I did, as well as a dive into the WordPress configuration and how it maps to Gatsby.</p><h2>Some History</h2><p>A few years back, I decided to experiment with Gatsby, a popular static site generator using React. I wasn’t big into React at the time (was more of a Vue fan) but I wanted to see what all the fuss was about. I was hooked on SSGs from the start, especially since I could still use WordPress as the CMS. Since I was more of a Vue fan, I then decided to port it over to Gridsome, which is effectively the same as Gatsby but with Vue instead of React. I also built my own custom CMS since its something I’d wanted to do for a while, and figure it’d give me more experience with AWS & could theoretically have better control over my data.</p><h2>So Why Move Back?</h2><p>Last year, I started building my SaaS called <a href=\"https://guardianforge.net/\" target=\"_blank\">GuardianForge</a> for Destiny players. I started with Vue but eventually switched to React since it is a more popular framework and figured it’d be easier to find support & components to get things done quicker.</p><p>Now as far as WordPress goes, I really like the Guttenberg editor, one of the 5 people who probably do from what I hear on the interwebs 😅. That on top of the fact that Gatsby’s support for WordPress has gotten MUCH better over the last few years, it was easy for me to decide that this was the direction I wanted to go. I’m also experimenting with Ulysses (which I get through <a href=\"https://go.setapp.com/invite/y0wyj8aa\" target=\"_blank\">SetApp</a>), which I’ve always heard good things about. Ulysses is cross platform in the Apple ecosystem and I can publish directly to WordPress from it.</p><h2>How I Use WordPress</h2><p>If you’ve used one of these static site generators, you know they support multitude of backend data systems, from local markdown files to calling APIs directly to get data and load it into the GraphQL layer of the framework. My WordPress instance is actually hosted in AWS as a LightSail VM. Its setup with multisite support since I use it for both this site and the Blog & Docs for GuardianForge. Using the Custom Post Types UI plugin lets me create custom post types very easily in the UI. For example, my <a href=\"notion://www.notion.so/portfolio\" target=\"_blank\">Porfolio</a> items are actually created like this.</p><p>Creating a new post type adds a whole new section to the left nav.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/c067c7f0-227b-4e6d-bae1-5140731832aa-image.png\" /><figcaption></figcaption></figure><p>In the GraphQL layer of Gatsby, I can easily query for this data to build pages out.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/49e85ad1-d7d2-440a-802c-6c96f5c3e685-image.png\" /><figcaption></figcaption></figure><p>The Advanced Custom Fields plugin lets me extend any type by adding my own fields. An example of this are my <a href=\"notion://www.notion.so/blog\" target=\"_blank\">Blog</a>posts where I have fields that define a custom index icon, YouTube video URL, and GitHub URL.</p><p>Here are the custom fields I have configured at the time of this writing.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/e75ecb95-6b53-4d2a-a4bc-573f5b571c8f-image.png\" /><figcaption></figcaption></figure><p>In the WordPress backend, I can configure these fields using the sidebar.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/fd20883b-94e1-418d-b7ab-0bef510aa149-image.jpeg\" /><figcaption></figcaption></figure><p>In the blog post, the YouTube embed is a component I built that renders the video based on that YouTube URL field from WordPress.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/39e3555a-1cd0-4952-bdc6-ebc3a92c626f-image.jpeg\" /><figcaption></figcaption></figure><p>Combining both of these plugins lets me do some pretty advanced stuff like how I created a custom taxonomy in WordPress for my Series definitions for blog posts using CPT UI. I also used ACF to add an Icon field to any Series that I can use to automatically display next to my posts on the blog page.</p><p>My 'Series' taxonomy I created.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/032b1eef-41b9-4907-b328-2812feb787e9-image.png\" /><figcaption></figcaption></figure><p>The custom fields configured for the series taxonomy.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/f3e2d1ab-bf28-4ba6-b7e2-4181e8ee0240-image.png\" /><figcaption></figcaption></figure><p>A series definition with that custom icon field.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/0dbdc263-c76d-44b1-926f-cde213ea3baa-image.png\" /><figcaption></figcaption></figure><p>The series data being queried in the GraphQL layer in Gatsby.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/541b8772-c662-4348-8451-4b8bab4f7bcc-image.png\" /><figcaption></figcaption></figure><p>You can see how I'm using the icon from the series data to render a custom icon for the blog post.</p><figure><img src=\"/img/n/porting-my-website-to-gatsby-wordpress/3d00d862-6a5e-4de2-93eb-d5c7ae844e32-image.png\" /><figcaption></figcaption></figure><p>The final thing I haven’t done is setup pages to be something that gets pulled into Gatsby so I can manage even that content in WordPress, but thats less of a priority.</p><h2>Migrating the Data</h2><p>So building stuff with new frameworks & infrastructure is fun and all, but one obvious thing that needs to be done that isn’t so fun/new is the data migration. I needed to get my posts & portfolio data from my custom CMS to WordPress.</p><p>What I ended up doing is writing a node app that can pull the data in from my CMS through the same API endpoints my Gridsome site did this with, restructure it a bit to be supported in WordPress, and use the WordPress REST API to post it all back. It took a few hours to get it all nailed down right (code snippets are surprisingly hard to work with like this) but it was generally a success. I also was able to figure out how to move my images from my CloudFront CDN to WordPress, which was also quite a challenge.</p><blockquote>Feel free to explore the code that performs all this magic</blockquote><h2>Netlify over AWS?</h2><p>Throughout this entire process, I opted to move my website from AWS to Netlify. In the old configuration, I had an S3 bucket behind CloudFront configured to serve up the site. The CI/CD pipeline was powered by Azure DevOps, which I prefer over most other DevOps platforms. I had two separate instances of my site, a staging environment & a production environment. The motivation behind doing this originally was to get more experienced with AWS. Now that I have GuardianForge in AWS using mostly the same process, this was just extra overhead for my personal blog.</p><p>With Netlify, I’ve simplified the deployment process by removing Azure DevOps entirely and relying on the automatic deployments that Netlify offers. Since Netlify uses AWS behind the scenes, I was also able to easily move my few Lambda Functions into Netlify Functions with almost no reconfiguration.</p><h2>Whats Next</h2><p>With my site entirely moved over, the main goal is to write more now that Im in a setup that I’m more comfortable with. I also play to open source my old site and CMS for research purposes, I just have a bit of sanitation to do before I can do that.</p><p>If you are interested in the source for this site, please feel free to check it out at <a href=\"https://github.com/bmorrisondev/brianmorrison.me\" target=\"_blank\">https://github.com/bmorrisondev/brianmorrison.me</a>.</p></div>","featuredImage":"/img/n/porting-my-website-to-gatsby-wordpress/4f9c5931-99ae-401b-a453-0fabd0aab022-featured-imag.png","excerpt":"Over the past few days, I decided to rebuild my website for the fourth time. The goal was to make it use Gatsby so I ...","cachedOn":1681394441},{"id":"61c7f563-30a5-4947-b592-5868424b29ad","notion_id":"61c7f563-30a5-4947-b592-5868424b29ad","status":"Published","slug":"using-go-with-api-gateway","relation_series":["c737786e-805c-4c6a-b356-10f6db2451b4"],"publishOn":"2022-04-19T00:00:00.000Z","codeURL":null,"seriesOrder":2,"youTubeURL":"https://youtu.be/m7-K8p0Hp90","title":"Using Go with API Gateway","html":"<div><p>In the previous entry of the series, we build a very basic AWS Lambda Function in Go & tested it using the AWS Console. In this one, we’ll expand on that by updating our function to support what’s called the API Gateway Proxy Event, as well as connecting API Gateway to the Lambda function & testing it using the VSCode Rest Client plugin.</p><div class=\"callout\"><div class=\"callout-icon\">🔗</div><div class=\"callout-content\">If you aren’t familiar with the VSCode Rest Client plugin, feel free to check out my video ‘</div></div><h2>Lambda & Events</h2><p>Before we get into the main portion of the article, I want to take a moment to explain how events & Lambda work. One of the best and most confusing things about Lambda is they are really designed to be agnostic as to what's executing them, meaning they really don’t care what service in AWS calls them.</p><p>Each service that calls a Lambda has a specific type of Event associated with it. For example, you can configure an AWS Lambda Function to trigger when a file is uploaded to S3 (a file hosting service in AWS) and the parameters that are passed into Lambda will be different than if API Gateway calls the function. In Go, the AWS SDK has a different struct designed for each caller of the Lambda function so you get a strongly typed object to work with when parsing data from the caller.</p><p>If this is confusing, it will make more sense as we progress in this article.</p><h2>Wiring up API Gateway</h2><p>Let's start in AWS by navigating to the API Gateway service and creating a new API. On the next screen, select the Build button in the HTTP API card.</p><figure><img src=\"/img/n/using-go-with-api-gateway/d05feecf-c2c1-4805-b853-13fec2d7a042-DraggedImage.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/using-go-with-api-gateway/6b4ff4a8-b925-4ea0-83c1-a911f32d75aa-DraggedImage-1.png\" /><figcaption></figcaption></figure><p>Now click Add Integration, select Lambda, and use the Lambda function box to search for the Lambda you created previously. Also make sure to give your API a name. Click Next.</p><figure><img src=\"/img/n/using-go-with-api-gateway/7ac677ad-76d4-4776-913f-76bfd3b46a76-DraggedImage-2.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/using-go-with-api-gateway/b463cd2f-189d-4004-b3fa-f738f78eda0f-DraggedImage-3.png\" /><figcaption></figcaption></figure><p>AWS will automatically create a resource path for you and default the Method to ANY. I just want to note that Method field. You have the option to create individual lambdas that are handled differently based on the incoming HTTP Method on a specific route/path, OR in this case pass all Methods into a single Lambda. I prefer the latter but it definitely gives you some flexibility if needed. You can click Next without changing anything.</p><figure><img src=\"/img/n/using-go-with-api-gateway/32818bf5-37ee-4059-92dd-28a8d57d8976-DraggedImage-4.png\" /><figcaption></figcaption></figure><p>You can click Next on this screen as well as we won't be configuring any additional stages in this tutorial. Finally, click Create on the last screen.</p><figure><img src=\"/img/n/using-go-with-api-gateway/8ac70350-049e-4f40-a953-f3d99b87b142-DraggedImage-5.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/using-go-with-api-gateway/45df623b-919c-48cb-99bf-c6d6901d0a5b-DraggedImage-6.png\" /><figcaption></figcaption></figure><p>In the following screen after the API Gateway instance has been created, you’ll notice there is an Invoke URL listed. Note that down as we’ll be using that to test our API using the VS Code Rest Client plugin.</p><figure><img src=\"/img/n/using-go-with-api-gateway/db1c352c-dcc8-4707-9e7d-1beca31f219e-DraggedImage-7.png\" /><figcaption></figcaption></figure><h2>Testing our Integration</h2><p>If you followed along from the previous article, we actually already setup our function to handle API Gateway Proxy Events as shown in the <code>request</code> parameter of our <code>handler</code> function. So at this point, we can simply execute a GET request to our endpoint. Simply take the route created earlier and append it to the Invoke URL to test it properly.</p><pre class=\"language-go\"><code>package main\n\nimport (\n    \"log\"\n\n    \"github.com/aws/aws-lambda-go/events\"\n    \"github.com/aws/aws-lambda-go/lambda\"\n)\n\nfunc main() {\n    lambda.Start(handler)\n}\n\nfunc handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) {\n    log.Println(\"Hello world\")\n\n    response := events.APIGatewayProxyResponse{\n        StatusCode: 200,\n    }\n\n    return response, nil\n}\n</code></pre><figure><img src=\"/img/n/using-go-with-api-gateway/399841d4-04d8-4c03-a1af-a7ac6607eb59-DraggedImage-8.png\" /><figcaption></figcaption></figure><p>We received a 200 response with a null body, which is perfectly fine since we didn't actually return any data from the function. Let's fix that now.</p><h2>Handling Incoming & Outgoing Data</h2><p>Most APIs don't just return a 200 without being able to handle some kind of input & output, so let's explore how we can update our function to both receive & respond with some JSON data. To handle incoming data, there is a field in the <code>request</code> object called, as expected, <code>Body</code>. Body is a string value which means we’ll need to marshal it to an object if we’re passing in some kind of data. First thing I’ll do is create struct called <code>Person</code> and add a few fields to it. You can add this anywhere in your <code>main.go</code> file.</p><pre class=\"language-go\"><code>type Person struct {\n    FirstName *string json:\"firstName\"\n    LastName  *string json:\"lastName\"\n}</code></pre><p>Now update the handler function to parse the body into an instance of the struct.</p><pre class=\"language-go\"><code>func handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) {\n    var person Person\n    err := json.Unmarshal([]byte(request.Body), &person)\n    if err != nil {\n        return events.APIGatewayProxyResponse{}, err\n    }\n\n    response := events.APIGatewayProxyResponse{\n        StatusCode: 200,\n    }\n\n    return response, nil\n}\n</code></pre><p>Let's also add a response struct that returns a message to the caller with the data passed in.</p><pre class=\"language-go\"><code>type ResponseBody struct {\n    Message *string json:\"message\"\n}</code></pre><p>Update the handler to create the response body object and add a message. You’ll also need to convert that object to a string before it can be returned. Once you do that, simply assign it to the <code>Body</code> field of the response object. Below is what the final version of the function should look like.</p><pre class=\"language-go\"><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n\n    \"github.com/aws/aws-lambda-go/events\"\n    \"github.com/aws/aws-lambda-go/lambda\"\n)\n\nfunc main() {\n    lambda.Start(handler)\n}\n\nfunc handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) {\n    var person Person\n    err := json.Unmarshal([]byte(request.Body), &person)\n    if err != nil {\n        return events.APIGatewayProxyResponse{}, err\n    }\n\n    msg := fmt.Sprintf(\"Hello %v %v\", *person.FirstName, *person.LastName)\n    responseBody := ResponseBody {\n        Message: &msg,\n    }\n    jbytes, err := json.Marshal(responseBody)\n    jstr := string(jbytes)\n\n    response := events.APIGatewayProxyResponse{\n        StatusCode: 200,\n        Body: jstr,\n    }\n\n    return response, nil\n}\n\ntype ResponseBody struct {\n    Message *string json:\"message\"\n}\n\ntype Person struct {\n    FirstName *string json:\"firstName\"\n    LastName  *string json:\"lastName\"\n}</code></pre><p>Now let’s deploy our function out by zipping it up and uploading it to AWS. Follow steps in the <a href=\"notion://www.notion.so/blog/your-first-aws-lambda-function-in-go\" target=\"_blank\">previous article</a> if you aren’t sure how to do that. Once done, create a POST request so you can send some data to the Lambda function.</p><figure><img src=\"/img/n/using-go-with-api-gateway/18f8c1dd-d9fb-477b-b7c2-2a16927bd6dd-DraggedImage-9.png\" /><figcaption></figcaption></figure><p>In the next entry of the series, we’ll explore how to use Cognito to implement serverless Authentication into the API.</p></div>","featuredImage":"/img/n/using-go-with-api-gateway/49ccf4f1-8b95-4202-8fbf-5ea8f541686e-background-1-2.jpg","excerpt":"In the previous entry of the series, we build a very basic AWS Lambda Function in Go & tested it using the AWS Consol...","cachedOn":1681394449},{"id":"21418bc0-e3bd-4e9d-8bed-1550c90751c9","notion_id":"21418bc0-e3bd-4e9d-8bed-1550c90751c9","status":"Published","slug":"your-first-aws-lambda-function-in-go","relation_series":["c737786e-805c-4c6a-b356-10f6db2451b4"],"publishOn":"2022-03-29T00:00:00.000Z","codeURL":null,"seriesOrder":1,"youTubeURL":"https://youtu.be/13rnse0zYK8","title":"Your First AWS Lambda Function in Go","html":"<div><p>This article kicks off a series on building a Serverless API in Go in AWS using Lambda, API Gateway, and AWS Cognito! In this one, we’ll start by creating a simple lambda function using AWS’s tooling for Go.</p><p>Before you can follow this, you should have a basic understanding of Go, as well as an AWS account. I’ll be using VSCode on a Windows workstation, but you can very easily replicate this on Mac or Linux. Lets dive in!</p><h2>Your First Go Lambda Function</h2><p>Lambda is an AWS Service that allows you to write and execute serverless functions. Along with Go, it supports most of the popular development languages out there. It is essentially the backbone of any serverless offering thats hosted on AWS.</p><p>Lambda Functions may seem daunting to build and write, but once you understand the basic requirements, they become very easy to put together and deploy! Let’s write a simple Hello World Lambda and deploy it.</p><p>In VSCode, open an empty folder on your machine. Then open the integrated terminal and create a new Go module with the following command.</p><pre class=\"language-plain text\"><code>go mod init hello-world-lambda\n</code></pre><p>You should have a single file named <strong>go.mod</strong>.</p><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/7be8156f-e3f7-425b-b75b-b6baa86bcfca-image.png\" /><figcaption></figcaption></figure><p>Now create your entry main.go file and populate it with the main function as usual. For good measure, add a log.Println method as well just to make sure we’re setup properly.</p><pre class=\"language-go\"><code>package main\n\nimport \"log\"\n\nfunc main() {\n    log.Println(\"hello world!\")\n}\n</code></pre><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/71406cf2-31d6-41bc-8f64-6ec36e5530e9-image.png\" /><figcaption></figcaption></figure><p>Before we continue, make sure you have the <strong>aws-go-sdk</strong> installed by running the following command in the terminal.</p><pre class=\"language-bash\"><code>go get -u github.com/aws/aws-lambda-go</code></pre><p>Now lets modify this to support Lambda, specifically the API Gateway Proxy Event which will be sent to our Lambda Function by API Gateway when we get to that portion of the series. Modify your code to match the following. Much of the code has been commented to describe what each section is doing. The vast majority of Lambda functions you find will generally start with this as a base.</p><pre class=\"language-go\"><code>package main\n\nimport (\n    \"log\"\n\n    // Importing our AWS Libraries\n    \"github.com/aws/aws-lambda-go/events\"\n    \"github.com/aws/aws-lambda-go/lambda\"\n)\n\nfunc main() {\n    // lambda.Start is how Lambda handles passing various events into your code\n    lambda.Start(handler)\n}\n\n// 'handler' will change the inputs & outputs depending on the type of event you trying to handle\nfunc handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) {\n    // Logging a simple message to the console, which will display in the AWS Console\n    log.Println(\"hello world!\")\n\n    // Creating our response with a status code of 200\n    response := events.APIGatewayProxyResponse{\n        StatusCode: 200,\n    }\n\n    // Finally, return the response with a nil error object to satisfy the outputs\n    return response, nil\n}</code></pre><h2>Deploying the Lambda Function</h2><p>To test, we’re actually going to package up our function and deploy it straight to AWS. From the AWS console, search for Lambda in the search bar and select it.</p><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/5a186ef5-37d4-4236-8f33-8fb278ae96f2-image.png\" /><figcaption></figcaption></figure><p>Once there, click on Create Function. Give your function a name and make sure to change the runtime to Go 1.x.</p><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/3fb73000-48d1-470f-b822-46b253681c02-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/5b033cd9-d40c-4563-a7b0-9d4a32a2a1db-image.png\" /><figcaption></figcaption></figure><p>Now since AWS runs these on Linux, we need to specify the system we are building for with Go instead of just accepting the default. Run the following in your terminal to build the function.</p><pre class=\"language-bash\"><code># Run this on Windows\n$Env:GOOS = \"linux\"\ngo build main.go\n\n# Run this on Mac\nGOOS=linux go build main.go\n</code></pre><p>You should get a single file added to your file system called <strong>main.</strong> This is the compiled version of our Lambda function. You’ll need to zip up the file before you can upload it. On Windows, you can right click the file, open the Send To menu, and select Compressed folder</p><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/294c5992-101b-49ba-9ece-8f8ff575862a-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/a82c8d35-789b-4775-af18-549406d43d85-image.png\" /><figcaption></figcaption></figure><p>You should have a <strong>main.zip</strong> that was created. Now back in AWS, click the Upload From button, select .zip file. Select the zip you just created.</p><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/a016d8d8-2e15-4680-a75f-74f4d606c9e5-image.png\" /><figcaption></figcaption></figure><p>Once its uploaded, you’ll need to change the Handler in the settings from <strong>hello</strong> to <strong>main</strong>. Click on Edit in the Runtime Settings section, then change the Handler to main in the next dialog and save it.</p><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/844dafe8-4d07-4801-b2db-00eddb55dfc2-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/609f3266-1f80-4598-80da-75b5f297f8e6-image.png\" /><figcaption></figcaption></figure><h2>Testing Our Work</h2><p>Now we can test the function. Head to the Test tab and simply click Test. You should have a green alert that shows up. Expand that to see the details of the execution. You can see we’ve returned that 200 status code, as well as see our <code>hello-world</code> in the logs below!</p><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/27d4ca44-1c55-4cbe-b964-3df80b87db11-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/your-first-aws-lambda-function-in-go/32524344-c249-4156-89b0-46a12bf2bd8b-image.png\" /><figcaption></figcaption></figure><p>In the next entry, I’ll show you how to wire up an API Gateway to Lambda functions and hit our functions using Postman!</p></div>","featuredImage":"/img/n/your-first-aws-lambda-function-in-go/66ddef0d-201b-42c9-9afc-50c576af272a-your-first-aws-lambda-function-in-go-featured-image.jpg","excerpt":"This article kicks off a series on building a Serverless API in Go in AWS using Lambda, API Gateway, and AWS Cognito!...","cachedOn":1681394451},{"id":"427d3bcf-a7b3-49af-bc0e-393e1e677734","notion_id":"427d3bcf-a7b3-49af-bc0e-393e1e677734","status":"Published","slug":"publish-your-own-go-package","relation_series":[],"publishOn":"2022-03-22T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":"https://youtu.be/KNHEXOoV-H4","title":"Publish Your Own Go Package","html":"<div><p>If you ever work on multiple projects, you likely have duplicated code between your projects to handle simple things. One example of this for me specifically is building Lambda Functions in Go. Instead of constantly returning an object that contains the standard API response (response code, body content, CORs headers, etc), I created my own Go package (<a href=\"https://github.com/bmorrisondev/go-utils\" target=\"_blank\">https://github.com/bmorrisondev/go-utils</a>) that I can refernce from any of my Go projects! Here’s how you can do the same.</p><p>I’ll be using a Windows machine with VSCode as my IDE throughout this tutorial. You may need to tweak things to fit your environment but the process is generally the same.</p><h2>Building A Package</h2><p>First thing you’ll need to know how to do is create a Go module. Go works by pulling in the source code from a public repository using Git tags to version the package. This means you dont need to worry about publishing to a third party package repository like NPM or Nuget. In this article, we’ll be publishing our libary to GitHub so you should create a package with your GitHub profile URL along with the specific package name. Lets create a simple logger that we can use to log messages out along with the type of log message (Info, Warning, Error).</p><p>At your project root, open a terminal and issue the command <code>go mod init github.com/{Your_GitHub_Username}/my-logger</code> , just make sure you replace <strong>Your_GitHub_Username</strong> with, well, your username. My username is <strong>bmorrisondev</strong>, so on my box it will be <strong>github.com/bmorrisondev/my-logger.</strong> You should have a <strong>go.mod</strong> file that looks like similar to this:</p><pre class=\"language-plain text\"><code>module github.com/bmorrisondev/my-logger\n\ngo 1.16</code></pre><p>Now create a file called <strong>main.go</strong> an put the following code in there:</p><pre class=\"language-go\"><code>package mylogger\n\nimport \"log\"\n\nfunc LogInfo(message string) {\n  log.Printf(\"INFO - %v\", message)\n}\n\nfunc LogWarning(message string) {\n  log.Printf(\"WARN - %v\", message)\n}\n\nfunc LogError(message string) {\n  log.Printf(\"ERROR - %v\", message)\n}\n</code></pre><p>We have three exported functions that simply prepend the log level to the message we pass in. This will be sufficient enough to demonstrate how to create packages. Now create a repository in GitHub that matches the name of your package, which is <strong>my-logger</strong> in this case. (Ignore the error message here, it only shows that because I made mine already.)</p><figure><img src=\"/img/n/publish-your-own-go-package/1a927ec6-2b7a-4ca5-afa9-6e2975f48460-image.png\" /><figcaption></figcaption></figure><p>Now back in the VSCode terminal, lets wire up our local project with the new GitHub repo we created. Here are the commands you can run in the terminal to do so.</p><pre class=\"language-bash\"><code>git init\ngit remote add origin https://github.com/bmorrisondev/my-logger.git</code></pre><p>Now lets commit our code & push it up to GitHub.</p><pre class=\"language-bash\"><code>git add .\ngit commit -m \"Initial commit\"\ngit push --set-upstream origin master</code></pre><p>Now here’s the trick, Go uses Git Tags to manage versions of your code. In order to tag our package as v1, issue the following command in the terminal to tag it and push the tags.</p><pre class=\"language-bash\"><code>git tag \"v1.0.0\"\ngit push --tags</code></pre><p>Lets head back to GitHub and make sure that our code is there and is properly tagged.</p><figure><img src=\"/img/n/publish-your-own-go-package/a3e177ed-7ed7-403a-8d5e-bd4ee8f07a9c-d4e54add-4df3-4fa7-bfc7-141a6bc889f6.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/publish-your-own-go-package/95b6d2d0-494b-4ec7-b505-7eff0711231a-image.png\" /><figcaption></figcaption></figure><h2>Using Your Package</h2><p>Now setup another Go project on your computer. We wont be pushing this one to GitHub, so no need to use a GitHub URL with the package name. I’ll name my package <strong>logger-testing.</strong> Then I’ll issue the command to install the package using the full project name we setup in the previous step.</p><pre class=\"language-bash\"><code>go mod init logger-testing\ngo get -u github.com/bmorrisondev/my-logger\n</code></pre><figure><img src=\"/img/n/publish-your-own-go-package/f170c822-4279-4b1f-b578-e39fa4867af3-image.png\" /><figcaption></figcaption></figure><p>Then I’ll create a <strong>main.go</strong> file again with the following code in it.</p><pre class=\"language-go\"><code>package main\n\nimport mylogger \"github.com/bmorrisondev/my-logger\"\n\nfunc main() {\n  mylogger.LogInfo(\"This is an info message!\")\n  mylogger.LogWarning(\"This is a warning message!\")\n  mylogger.LogError(\"This is an error message!\")\n}</code></pre><p>Now if we head back to the terminal and run the go file with <code>go run main.go</code>, you should see the following output.</p><figure><img src=\"/img/n/publish-your-own-go-package/ad3b033e-2bc8-4d19-b25b-aa582a7b2549-image.png\" /><figcaption></figcaption></figure><p>And there you have it! While this example was relatively simple, you can build your very own utility library to simplify things and share it across all your projects!</p></div>","featuredImage":"/img/n/publish-your-own-go-package/ad81708e-e901-4cdc-a38f-692368991a78-publish-your-own-go-package-featured-image-3.jpg","excerpt":"If you ever work on multiple projects, you likely have duplicated code between your projects to handle simple things....","cachedOn":1681394452},{"id":"9af6f7cb-ab01-4fa0-b696-47aeda9616b9","notion_id":"9af6f7cb-ab01-4fa0-b696-47aeda9616b9","status":"Published","slug":"create-your-own-vscode-snippets","relation_series":[],"publishOn":"2022-03-15T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":"https://youtu.be/s9p1MQJ8vUA","title":"Create Your Own VSCode Snippets","html":"<div><p>Snippets in VSCode are such a huge time saver. Whenever I am creating a file for a React application, all I have to do is type in <code>rfce</code> into my text editor to get a nicely constructed functional React component named exactly what the file. More recently, I’ve been creating a lot of AWS Lambda Functions with Go and wanted to do something similar as the structure of almost every Go Lambda is identical to start. This lead to the discovery of a plugin called Snippet Generator, which I’ll show you exactly how to use in this article.</p><h2>Installation</h2><p>Before we can use the plugin, we first need to install it. Open VSCode, head to Extensions, and type in Snippet Generator. You’re looking for the one by Wenfang Du. Once you find it, simply click install and you are ready to start creating your own snippets!</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/e8bc8749-6f81-420b-994e-a8e41fee48fa-image.png\" /><figcaption></figcaption></figure><h2>Creating a Snippet</h2><p>Snippet Generator works by highlighting some code and using it as the base for what you wnt to create. I’m going to run with the example I lead with, creating a simple Lambda Function in Go. So I’ve created a new file in VSCode and pasted the following code. This essentially gets me a basic Lambda function in Go that can handle any of the common HTTP methods using the LambdaRouter struct from my utils library.</p><pre class=\"language-go\"><code>package main\n\nimport (\n\t\"github.com/aws/aws-lambda-go/events\"\n\t\"github.com/aws/aws-lambda-go/lambda\"\n\tutils \"github.com/bmorrisondev/go-utils\"\n)\n\nfunc main() {\n\trouter := utils.LambdaRouter{\n\t\tGet: Get,\n\t}\n\n\tlambda.Start(router.Handler)\n}\n\nfunc Get(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) {\n\treturn utils.OkResponse(nil)\n}\n</code></pre><p>So I’ll highlight this code, open the command palette in VSCode by using Ctrl + Shift + P (Cmd + Shift + P on a Mac), and run Generate Snippet.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/f1f1771d-aa03-40ec-a202-4ab9efb2e7bb-image.png\" /><figcaption></figcaption></figure><p>There are a few things the plugin will ask us for, starting with the Name. This will be displayed in the intellisense box that shows up when we are typing.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/955638a5-f47f-46f1-a997-a42c3f5c288c-image.png\" /><figcaption></figcaption></figure><p>Next it will ask for the scope. This is optional, but you can type in the language or project you are working with to help VSCode only show it where you want to. Skip this if you are unsure what to put.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/bd7370b3-e8e8-4b2f-897d-8a05a1262793-image.png\" /><figcaption></figcaption></figure><p>Next it will ask for the shortcut we want to use to generate the snippet. Keep this one in mind as its what you'll need to remember to use the snippet.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/8aa836ce-1f28-47de-b4b3-b728c23d162c-image.png\" /><figcaption></figcaption></figure><p>Finally, feel free to enter a description if you like.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/81668870-8e41-45f6-8082-fbe905b17930-image.png\" /><figcaption></figcaption></figure><p>Once you are done, you should get a message in the lower right saying that the snippet was added to your clipboard.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/683d987c-97a4-459b-a4c6-c6d13cd57dd7-image.png\" /><figcaption></figcaption></figure><p>Now open your user-defined snippets by opening the command palette again and searching for Configure User Snippets. If you don’t have a file defined already, VSCode will ask you if you want to make one. I have one so I’ll open it here.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/74052f96-7dbd-4234-b056-5f2da081de96-image.png\" /><figcaption></figcaption></figure><p>If its a new file, you should see some generic helper text commented out. If not, it’s simply a JSON map used to store snippets.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/eeaa031d-d928-4e9f-beb5-ad8fa76db804-image.png\" /><figcaption></figcaption></figure><p>Remove all the comments and paste in your snippet. Make sure there are still a set of curly brackets surrounding the snippets though, don’t delete them with the comments. If you followed along, your file should like like this.</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/f268decf-34f1-498a-b8e8-f685a37e459d-image.png\" /><figcaption></figcaption></figure><h2>Using Your Snippet</h2><p>Honestly there isn’t much to this one, simply create a new file and start typing in that command you entered earlier (<code>go-lambda</code> in this case). VSCode should start listing out snippets that match what you are typing, hit enter once yours is selected and your file will be pre-populated with the snippet we created earlier!</p><figure><img src=\"/img/n/create-your-own-vscode-snippets/d93d64e6-1ae0-4b25-b047-ace221210502-7OdcXT9.gif\" /><figcaption></figcaption></figure><p>And there you have it! A super easy way to save yourself a BUNCH of time if you ever get into the business of creating the same kind of files over and over.</p></div>","featuredImage":"/img/n/create-your-own-vscode-snippets/03d142d8-f16a-4046-a9f7-93e4f1db60de-create-your-own-vscode-snippets-featured-image.jpg","excerpt":"Snippets in VSCode are such a huge time saver. Whenever I am creating a file for a React application, all I have to d...","cachedOn":1681394454},{"id":"02475b27-7044-4ac4-90a1-24df6c4effc4","notion_id":"02475b27-7044-4ac4-90a1-24df6c4effc4","status":"Published","slug":"setting-up-a-content-pipeline-in-notion","relation_series":[],"publishOn":"2022-03-04T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Setting Up a Content Pipeline in Notion","html":"<div><p>Over the past few years, I’ve tried to get serious on producing content on and off. I admit I’ve been seriously inconsistent when it comes to it, but I also know that other priorities generally take over so I dont beat myself up too much about it. Nevertheless, when I first got into making videos & writing articles, I had a decent system setup and this series is all about trying to get back to that place and increase the frequency of writing & recording.</p><h2>Where Im Coming From</h2><p>I want to start a little about what I had setup before and was able to get working so it makes more sense as this series goes on (and also to reframe in my mind what I want to get out of this.) Back in May 2022 right after everything locked down, I wanted to try recording videos for YouTube with some of the spare time I had. I recorded my first series on Coding Discord Bots in JavaScript, and it was received EXTREMELY well. I knew that the process I went through to build that series could be iterated on. The main gist goes like this:</p><ul><li>Brainstorm a series of content (both written & video versions)</li><li>Write the written versions first, while fixing any code issues as I went.</li><li>Publish the working code as a private GitHub repository (one per series entry).</li><li>Record the videos based off the article & code, essentially using the article as my script of sorts.</li><li>Coordinate the publishing of the Article, Video, and private repo all at the same time.</li></ul><figure><img src=\"/img/n/setting-up-a-content-pipeline-in-notion/bc2df45e-dbe6-423c-b879-38ba604706f0-image.png\" /><figcaption></figcaption></figure><p>I mostly did this manually for the first series but I wanted to make it more streamlined. I wrote some Node code that would help automate some of it like this:</p><ul><li>Use an Excel Spreadsheet to create my series outline, along with formulas to automatically come up with the article slug, GitHub repo name, and some other metadata info.</li><li>I wrote a Node script that would analyze that spreadsheet using the Office Graph API (I’m an Office 365 user) to create blank entries of the articles in my CMS, create a number of cards with checklists in Trello to work the pipeline, and create private repos based for each entry.</li><li>I also wrote a Node script that was run in a Docker container to poll my CMS for when an article was published and use the GitHub API to make that private repo public.</li></ul><p>Honestly I’ve fallen away from this process for quite some time, but as stated in the intro of this article, I want to start getting more consistent and automating a bunch of this process will be super helpful in removing much of the friction that comes along with planning & creating content. Overkill? Maybe, but welcome to how my mind works!</p><h2>The Plan</h2><p>My general plan is to take this old system and iterate on it with new tools & techniques. First off, I plan to use Notion for tracking most of this. I’ll also be creating a program with Go that will monitor Notion databases and trigger actions based on the data. These actions may be creating those blank articles in my CMS, perhaps migrating the draft from Notion into my CMS, creating the repositories I want to use, or setting up video/media templates in my OneDrive account.</p><h2>Setting Up a New Pipeline in Notion</h2><p>Notion has become my tool of choice for tracking most of my life. While I’m always on the look out for new tools to help track things, I often come back to Notion for its wide feature offering and relatively low cost (yes, I pay for premium). So here’s a rundown of the work I’ve done so far.</p><h3>Content Collections</h3><p>A Content Collection is my new term for a series of content, and if I am putting together any content that requires more that one article/video/whatever, this is where I start. It is really more for the planning phase. This series will be a bit different than others I plan to create since I’m building it out as I go.</p><p>Simplicity is key when starting a new process in my opinion, and I took that approach with this board.</p><figure><img src=\"/img/n/setting-up-a-content-pipeline-in-notion/4ab13d88-ecd4-493e-82f5-7a8b37739803-image.png\" /><figcaption></figcaption></figure><p>Any column I define in my systems need to have a meaning associated with them. Here is how I’ve broken these down:</p><ul><li>Planning - Collections I’m actively planning, meaning I’m working on defining the metadata associated with the content items (I’ll show that in a moment)</li><li>In Progress - The Collection moves here when I am done planning and want to start drafting articles or video outlines and such.</li><li>Scheduled - All of the content items are completed and scheduled to be released. Meaning entered into my CMS, uploaded to YouTube, and all code published to the associated GitHub repos.</li><li>Backlogged - Collections I may want to start at some point in the future. This will ideally be reviewed once per week.</li><li>Released - Its all done and out there.</li><li>Cancelled - I’m a bit of a data hoarder and dont like deleting things, so if I make the decision to NOT work on a collection, I’ll drop it here.</li></ul><p>Opening up the card thats on there shows the following. This is actually a Linked Database to the Content Items database which I’ll show in the next section. I like the table view as it gives me a good outline of every entry in the series I want to create. The Slug field is a formula based on the title which I can use to predict the URL that will be used for this article. GitHub URL is where the associated repo will be so my system can create it and publish it when the content goes live.</p><figure><img src=\"/img/n/setting-up-a-content-pipeline-in-notion/79facabf-60c6-450e-8a39-ee1a8985be55-image.png\" /><figcaption></figcaption></figure><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\">This brings me to my first automation point. When a series moves from Planning to In Progress, I will create any repos, drafts, and media templates and link them back to Notion for tracking. I also want to change the status of the content items to Selected to indicate the are ready to start on.</div></div><h3>Content Items</h3><p>A Content Item represents an article, video, or (maybe sometime soon) live stream I want to produce. Opening the first item from the database above shows the following. You’ll notice there are a few new fields here. Status is to track it from drafting to publishing, there is also a Type here to indicate what kind of item this is (I may use it to perform additional actions in that Go program), and some dates to help me plan.</p><p>I also have a checklist here that I can use to make sure that everything about this item lines up the way I want it to. These checklists are added automatically for me using Notion Templates (I’ll showcase my templates in the next article of this series).</p><figure><img src=\"/img/n/setting-up-a-content-pipeline-in-notion/93af2dc5-4f9b-4888-a22b-ee376add1ed9-image.png\" /><figcaption></figcaption></figure><p>Now I primarily use the Content Collections boards for planning, but I also have a Content Items board to track the progress of individual pieces of content through my system.</p><figure><img src=\"/img/n/setting-up-a-content-pipeline-in-notion/ac6e815a-d685-42a7-9366-fdd3e124cd4a-image.png\" /><figcaption></figcaption></figure><p>Again, I like defining my statuses, so here are what each of these mean:</p><ul><li>Selected - Planning is done and I’m ready to start drafting these items.</li><li>Draft In Progress - Actively working on this item.</li><li>Ready to Publish - The draft is done and ready to be published.</li></ul><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\">I plan to monitor this column to automatically migrate the draft into my CMS and setup the publish dates there.</div></div><ul><li>Idea - Ideas I have for individual articles/videos.</li><li>Published - Done and out there.</li><li>Rejected - Again, I dont like deleting things.</li></ul><h2>Whats Next</h2><p>My next entry in this series will outline the templates I’m using within Notion to define an article, video, both, or something else. I’ll also be publishing the code to my tool as I build it and documenting that process here!</p></div>","featuredImage":"/img/n/setting-up-a-content-pipeline-in-notion/89ee710e-ab67-4f1b-9458-d6f7d7c0497a-setting-up-a-content-pipeline-in-notion-featured-image.jpg","excerpt":"Over the past few years, I’ve tried to get serious on producing content on and off. I admit I’ve been seriously incon...","cachedOn":1681394456},{"id":"f0927809-4f1f-42e0-9695-239717f89b95","notion_id":"f0927809-4f1f-42e0-9695-239717f89b95","status":"Published","slug":"experiences-with-faunas-query-language-fql","relation_series":[],"publishOn":"2022-12-28T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Experiences with Fauna's Query Language (FQL)","html":"<div><p>So I’ve been in talks with Fauna recently for…reasons…and one of the recommendations throughout our conversations was to explore the product and build something with it. Now I’ve worked with Fauna in the past but I admittedly built a wrapper around FQL so I didn’t have fully learn and understand yet another query language.</p><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\">If you want to try out that wrapper, its a public repo that’s been published to NPM here: <a href=\"https://www.npmjs.com/package/@brianmmdev/faunaservice\" target=\"_blank\">@brianmmdev/faunaservice</a></div></div><p>What I ended up doing is rebuilding some of the database bits to call Fauna instead of GuardianForge. So I wanted to dive into the calls that I replaced, what they look like for querying DynamoDB, and the rebuilt queries in FQL. The backend is built with Go, so all examples for this will be in Go.</p><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\">To better understand GuardianForge and what it does, give this article a read:</div></div><p>Last thing I'll leave you with before diving in: I absolutely do not claim to be an expert in FQL yet, so I might have gotten some the explanations wrong. It's simply my understanding & observations by converting existing DynamoDB code into FQL.</p><h2>Fetch The Latest 15 Builds</h2><p>On the home page of GuardianForge is a list of the most recent builds for visitors to start exploring. To fetch this data in Dynamo, I have a Local Secondary Index created on the <code>publishedOn</code> field of the build record. This lets me perform range queries using the Unix time stamp representation of any date/time, and passing in a limit of 15 only grabs the most recent 15. This returns an array (slice) of Build Summaries, which contain all the meta information on a Build (the bulk of the build data is stored as a JSON in S3).</p><pre class=\"language-go\"><code>// dynamo.go\n\nfunc FetchLatestBuilds() ([]dbModels.BuildSummary, error) {\n  // Here I'm setting up the initial connection to Dynamo so I can use\n\ttableName := os.Getenv(\"TABLE_NAME\")\n\tcontext, err := db.MakeContext(nil, &tableName)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(FetchLatestBuilds) make context\")\n\t}\n\n  // Set a limit of 15 records to pull\n\tvar limit int64 = 15\n  // Scan from the bottom of the index (newest first)\n\tscanIndexForward := false\n\n\t// Build the query input parameters\n\tparams := &dynamodb.QueryInput{\n\t\tKeyConditions: map[string]*dynamodb.Condition{\n\t\t\t\"entityType\": {\n\t\t\t\tComparisonOperator: aws.String(\"EQ\"),\n\t\t\t\tAttributeValueList: []*dynamodb.AttributeValue{\n\t\t\t\t\t{\n\t\t\t\t\t\tS: aws.String(\"build\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n      // publishedOn is a unix timestamp in the db, so passing in\n      //   the current unix timestamp to filter out any builds\n      //   that go public in the future\n\t\t\t\"publishedOn\": {\n\t\t\t\tComparisonOperator: aws.String(\"LT\"),\n\t\t\t\tAttributeValueList: []*dynamodb.AttributeValue{\n\t\t\t\t\t{\n\t\t\t\t\t\tN: aws.String(fmt.Sprint(time.Now().Unix())),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n    // Here I'm specifying the index I want to scan, along with\n    //   the other params declared above\n\t\tIndexName:        aws.String(\"idx_publishedOn\"),\n\t\tLimit:            &limit,\n\t\tTableName:        context.TableName,\n\t\tScanIndexForward: &scanIndexForward,\n\t}\n\n\t// Make the DynamoDB Query API call\n\tresult, err := context.DynamoSvc.Query(params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsummaries := []dbModels.BuildSummary{}\n  // Loop over the items and convert them from Dynamo objects to\n  //   builds used through the system\n\tfor _, i := range result.Items {\n\t\tb := dbModels.Build{}\n\n\t\terr = dynamodbattribute.UnmarshalMap(i, &b)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"(getLatestBuilds) Unmarshalling dynmamo results\")\n\t\t}\n\t\ts, err := b.GetBuildSummary()\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"(getLatestBuilds) Get build summary\")\n\t\t}\n\t\tsummaries = append(summaries, *s)\n\t}\n\n\treturn summaries, nil\n}\n</code></pre><p>Let’s take a look at the Fauna equivalent now. I created a “builds” collection in Fauna to store the same data as in Dynamo. I also needed to create an index in Fauna to pull this off but what was confusing is that it doesn’t seem that the web UI has feature parity with the CLI, so I couldn’t create an index sorted on a data field using the UI. Luckily the web portal also has an area where I can write queries directly, which was very helpful. The following script is what I used to create that index:</p><pre class=\"language-go\"><code>CreateIndex({\n  name: \"builds_orderBy_publishedOn_desc2\",\n  source: Collection(\"builds\"),\n  values: [\n    { field: [\"data\", \"publishedOn\"] },\n    { field: [\"ref\"] }\n  ]\n})\n</code></pre><blockquote>Here is the article I used to help with this process: Sort with indexes</blockquote><p>Once that was in place, I could perform the same query like so.</p><pre class=\"language-go\"><code>// FaunaProvider.go\n\n// I needed to create a struct since all data in a Fauna collection\n//  is stored in the `data` field\ntype FaunaBuildRecord struct {\n\tBuild dbModels.Build `fauna:\"data\"`\n}\n\n// FaunaProvider is a type I created to hold the Fauna client\nfunc (fp *FaunaProvider) FetchLatestBuilds() ([]dbModels.BuildSummary, error) {\n  // Every query to Fauna is wrapped in this Query call\n\tresult, err := fp.Client.Query(\n    // Take is used to pull only the first N items in an array\n    // Map is used to take a set of data and pass it\n    //   into another function (Lambda in this case)\n\t\tf.Take(15, f.Map(\n      // Paginate is used to paginate records (I'm not sure this\n      //    needed to be honest)\n\t\t\tf.Paginate(\n        // Match is used make a comparison. Our index has no terms defined so\n        //   nothing needs to be matched\n\t\t\t\tf.Match(\n          // Index specifies the index we created in the earlier step.\n\t\t\t\t\tf.Index(\"builds_orderBy_publishedOn_desc\"),\n\t\t\t\t),\n\t\t\t),\n      // Lambda lets you perform operations with the data within Fauna\n      // Arr is pulling data from the Map function and defining\n      //    variables with the data that Fauna can work with\n      // Get & Var are used to pull the `ref` value of the item (similar to a\n      //    primary key or ID in a database) and get the actual document\n\t\t\tf.Lambda(f.Arr{\"publishedOn\", \"ref\"}, f.Get(f.Var(\"ref\"))),\n\t\t),\n\t\t))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(FaunaProvider.FetchLatestBuilds) Execute query\")\n\t}\n\n\t// Here Im parsing the data from the results into a struct I can work with.\n\tvar records []FaunaBuildRecord\n\terr = result.At(f.ObjKey(\"data\")).Get(&records)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(FaunaProvider.FetchLatestBuilds) result.Get\")\n\t}\n\n  // And here is where Im doing some final parsing on the data to make sure its good.\n\tsummaries := []dbModels.BuildSummary{}\n\tfor _, r := range records {\n\t\ts, err := r.Build.GetBuildSummary()\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"(FaunaProvider.FetchLatestBuilds) Get build summary\")\n\t\t}\n\t\tsummaries = append(summaries, *s)\n\t}\n\tjbytes, err := json.Marshal(summaries)\n\tif err != nil {\n    // Yes, I get creative with my errors in POC code 😅\n\t\treturn nil, errors.Wrap(err, \"asdlfkjasldkjf\")\n\t}\n\tstr := string(jbytes)\n\tlog.Println(str)\n\treturn summaries, nil\n}\n</code></pre><p>Here is the part that queries Fauna without the comments, just to see what it looks like.</p><pre class=\"language-go\"><code>\tresult, err := fp.Client.Query(\n\t\tf.Take(15, f.Map(\n\t\t\tf.Paginate(\n\t\t\t\tf.Match(\n\t\t\t\t\tf.Index(\"builds_orderBy_publishedOn_desc\"),\n\t\t\t\t),\n\t\t\t),\n\t\t\tf.Lambda(f.Arr{\"publishedOn\", \"ref\"}, f.Get(f.Var(\"ref\"))),\n\t\t),\n\t\t))\n</code></pre><p>Here are my final observations after the conversion:</p><ul><li>Although FQL is somewhat confusing when compared to working with Dynamo Query Params, the code itself is more concise when you remove all the comments I added (for better or worse).</li><li>I needed to define the sort order when creating the index in FQL, whereas the index in Dynamo can be sorted when executing a query. It doesn't seem like Fauna charges for more indexes, so this isn't too big of a deal in my opinion.</li><li>From my understanding, FQL executes all of the logic within the database engine, meaning you have more flexibility when querying the database, and queries can be optimized better to return only what is needed. This can be tricky to pull off in Dynamo as you generally have to fully understand your access patters to use Dynamo efficiently.</li></ul><h2>Saving a Build</h2><p>This is used to accept build data via the API and save it to the database. Here is the code for Dynamo. This one is relatively straightforward. Dynamo by default upserts data, so if it finds a record in the database that matches your unique composite key (partition key & sort key combined), it updates the data instead of creating a new record.</p><pre class=\"language-go\"><code>// dynamo.go\n\nfunc PutBuildToDynamo(awsSession *session.Session, buildRecord dbModels.Build) error {\n\tsvc := dynamodb.New(awsSession)\n\ttableName := os.Getenv(\"TABLE_NAME\")\n\tbuildRecord.EntityType = \"build\"\n\n  // Convert my Build struct into a Dynamo Item\n\titem, err := dynamodbattribute.MarshalMap(buildRecord)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(PutBuildToDynamo) Marshal map\")\n\t}\n\n\t// Save the item\n\tinput := &dynamodb.PutItemInput{\n\t\tItem:      item,\n\t\tTableName: aws.String(tableName),\n\t}\n\n\t_, err = svc.PutItem(input)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n</code></pre><p>Fauna was a bit more complex to mimic this logic. I needed to create another index to be able to fetch a build by its ID first. This is mainly because I already have an ID attribute for builds and I’m not using the native <code>Ref</code> object that identifies a document in Fauna. Once I had the index, I could use a the <code>If</code> function with <code>Exists</code> to check if the Build exists in the collection first, then decide to either create it or update it.</p><p>You’ll also notice we’re using <code>MatchTerm</code> instead of <code>Match</code>. I’m not sure if its a Go thing since MatchTerm is used to compare against another value, but Match seems to be used elsewhere regardless if you need to compare with a value or not.</p><pre class=\"language-go\"><code>// FaunaProvider.go\n\nfunc (fp *FaunaProvider) PutBuild(buildRecord dbModels.Build) error {\n\t_, err := fp.Client.Query(\n        // If lets you execute conditionals within the db engine using an (condition, true, false) syntax\n\t\tf.If(\n            // Check if the record with that Id exists\n\t\t\tf.Exists(f.MatchTerm(f.Index(\"builds_byBuildId3\"), buildRecord.Id)),\n            // If it does, run the Update function to update it\n\t\t\tf.Update(\n                // We need to run a Select to get the Ref so we can update it\n\t\t\t\tf.Select(\"ref\", f.Get(f.MatchTerm(f.Index(\"builds_byBuildId3\"), buildRecord.Id))),\n                // Replace the `data` field which holds all our data\n\t\t\t\tf.Obj{\n\t\t\t\t\t\"data\": buildRecord,\n\t\t\t\t},\n\t\t\t),\n            // If the record does not exist, Create it, passing in the Collection first, then Object second\n\t\t\tf.Create(\n\t\t\t\tf.Collection(\"builds\"),\n\t\t\t\tf.Obj{\n\t\t\t\t\t\"data\": buildRecord,\n\t\t\t\t},\n\t\t\t),\n\t\t),\n\t)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(FaunaProvider.PutBuild) Execute query\")\n\t}\n\treturn nil\n}\n</code></pre><h2>Fetch Build By Id</h2><p>Getting a single Build by ID is used all the time in GuardianForge. Here is the Dynamo code, relatively straightforward.</p><pre class=\"language-go\"><code>// dynamo.go\n\n\nfunc FetchBuildById(buildId string) (*dbModels.Build, error) {\n\ttableName := os.Getenv(\"TABLE_NAME\")\n\tcontext, err := db.MakeContext(nil, &tableName)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(FetchBuildsById) make context\")\n\t}\n\n\tparams := &dynamodb.GetItemInput{\n\t\tTableName: context.TableName,\n\t\tKey: map[string]*dynamodb.AttributeValue{\n      // entityType is my partition key, and its always `build` for a build\n\t\t\t\"entityType\": {\n\t\t\t\tS: aws.String(\"build\"),\n\t\t\t},\n      // entityId is the sort key, a simple GUID\n\t\t\t\"entityId\": {\n\t\t\t\tS: aws.String(buildId),\n\t\t\t},\n\t\t},\n\t}\n\n\tresult, err := context.DynamoSvc.GetItem(params)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(FetchBuildsById) GetItem\")\n\t}\n\n\trecord := dbModels.Build{}\n\terr = dynamodbattribute.UnmarshalMap(result.Item, &record)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(FetchBuildsById) Unmarshal Dynamo attribute\")\n\t}\n\n\treturn &record, nil\n}\n</code></pre><p>I needed to use an index again here because as I stated earlier, I’m using my own ID instead of a Fauna Ref. The key takeaway is the <code>MatchTerm</code> function only returned a Ref, and wrapping it in <code>Get</code> pulls the full document out of the collection.</p><pre class=\"language-go\"><code>// FaunaProvider.go\n\nfunc (fp *FaunaProvider) FetchBuildById(buildId string) (*dbModels.Build, error) {\n\tlog.Println(\"(FaunaProvider.FetchBuildById) start\")\n\tresult, err := fp.Client.Query(\n        // Get uses the Ref from MatchTerm to get the data in the record\n\t\tf.Get(\n\t\t\tf.MatchTerm(\n\t\t\t\tf.Index(\"builds_byBuildId3\"),\n\t\t\t\tbuildId,\n\t\t\t),\n\t\t),\n\t)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(FaunaProvider.FetchBuildById) Execute query\")\n\t}\n\n\tvar record FaunaBuildRecord\n\terr = result.Get(&record)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"(FaunaProvider.FetchByBuildId) Get\")\n\t}\n\tfmt.Println(record.Build)\n\n\treturn &record.Build, nil\n}\n</code></pre><h2>Delete a Build</h2><p>So far we’ve addressed all general database CRUD operations instead of delete, so just adding this for completeness. Again, its pretty straight forward for both Dynamo and Fauna.</p><p>Dynamo:</p><pre class=\"language-go\"><code>// dynamo.go\n\nfunc DeleteBuildFromDynamo(awsSession *session.Session, buildId string) error {\n\tsvc := dynamodb.New(awsSession)\n\ttableName := os.Getenv(\"TABLE_NAME\")\n\n\tinput := &dynamodb.DeleteItemInput{\n\t\tKey: map[string]*dynamodb.AttributeValue{\n\t\t\t\"entityType\": {\n\t\t\t\tS: aws.String(\"build\"),\n\t\t\t},\n\t\t\t\"entityId\": {\n\t\t\t\tS: aws.String(buildId),\n\t\t\t},\n\t\t},\n\t\tTableName: aws.String(tableName),\n\t}\n\n\t_, err := svc.DeleteItem(input)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(DeleteBuildFromDynamo) Delete Item\")\n\t}\n\n\treturn nil\n}\n</code></pre><p>Fauna:</p><pre class=\"language-go\"><code>// FaunaProvider.go\n\nfunc (fp *FaunaProvider) DeleteBuild(buildId string) error {\n\tlog.Println(\"(FaunaProvider.DeleteBuild) start\")\n\t_, err := fp.Client.Query(\n\t\tf.Delete(\n\t\t\tf.Select(\"ref\", f.Get(f.MatchTerm(f.Index(\"builds_byBuildId3\"), buildId))),\n\t\t),\n\t)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"(FaunaProvider.DeleteBuildFromDynamo) Execute query\")\n\t}\n\treturn nil\n}\n</code></pre><h2>Some Tips I’ve Gathered While Building for Fauna</h2><p>Here are some things I’ve learned along the way:</p><ul><li>Queries in general return an array of <code>Refs</code> and <code>Get</code> needs to be used to get the actual data.</li><li>Using <code>Map</code> and <code>Lambda</code> can help transform or filter data out BEFORE its returned to your code from Fauna’s engine.</li><li>Testing my queries in the Shell section of the web UI was incredibly helpful, and translated well into Go code.</li></ul><h2>My Final Thoughts</h2><p>So when comparing the way to access these two database systems, its pretty clear that they both have very distinct approaches. Fauna wraps much of the query into a set of nested functions, whereas Dynamo leverages objects & parameters passed into a small set of core functions.</p><p>I do like that Fauna seems to be more flexible than Dynamo while providing similar benefits, but you cant deny that the sheer scale of Dynamo and backing of Amazon makes it hard to beat from a support perspective. I don’t claim to be an expert in either, but I definitely will continue to use both going forward depending in the needs of the projects I'm building.</p></div>","featuredImage":"/img/n/experiences-with-faunas-query-language-fql/44e477ef-2014-448c-b11f-f6ad478a6496-experiences-with-faunas-query-language-fql-featured-image.jpg","excerpt":"So I’ve been in talks with Fauna recently for…reasons…and one of the recommendations throughout our conversations was...","cachedOn":1681394458},{"id":"859d9716-b9f5-4d20-ac7d-eb307eaf0c7b","notion_id":"859d9716-b9f5-4d20-ac7d-eb307eaf0c7b","status":"Published","slug":"finding-a-notion-databases-id","relation_series":["f6f6d8b3-97e2-429f-b337-7f53261c1733"],"publishOn":"2021-12-27T00:00:00.000Z","codeURL":null,"seriesOrder":1,"youTubeURL":"https://youtu.be/cFIZZHBaeH8","title":"Finding a Notion Database's ID","html":"<div><p>Notion opened up its API this year and writing code to work with databases is probably the thing I was most looking forward to. In order to work with any database though, you’ll need to find the ID of that database. You can use the Search API to find this, but there is an easier way if you know which database you want to work with.</p><p>To start, open Notion in your browser and navigate to the database in mind. Now if you look in your address bar, you’ll see IDs for both the database and the current view being displayed.</p><figure><img src=\"/img/n/finding-a-notion-databases-id/6e5a63d2-64e2-4b48-9bbe-fc6c0f2e0317-image.png\" /><figcaption></figcaption></figure><p>Note: If you are working with an inline database, click the icon to open the database as page, which will do exactly the same as above!</p><figure><img src=\"/img/n/finding-a-notion-databases-id/2b39f028-9c64-4ae7-8beb-9c345c8584b0-image.png\" /><figcaption></figcaption></figure></div>","featuredImage":"/img/n/finding-a-notion-databases-id/b94f6d87-f5ec-4ee8-8be2-8888004dd690-finding-a-notion-databases-id-featured-image.jpg","excerpt":"Notion opened up its API this year and writing code to work with databases is probably the thing I was most looking f...","cachedOn":1681394458},{"id":"dd98e28c-418a-4dd7-bd18-b42303f8b976","notion_id":"dd98e28c-418a-4dd7-bd18-b42303f8b976","status":"Published","slug":"goals-for-2022","relation_series":[],"publishOn":"2021-12-22T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Goals for 2022","html":"<div><p>Following up from my previous article where I looked back in 2021 on the various things I’ve accomplished, I also wanted to look ahead to the coming year and provide an outline of a few goals I want to achieve there too.</p><h2>Open Sourcing My Projects</h2><p>Like many developers, I use open source projects ALL THE TIME to simplify my life. Over the years, I’ve built numerous tools that have helped streamline my processes and I want to start looking to give back to the community in general through open sourcing these tools.</p><h3>DestinyDataTools</h3><p>When building GuardianForge, one of the most interesting struggles is trying to assemble the huge amount of data that is returned from the various APIs into something that usable. It was no small feat. Well I decided a few months after releasing Forge that I wanted to build an NPM package to simplify this process, thus DestinyDataTools (the name might change) was born. Its essentially a wrapper around the data that makes working with the data much easier, at least in my opinion. Here are some screenshots of it in action for the upcoming version of GuardianForge. (Please note that this is all very early in the development process, naming is likely to change before I push the first release)</p><p>These few lines handle the process of fetching a users’ inventory and building out data structures within the managing class.</p><figure><img src=\"/img/n/goals-for-2022/797312fc-cc0a-4db6-81c9-ac9168f2a53e-image.png\" /><figcaption></figcaption></figure><p>The <code>Item</code> object is the main entity, here are some of the methods that can be used to easily pull useful info for that item.</p><figure><img src=\"/img/n/goals-for-2022/cf8a367e-5377-4419-bd08-f0d385c0faf1-image.png\" /><figcaption></figcaption></figure><p>And finally, here are a few screenshots of what this is helping me do for Forge.</p><figure><img src=\"/img/n/goals-for-2022/eb3e2d22-dfca-4a33-b0d4-848dd8032b27-image.png\" /><figcaption></figcaption></figure><h3>GuardianForge</h3><figure><img src=\"/img/n/goals-for-2022/3c30b904-5b3d-4a7a-9531-878a25dc26f1-image.png\" /><figcaption></figcaption></figure><p>When I started building Forge, I made it closed source mostly because I had a ton of secrets and API keys that I was working with (yeah, I know, I know...) but I’d love to open it up so other developers can see how it was built and hopefully use it as a reference when building their own third party companion apps for Destiny. I just need to take time to comb through all the source, as well reconfigure my own workspace so its as productive as it was before.</p><h3>Quick Capture Tool</h3><figure><img src=\"/img/n/goals-for-2022/894b14fe-aa24-4f78-8e5d-05d9c39221ed-image.png\" /><figcaption></figcaption></figure><p>One of my secret productivity weapons. Many years ago I built an electron app with Vue that hooks into Trello via their API and lets me quickly add cards using global hotkeys. This way, if I want to quickly add myself a task, note, whatever, I can just press Ctrl+Alt+O and it gives me a quick dialog box to enter something. Click Enter and it disappears into the background, adding that note to my board. The beauty of it is I can swap the backend service out with anything that has an API. My goal is to update it for using with Notion then put it out for the world to see.</p><h3>My Custom CMS - BMOPS</h3><figure><img src=\"/img/n/goals-for-2022/2faaa23b-4431-4a0e-9778-aaa9b8c673df-image.png\" /><figcaption></figcaption></figure><p>When I built the current version of my website, I decided to build my own CMS which I wanted to do for years. There were three features I really wanted: A markdown editor, the ability to drop or paste images and have them upload to my CDN, and an API to access the data programatically. Thats BMOPS in a nutshell, with a backend on AWS and a frontend in Netlify. Oh, and that silly name stands for Brian Morrison’s Operations, because us techies love our abbreviations 😎</p><h2>Content</h2><p>My consistency with making content took a hit this year with some stuff going on personally, but I want to get back into a more consistent routine with creating content, with the initial focus on once every other week. Here are a few things you can expect from me in 2022 in this area. In a nutshell, it will mostly be content on how to use code to make life & business more productive.</p><h3>Focusing on Integrations</h3><p>Projects that integrate multiple platforms together are probably some of my favorite to work with. I got into development because I enjoyed automating things, so my goal for a chunk of my content going into 2022 is to focus on finding various platforms that with APIs, Webhooks, or other integration-type setups and create guides around them.</p><h3>Low/No Code</h3><p>On the topic of automation, sometimes low or no code platforms may be the easiest way to accomplish something, so I plan to explore these platforms to see what value they can provide in integrating other services together.</p><h3>Productivity & Project Management</h3><p>One of my other passions in life. I really strive to find tools & processes to simplify my life as much as possible. This will very likely be centered on Notion, but may branch out to other supporting tools I use in my daily work.</p><h3>A Go/AWS Book</h3><p>I started writing a book a month or so ago and I’m a few chapters in so far. Unfortunately the holidays and other properties have taken over so I’m currently planning on launching this sometime in Q1 2022. Here’s a sneak peek of the tentative chapter outline:</p><ul><li>Preface</li><li>1 - Getting Setup in AWS</li><li>2 - Serverless Functions w/Lambda</li><li>3 - Building APIs with API Gateway</li><li>4 - Serverless Auth using Cognito</li><li>5 - Working with DynamoDB</li><li>6 - File Management using S3</li><li>7 - Message Queueing with SQS</li><li>8 - Putting It All Together with AWS SAM</li></ul><h3>Destiny Content</h3><p>Honestly, Im on the fence on this one. When GuardianForge started getting some traction, one of the things I’ve had on my list is to create build videos that leverage the builds in Forge as a base. As much as I’d love to do this, it may be overstretching a bit, and I’m trying NOT to burn out again.</p><h2>Other Stuff</h2><h3>AWS Professional Certification</h3><figure><img src=\"/img/n/goals-for-2022/41fb5e4b-f0ec-428b-b877-a987bd19a70c-image.png\" /><figcaption></figcaption></figure><p>Hard to believe I first took my AWS certs 3 years ago, but they are coming due this year so instead of re-certifying my Associate level certificates, I’m instead going to obtain the Professional Solutions Architect certification. I’m considering going for the DevOps cert as well, but it might be a bit too much on my plate due to the following goal.</p><h3>PMP Disciplined Agile Certification</h3><figure><img src=\"/img/n/goals-for-2022/957e6c4d-dc53-4bdf-97c7-a1bc2e0432a6-image.png\" /><figcaption></figcaption></figure><p>As mentioned earlier, project management is another interest of mine and I’d love to get more practical knowledge on how to properly run projects. I may at some point transition my career fully into that area, but at the very least it is complimentary to my goals of working with backend code & architecture.</p><h3>Health & Fitness</h3><p>Many people don’t know this, but at one point in my life I was able to claim I’d lost over 100lbs, going from 280 → 175 in my late teens. Since then, maintaining a healthy lifestyle has been important to me. So much so that I actually wanted to get into bodybuilding as a career before I got into tech! Well the lockdown/isolation/covid stuff made me more complacent than I’ve been in a long time, and its had some negative effects on my overall health. My main goal this year is really just to optimize my health as much as possible, with short term goals of losing 30ish lbs and getting my cholesterol within healthy ranges. Its not an extreme amount but it certainly will be challenging, especially now that Im much older than the first time I’ve tried to get my health nailed down.</p><h3>Some Kind of Humanitarian Project</h3><p>I know that I am very blessed to have the resources and knowledge I do to grow myself, and there are so many people in this world that suffer for absolutely no reason. I dont even know what to put in this category, but I’d love to find some projects that I can collaborate with to reduce the pain that our fellow humans experience every single day.</p><blockquote>If you have a project along these lines, please (!) DM me on Twitter @brianmmdev and we can talk.</blockquote><h2>Moving On</h2><p>So now that I have SOME of my ambitions for 2022 laid out (less some other personal ones), I plan to follow this up with the final entry in the series on how I’m going to map these goals out into 2022.</p></div>","featuredImage":"/img/n/goals-for-2022/ce3ca475-baa8-45e1-af9a-77b1f8e06b4f-goals-for-2022-featured-image.jpg","excerpt":"Following up from my previous article where I looked back in 2021 on the various things I’ve accomplished, I also wan...","cachedOn":1681394460},{"id":"c02755fb-d33b-4f7c-a367-327896e50ac9","notion_id":"c02755fb-d33b-4f7c-a367-327896e50ac9","status":"Published","slug":"reflecting-on-2021","relation_series":[],"publishOn":"2021-12-21T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Reflecting on 2021","html":"<div><p>Its hard to believe that 2021 is nearly behind us already! As we get nearer to the end of the year, I’ve been thinking about what I’ve accomplished over the last year, as well as things I plan to knock out in 2022 (to be detailed in a separate post). I’m going to keep most of these to one paragraph, but hit me up on Twitter if you are interested in a deeper dive.</p><p>Honestly when I started thinking about 2021, it really felt like a rough year. The second year of lockdown from the pandemic (isolation really got to me this year), actually getting COVID earlier this year, the constant struggle of a family of boys growing up way too fast, and having been dealt a blow to my personal health recently got me down. But as I’m listing things I’ve accomplished this year, it definitely wasn’t all bad.</p><h2>Personal Projects</h2><h3>GuardianForge</h3><figure><img src=\"/img/n/reflecting-on-2021/bba82a3b-6af0-4b82-b682-2ed4cfa88bd3-image.png\" /><figcaption></figcaption></figure><p>Starting with the big one, <a href=\"https://guardianforge.net/\" target=\"_blank\">GuardianForge</a>. I’ve been a long time player of Destiny 2 and got into build crafting earlier in the year. I conceived the idea of Forge as a way for content creators to make simple links to their builds they can share with their audiences. The best moment is when it got shared by the developer of Destiny Insights on Twitter. Through that experience, I was able to get a decent user base to bounce ideas off of to grow the product. It was also my very first public-facing application I’ve built under my own name. Stack uses a combination of Go and JavaScript and is hosted in AWS. Many of my ideas I have for the app are still on the backlog due to lack of time, but lots of new things coming next year!</p><h3>Daily Reps</h3><figure><img src=\"/img/n/reflecting-on-2021/b06e8728-5b55-49b5-a83a-880412455bdc-image.png\" /><figcaption></figcaption></figure><p>So this is something I haven’t been so vocal about. I wanted a simple way to track reps of certain exercises I do throughout the day just to see how I progress over time. After scouring the App Store and turning up nothing, I decided to just build something myself. Basically I can log a simple number and it resets each day. Currently using Gatsby, Netlify, and Fauna.</p><h3>Twitter/Notion Bot</h3><figure><img src=\"/img/n/reflecting-on-2021/b183189a-46ab-4853-9762-66762aec2e9b-image.png\" /><figcaption></figcaption></figure><p>Still in development, but I built myself a bot to schedule Tweets using Notion as a backend. It’s written entirely in Go and running on my Docker server. The idea is I have a Notion table where I can store tweets for later around my content or whatever and I can defer sending them until a certain time. I also plan to use this to schedule tweets from the Forge account. I’ll eventually open source it when I get time.</p><h2>Work Stuff</h2><h3>Building a Go GraphQL API for Dynamic Asset Groups</h3><p>Assets are the core entity of our app at work, and one of the things thats been on the backlog since I started with the company over 2 years ago was to build a way to dynamically update groups of assets based on data fields. I made that a reality by building a GraphQL API in Go, and the necessary UI forms to build queries that can be run at intervals. Since GraphQL is essentially a HTTP endpoint that uses JSON, I simply had to create the necessary query strings and store them in a DB for later use. It was one of my favorite projects of the year hands down.</p><h3>A Custom OpenID Integration</h3><p>TECHNICALLY this one is still in the works, but essentially I’m building a fully custom SSO implementation in the work app. It was interesting learning about the various SSO protocols (mainly OpenID and SAML) and learning how to properly implement them. This allows our users to sign into our application using their own ID server, as well as our support team to map access groups across the systems.</p><h3>A Newsfeed Powered by WordPress</h3><p>We use WordPress as our marketing site, and the request was specifically to setup a way to notify our users when something new is added to the system. We built a system that pulls data from our WordPress site and renders it in our app using the WordPress REST API, along with ACF for custom data fields. This was mostly inspired by building previous iterations on my own blog. The nice thing is it allows a really nice WYSIWYG editor to be used by non technical people to show things within our app to our users.</p><h2>Other Accomplishments</h2><h3>Year 2 AWS Community Builder</h3><figure><img src=\"/img/n/reflecting-on-2021/8a2ed0e2-3439-439a-9971-4eae2e39545a-image.png\" /><figcaption></figcaption></figure><p>I was approved for a second year as AWS Community Builder. Honored to be a part of the program again 😊</p><h3>Lead Backend Engineer</h3><p>Earlier this year I decided I wanted to shift my career away from full stack development and more towards backend development. I have a pretty big background in infrastructure engineering and I realized I prefer projects where Im making systems talk over making them pretty. Recently a coworker of mine landed another opportunity and when I mentioned to my boss where I’d like to see my career go, without hesitation he gave me the title of Lead Backend Engineer!</p><h3>Celebrating 8 Years Married</h3><figure><img src=\"/img/n/reflecting-on-2021/f8c135f4-d8be-451e-8432-fb6917dcb29e-image.png\" /><figcaption></figcaption></figure><p>Hard to believe I’ve been married to the love of my life for 8 years, and 14 years together. Life has certainly dealt us some strange cards over the years, but we’ve never been stronger as a couple and I am very grateful for that.</p><h2>Wrapping it Up</h2><p>I feel like I’m missing a few things but these are the ones that came to mind. Expect another article soon outlining my goals for 2022!</p></div>","featuredImage":"/img/n/reflecting-on-2021/16127100-ea59-4067-9179-f581bdf1e5d7-reflecting-on-2021-featured-image.jpg","excerpt":"Its hard to believe that 2021 is nearly behind us already! As we get nearer to the end of the year, I’ve been thinkin...","cachedOn":1681394461},{"id":"0901f7be-8777-47c8-a191-55bc930c66d6","notion_id":"0901f7be-8777-47c8-a191-55bc930c66d6","status":"Published","slug":"my-personal-to-do-process","relation_series":[],"publishOn":"2021-12-02T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"My Personal To Do Process","html":"<div><p>One of the things Im pretty into besides code is personal productivity and project management. Over the recent Thanksgiving holiday, I purposely stayed away from focusing too much on work & side projects, but one thing that I got inspired to do was re-evaluate my current process of managing tasks and projects, and figure out how I can do better. This article is the outline of the results of that process.</p><h2>A GTD Primer</h2><p>So I'm a pretty big fan of the Getting Things Done (aka GTD) process outlined by David Allen in his iconic, and appropriately named, book. I highly recommend giving it a read as it can help give pointers to anyone who is interested in this sorta thing: <a href=\"https://amzn.to/31mp3qo\" target=\"_blank\">Getting Things Done</a>.</p><p>For those of you who haven't read it, there are 5 distinct phases of managing tasks:</p><ul><li><strong>Collect</strong> - The process of capturing data, be it tasks, notes, etc.</li><li><strong>Process</strong> - Review everything you've collected and decide what actions are needed for it.</li><li><strong>Organize</strong> - Organize your tasks into \"Contexts\", physical areas of life where those actions can be done.</li><li><strong>Review</strong> - Review your ENTIRE system at least weekly and keep it up to date.</li><li><strong>Do</strong> - Get sh** done!</li></ul><p>I'll use these phases to describe the tools I use, how they are used, and why I chose those tools. All that said, my process is a slightly modified version of the whole process. Its worth noting I recently went all in on the Apple ecosystem, so much of this article will be from that standpoint.</p><h2>Collect</h2><p>Collecting is one of the most important parts of making this whole thing work. Having a well defined collection process is critical! The easier capturing is, the better you'll be at doing it. Here are the tools I use for this process:</p><h3>Microsoft To Do</h3><p>I use To Do over other task management for two reasons. Firstly, I really like the way it looks, and I use it as the primary tool for managing everything. I think if you are going to use a tool on a regular basis, its important to enjoy the process of actually using it. Secondly, the way it allows me to force touch (or whatever its called) on my phone to get a quick menu item to add a task is the best Ive used so far. Every task captured in that way goes to a default \"Tasks\" list, which acts as an inbox for my tasks.</p><figure><img src=\"/img/n/my-personal-to-do-process/16ed8d56-4b8a-4ee8-abdd-8c20a4f9bfb4-image.png\" /><figcaption></figcaption></figure><p>Edit: I forgot to add that because To Do is backed by Exchange, and I use Office 365 Small Business for my personal stuff, it actually allows me to use Siri on my iPhone to add notes as well. So saying something like \"Remind me to take out the garbage\" will add \"Take out the garbage\" to my Tasks list, which is really helpful if I don't want to take the time to stop and type it myself.</p><h3>Apple Notes</h3><p>To Do is great for individual tasks, but if I want to quickly snap a picture or scan a document, Apple Notes does a better job in that area. It also is very tightly integrated into all Apple devices for quick notes, such as just tapping the lock screen with my Apple Pencil on the iPad to start jotting down notes. While this does add an extra inbox for me to process, it doesn't capture too much so cleaning it out is pretty easy.</p><figure><img src=\"/img/n/my-personal-to-do-process/80d7928c-63ce-45d8-8f3d-2e7dbfe7539a-image.png\" /><figcaption></figcaption></figure><h3>Pocket + Microsoft Power Automate</h3><p>This one is a bit nerdier of a solution. Pocket is a browser extension that lets you quickly grab a web page and store it in their system. It also has the smoothest URL share sheet on iOS devices, less taps = faster capture!</p><p>Power Automate is a no-code solution for building quick and dirty integrations, very similar to IFTTT if you've ever used that. Since I use the Office 365 suite, its included with my subscription, although I think there is a free version that can be used by anyone. I use Power Automate to grab any new items from Pocket and add them to Microsoft To Do.</p><figure><img src=\"/img/n/my-personal-to-do-process/5ca9a507-a74e-4706-ba98-eac3e6789f65-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/my-personal-to-do-process/a9b6172d-0450-40ce-979b-7623ce667fbe-image.png\" /><figcaption></figcaption></figure><h3>Other Inboxes</h3><p>For the sake of completion, I also wanted to list other inboxes I use that are pretty common across all professionals:</p><ul><li>Email (Work & Personal)</li><li>Text Messages</li><li>Voicemail</li><li>Physical notebook</li><li>Physical inbox (paper tray)</li></ul><h2>Process/Organize</h2><p>In traditional GTD, these are two different phases, but I often organize as I'm processing my notes. The important bit is that every item within my inboxes fall into one of three categories:</p><ul><li><strong>Next Action</strong> - Tasks that can be done with no dependancies.</li><li><strong>Projects</strong> - Multi-step actions, even the simplest of things.</li><li><strong>Reference Material</strong> - Stuff I want to save for use in a project, or just for easily pulling back.</li></ul><p>I try to process all my collected data at least every other day depending on how busy I am. I also follow the 2 Minute Rule, which states that if an action within an inbox can be marked as done in 2 minutes or less, just do it instead of processing it farther into the system. So my processing process often looks like this:</p><ul><li>Review every item in ALL my inboxes</li><li>Ask if the item falls into each of the above categories</li></ul><p>My physical items take a bit more work to process, but in general I do the same thing. I only other things I do for physical items is scan the document, save it into my OneDrive account for archiving, and file it in my file cabinet.</p><h3>Contexts in To Do</h3><p>In To Do, I have a number of lists that are categorized for how I work.</p><figure><img src=\"/img/n/my-personal-to-do-process/a0ba336f-1a3f-4262-ba8f-2bb9dc455073-image.png\" /><figcaption></figcaption></figure><ul><li>Tasks - Tasks acts as my inbox, its where all the new stuff ends up</li><li>Waiting For - If I'm waiting on someone to get back to me, I'll write it here so I can review it each day.</li><li>Blocked - If I'm waiting on a prior task to complete, or a certain date to come, I'll add those tasks here.</li><li>Computer - Work - Work stuff that I can do at my computer</li><li>Computer - Personal - My personal stuff I can do at my computer</li><li>Home - Stuff I need to do around the house</li><li>Web R&D - Stuff I eventually want to research & look into</li><li>Downloads - Stuff I need to download and archive off</li><li>Errands - Stuff I need to do while Im out & about</li><li>Phone - Calls & texts I need to make</li><li>Agendas - Stuff that relate to a specific individual I need to meet with</li><li>Destiny - Things I want to do in Destiny (yes, nerdy gamer...)</li><li>Explore Tech - New platforms, frameworks, tools, etc that I want to explore whenever I have time</li><li>Someday/Maybe - Things that don't have any action yet, but I might want to do some day</li><li>Read/Review - Articles I eventually want to read, but aren't critical to any projects Im working on</li><li>Kaizen - A list of personal improvements I can make</li></ul><p>This is only my current set of lists, these change quite frequently as I see better ways to implement them and keep myself organized.</p><h3>Notion</h3><p>The new tool I mentioned in this phase is Notion. Notion is an all-in-one workspace type tool. You can use it to create simple pages that act as notes, or nest pages within databases & kanban boards. Notion is super flexible and is very easy to work with, which is why I use it for planning & tracking projects and storing reference material. If you've followed GTD for any length of time, you know that the project list is crucial for keeping your systems organized. This is where I hold mine because each page can store any amount of structured or unstructured data I need as it relates to each project.</p><figure><img src=\"/img/n/my-personal-to-do-process/2946cd60-c155-4c67-8c76-7717d7bb298b-image.png\" /><figcaption></figcaption></figure><p>I also have my own personal knowledgebase here for any data I collect that may not directly relate to a project, but is something useful that I might want to call back at a later time. One example of this would be like a tutorial on how to build something that I read online. Ill often save a link and copy/paste the content of that article so I can use Notions search tools to find it.</p><blockquote>If you are interested in building your own personal KB, I did a livestream on my process a while back: Building a Personal Knowledgebase with Notion</blockquote><figure><img src=\"/img/n/my-personal-to-do-process/a5523490-5ac1-417b-887d-24addfdbaac2-image.png\" /><figcaption></figcaption></figure><h2>Review</h2><p>The Review phase is super important to keep things up to date, but I honestly dont have any magic here. I do mine weekly, which is what is usually recommended. At this point, the assumption is that all of my various inboxes have been processed & organized, so this is just an opportunity to think through my projects in depth. I work in a hybrid digital/paper-based system, so often my weekly reviews will often look something like this:</p><ul><li>Get coffee (!important)</li><li>Pull up my project list in Notion</li><li>Pull out my physical notebook.</li><li>For every project, create a list of new tasks that I need to work on throughout the next week.</li></ul><p>Once Im finished, that sheet goes into my physical inbox for processing. But as long as I know its there, I dont have to stress that I might be missing stuff.</p><h2>Do</h2><p>This is where my system REALLY deviates from GTD. In GTD, the big idea is that instead of creating daily/weekly/monthly to-do lists, you have a giant list of next actions, and any of them can be done at any time provided you are in the context to do them. Since 90% of my work is done on the computer, I often have MASSIVE next action lists and I get some sort of weird paralysis when I see all those things that I need to get done.</p><p>In this latest system, I've done something that seems work really well for me. At the end of every day, I'll scan my next actions list and pick up to 20 next actions that I want to try and knock out for the next day. I specifically limit myself to 20 because MY next actions, while usually quick and simple things, often have other actions embedded in them that I dont quite see yet. I can be pretty bad at thinking my things all the way through 😅.</p><p>In To Do, there is a special list called My Day. You can add any action to My Day by simply clicking the Add to My Day when looking at a task. On the phone, you can also do this by swiping right on the task and clicking the icon there, so its very easy to plan my day out.</p><figure><img src=\"/img/n/my-personal-to-do-process/455db7a8-795a-46f2-9c8a-be34cd864e79-image.png\" /><figcaption></figcaption></figure><p>This was a huge help to me specifically in keeping myself focused on the stuff I know I need to get done.</p><h2>What Do You Do?</h2><p>This really is just what I've found works well for me. My system is not perfect, and I know everyone has their own way of managing tasks. I'd love to hear what others are doing! Hit me up on Twitter <a href=\"https://twitter.com/brianmmdev\" target=\"_blank\">@brianmmdev</a> and let me know what your workflow looks like.</p></div>","featuredImage":"/img/n/my-personal-to-do-process/4efeed97-deb7-4906-a396-9671b6d1255b-my-personal-to-do-process-featured-image.jpg","excerpt":"One of the things Im pretty into besides code is personal productivity and project management. Over the recent Thanks...","cachedOn":1681394464},{"id":"4c020957-cb55-4508-82e7-6e5c06555142","notion_id":"4c020957-cb55-4508-82e7-6e5c06555142","status":"Published","slug":"how-i-use-dynamodb-for-guardianforge","relation_series":["22756350-048e-4423-8948-233370c99f6e"],"publishOn":"2021-08-23T00:00:00.000Z","codeURL":"https://github.com/guardianforge/guardianforge.net","seriesOrder":null,"youTubeURL":null,"title":"How I Use DynamoDB for GuardianForge","html":"<div><p>In my last article, I covered a big chunk of the planning & discovery I did when building the first version of GuardianForge. As a quick recap, my backend at this phase of the project leveraged DynamoDB for storing information about builds, S3 for storing the JSON object that makes up a build, and Lightsail to host the API written in Go. In this one, I'm going to dive into more into detail with how I'm using DynamoDB.</p><p>Unless otherwise specified, code in this article should be considered \"pseudo-code\" and will likely not work in any specific language.</p><h2>Accessing Data in DynamoDB</h2><p>DynamoDB is a NoSQL solution in AWS. Its designed for scalability & performance, but structuring your data to properly take advantage of these capabilities is critical. Before I even considered how I wanted to store my data, I first needed to understand what I needed to store and how I wanted that data to be accessed. I started by creating a list of access patterns that I knew I would need;</p><ul><li>Get a single build.</li><li>Get the latest N builds.</li><li>Get all builds for a user.</li><li>Get all builds that a user has saved.</li><li>Get a user by their ID.</li></ul><h2>Understanding Keys</h2><p>Just like every database system, there needs to be some way of uniquely identifying a record. In a traditional, relational SQL database, this would be the primary key. In Dynamo, you have two attributes which can be used to make up a unique identifier called the Partition Key (PK) and the Sort Key (SK). These together make up what's called a composite primary key, and as long as the combination are unique, it works.</p><h3>Partition Key</h3><p>Part of the way Dynamo achieves its performance & scalability goal is by storing data in \"partitions\" in AWS. As you'd expect, the PK is what determines how the data is partitioned.</p><h3>Sort Key</h3><p>The SK is used to perform queries on records within a partition, and only this field can be used to perform these queries. If you ever query data elements that are not specified in the SK, you're actually pulling all that data into your program and tossing out what doesn't match the query, which is pretty inefficient and should be avoided if possible. A way around this is using indexes, which I'll touch on in a bit.</p><p>The Sort Key has a number of operations that can be used when querying Dynamo;</p><ul><li>Equals</li><li>Begins With</li><li>Greater Than/GT Equal To</li><li>Less Than/LT Equal To</li><li>Between</li></ul><p>SKs can also be used (as you'd imagine) to sort your data before it is returned to your program from Dynamo.</p><p>Before we get into how I decided to structure my data, I started with defining the base schema;</p><ul><li>PK: <code>entityType: string</code> - The type of record it is. For builds, this will always be set to <code>build</code>. For users, this will be <code>user</code></li><li>SK: <code>entityId: string</code> - A unique ID for the record. For builds, this will be a randomly generated GUID. For users, it is a unique integer provided from the auth system.</li></ul><p>This tackles the first access pattern of \"Get a build by ID\" using the query;</p><pre class=\"language-plain text\"><code>entityType == 'build' && entityId == 'some-random-guid-here'\n</code></pre><p>The query above would be the same for \"Get a user by ID\" as well, with <code>entityType</code> being set to 'user' instead.</p><h2>Structuring the Data</h2><p>Now that we understand how keys work in Dynamo, below is how I decided to structure my build data in Dynamo.</p><pre class=\"language-plain text\"><code>- build\n    - id: string\n    - createdOn: int\n    - createdById: int\n    - isPrivate: bool\n    - buildSummaryData: object\n</code></pre><p>Now for the eagle-eyed dev out there, you're probably asking yourself something now: <em>\"Why the hell is he using int for createdOn instead of a date?\"</em> Well that's because Dynamo doesn't have a Date data type! The way we get around this is to use the epoch time as a timestamp. This would allow me to query builds within a date range provided I convert the timestamp to epoch before querying the date.</p><p>Epoch time is an numerical timestamp format that corresponds to a specific date & time. More accurately, it is the number of seconds since midnight on 1/1/1970 UTC.</p><p>So we know from the previous section of this article that <code>id</code> corresponds with the SK of the schema, and therefor can be queried directly. The other top-level fields (<code>createdOn</code>, <code>createdById</code>, and <code>isPrivate</code>) will be used for queries too.</p><p>I assume you probably have another question at this point: \"If we can only query on the PK & SK, how are you going to query on the other fields?\"</p><p>This is where Indexes come into play.</p><h2>Indexes in Dynamo</h2><p>If you want to access your data in different ways that the base schema, indexes help you do this. There are two types of indexes that can be specified in Dynamo:</p><ul><li>Local Secondary Indexes (LSIs) - Use the same PK, but a different SK</li><li>Global Secondary Indexes (GSIs) - Can have a different PK and a different SK</li></ul><p>Its worth noting that while GSIs can be created at any time, LSIs need to be specified when the table is first created.</p><p>So now lets take a look at the rest of the access patterns specified above and how we can use indexes to address them, along with the queries that will be used.</p><p><strong>Get the latest N builds.</strong></p><p>LSI Schema:</p><ul><li>PK: <code>entityType: string</code></li><li>SK: <code>createdOn: int</code></li></ul><p>Query:</p><pre class=\"language-plain text\"><code>entityType == 'build' && createdOn &gt; '{currentEpochTime}', sort by createdOn descending, get first N records\n</code></pre><p><strong>Get all builds for a user.</strong></p><p>LSI Schema:</p><ul><li>PK: <code>entityType: string</code></li><li>SK: <code>createdById: int</code></li></ul><p>Query:</p><pre class=\"language-plain text\"><code>entityType == 'build' && createdById == '{userId}'\n</code></pre><p>While none were used initially, I ended up using a GSI for searching, which I'll cover in an upcoming entry.</p><h2>What About \"Get all builds that a user has saved\"?</h2><p>So this is where some creativity comes in. To address this pattern, I didnt actually need another index. Instead, I used a different record structure for bookmarked builds.</p><ul><li>PK: <code>entityType: string</code> - Always set to <code>bookmarks</code></li><li>SK: <code>entityId: string</code> - Im actually setting this to the <code>userId</code> of the user who saved the build.</li></ul><p>Here is the data structure:</p><pre class=\"language-plain text\"><code>- bookmarks\n    - id: string\n    - buildSummaries: []object\n</code></pre><p>So you'll notice that there are no real top-level elements being stored simply because I dont need to index any other data. You also might be thinking \"Isn't storing the build summary data with the bookmark records going to add duplicate data into the database\" The answer is yes, yes it will.</p><p>That is one of the tradeoffs of using Dynamo (or most other NoSQL databases). The concept of joins that you'd use in a traditional DB system doesn't really exist. This is why its important to think about access patterns ahead of time.</p><p>So while there might be duplicate data, the performance benefits I get from using Dynamo outweigh having to manage that duplicate data. In GuardianForge, there is no way to update builds after they've been created. This means I can confidently know that the build summary data stored in <code>build</code> records should always be consistent with what's stored in the <code>bookmarks</code> records.</p><h2>About Those Build Summaries</h2><p>Throughout this article, you'll notice I keep mentioning build summaries instead of build data. As mentioned at the beginning, the bulk of the data for builds is stored in JSON files in S3. I do this because while Dynamo is fast, storing data is generally more expensive than S3.</p><p>Looking at GuardianForge, there are what are called \"Build Summary Cards\" throughout the app. The home page is one example of this.</p><figure><img src=\"/img/n/how-i-use-dynamodb-for-guardianforge/38edad75-fa30-4c70-b8be-348d0463bea7-image.png\" /><figcaption></figcaption></figure><p>Those six different blocks are all Build Summary Cards. Any place in the app where I need to load multiple builds will show these because this is the data coming from Dynamo. If you click on any of these, you'll be brought to the build page which has a bunch more info on it. This is accessing the JSON file in S3 (which I'll cover in the next article).</p><figure><img src=\"/img/n/how-i-use-dynamodb-for-guardianforge/2d22408d-05e2-4cb7-9208-92ec419a15e2-image.png\" /><figcaption></figcaption></figure><p>So lets break apart a build card. You'll notice there is a build name, four images, and the users name whos character this was based off of.</p><figure><img src=\"/img/n/how-i-use-dynamodb-for-guardianforge/70f6ea8b-cb10-40b5-9b3d-8a38ef190d3e-image.png\" /><figcaption></figcaption></figure><p>Realistically, that is the only data that needs to be stored in Dynamo. So Im saving cost and increasing performance by storing as little as needed in Dynamo. Comparing what each of these records look like shows what I mean.</p><p>Here is the build summary data that stored for a single build (there is some info that is hidden as well). These records average at about 500 bytes in Dynamo.</p><figure><img src=\"/img/n/how-i-use-dynamodb-for-guardianforge/11ad0cde-e367-453f-9bba-d9285c423754-image.png\" /><figcaption></figcaption></figure><p>And here is a full build record for that same build from S3. Notice how I have a SINGLE item expanded and the rest of it pretty much goes off the page. This file is about 14KB.</p><figure><img src=\"/img/n/how-i-use-dynamodb-for-guardianforge/3d0e556c-e5d3-49f5-962b-48e61ef02027-image.png\" /><figcaption></figcaption></figure><p>On the surface, 14K doesnt sound huge, and it isn't on its own. But lets see how this scales over time. All values in the spreadsheet below are KB and KB is assumed to be at 1000b instead of 1024b (its just an example).</p><figure><img src=\"/img/n/how-i-use-dynamodb-for-guardianforge/e922dbb6-7ff2-4d44-97a1-cd25b61d0985-image.png\" /><figcaption></figcaption></figure><p>As of this writing, Forge has about 250 builds, which equates to a savings of 3375KB.</p><p>Another way to think about this is if I wanted to get a list of all builds in Forge, the response size from the API would be about 125K with the summaries, and about 3.5MB with full builds. That equates to a HUGE performance difference to the user.</p><h2>Next</h2><p>In the next article, I'll cover how I setup S3 to access build data and the configuration required there. Got questions? Reach out to me on Twitter <a href=\"https://twitter.com/brianmmdev\" target=\"_blank\">@brianmmdev</a>. See you soon!</p></div>","featuredImage":"/img/n/how-i-use-dynamodb-for-guardianforge/6dbc96db-bfa9-411e-9969-d1de4837ef23-how-i-use-dynamodb-for-guardianforge-featured-image.jpg","excerpt":"In my last article, I covered a big chunk of the planning & discovery I did when building the first version of Guardi...","cachedOn":1681394472},{"id":"8a1a4c93-33fb-4f70-9e9c-a913f73cc71d","notion_id":"8a1a4c93-33fb-4f70-9e9c-a913f73cc71d","status":"Published","slug":"guardianforge-planning-discovery","relation_series":["22756350-048e-4423-8948-233370c99f6e"],"publishOn":"2021-08-20T00:00:00.000Z","codeURL":"https://github.com/guardianforge/guardianforge.net","seriesOrder":1,"youTubeURL":"https://youtu.be/Qva-s4UwOWg","title":"GuardianForge Planning & Discovery","html":"<div><p>Today I'm going to write about something quite different than anything I've done yet. Most of my articles to date have been very specific tutorials on how to accomplish something in a given language or framework. Today I'm going more conceptual and I'll explain my thought process behind how I built a web app from start to finish. This article specifically will cover a lot of the brainstorming, discovery, and project planning, whereas the others will cover specific features and how they were implemented.</p><h2>Introducing GuardianForge</h2><p>Over the last few months, I've been building a web app called GuardianForge, which acts as something of a social media for players of the game Destiny 2. It allows users to create snapshots of their current equipment in the game and share with other players. The equipment you have on your character in the game affects how the game plays, and can make activities either harder or easier depending on how you setup your loadout.</p><p>Build crafting in Destiny seems to be on the rise, and one thing I personally do before going into high level activities is ask around to find the best items to have equipped. A lot of Destiny content creators also produce build video for these activities which describe the items you should have equipped. Here is a small sample of one of my favorite creators Castle explaining a build in the game and how it works.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/afd8e3e9-1452-4eee-8e33-490bea5604b3-MTvZu1q.gif\" /><figcaption>Video: https://youtu.be/KV1lVBPg4oA</figcaption></figure><p>The problem is there are so many combinations I often cant remember much of the equipment they suggest, so one day I thought <em>\"Wouldn't it be nice if there was a convenient way to share an entire loadout using just a link?\"</em></p><p><strong>And that thought was the beginning of GuardianForge.</strong></p><p>GuardianForge is still a work in progress, but the basic gist is;</p><ul><li>Find a players character & view the loadout.</li><li>Create a build snapshot containing all the equipment, along with optional notes.</li><li>Share the unique link for that build to...anyone.</li></ul><figure><img src=\"/img/n/guardianforge-planning-discovery/0ab91e6f-6024-4ff9-8bfd-7a461fbce2de-image.png\" /><figcaption></figcaption></figure><p><a href=\"https://guardianforge.net/build/226dee1e-6b03-4074-8de9-576f60443e7e\" target=\"_blank\">https://guardianforge.net/build/226dee1e-6b03-4074-8de9-576f60443e7e</a></p><h2>Exploring The Destiny API</h2><p>Before I even start writing any code or planning infrastructure, I always explore APIs to see if my idea is even possible with what's provided. Bungie has a decently documented API located at <a href=\"https://bungie-net.github.io/multi/index.html\" target=\"_blank\">https://bungie-net.github.io/multi/index.html</a> so that's where I started. In order to access it, I first needed to register an application in their developer portal.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/786d0706-619b-49c3-bee6-688af6e53dde-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/guardianforge-planning-discovery/aedc9264-5bc9-4395-9735-eaab8253851c-image.png\" /><figcaption></figcaption></figure><p>So the API call that seemed to give me some of the data I wanted is the GET Character API. There were a few pieces of info I needed first, the <code>membershipId</code> and <code>membershipType</code>. After doing some digging, I found that I was able to find this info on the GET User API.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/74611b6c-e960-4bf7-bfba-ff10c0c81b36-image.png\" /><figcaption></figcaption></figure><p>Before I could call GET User, I needed the unique ID for that user in Bungie's system. I traced that back to the GET SearchUsers call, which let me search by username.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/735b9536-9932-46af-8801-2f053e5efead-image.png\" /><figcaption></figcaption></figure><p>Perfect, now back to the GET Character API. Bungie's API uses a query parameter called components that lets you request only the info you need from that specific call. Unfortunately I didn't know what each of these components returned (and although it was documented, it still didn't mean much to me at the time).</p><h2>Exploring How Other Apps Work</h2><p>Its about here that I started exploring other third party Destiny apps to see how they did it. So I reviewed one of these app's network requests using the Chrome dev tools' Network tab. The numbers at the end all look weird, but that's because they are URL Encoded (meaning they are changed to be more compatible with URLs). You can swap the <code>%2C</code> with a <code>,</code> and it looks more like <code>100,101,103...</code></p><figure><img src=\"/img/n/guardianforge-planning-discovery/081d6e77-7b42-4fb4-873a-de5a850e1de9-image.png\" /><figcaption></figcaption></figure><p>So I can match up those numbers with the API documentation to figure out what each really does.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/477d07fc-a8fc-4891-a4e3-79f7cfda4dcc-image.png\" /><figcaption></figcaption></figure><p>So now Ive got a list of components that I needed to request. Requesting it resulted in a rather large response and a whole lot of data that meant very little to me. I had no idea how to decipher it.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/2687117d-63ce-4094-aa25-8c51ffe0e162-image.png\" /><figcaption></figcaption></figure><h2>The Destiny Manifest</h2><p>Now Ive used third party Destiny apps for a number of years and I've always heard about the \"Manifest\" that is downloaded whenever you use one of these apps. I saw there was a call to GET Manifest in the API docs, so I ran that call to see what came back. To my surprise, it was a bunch of URLs along with some other metadata.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/4a0f4e49-8889-4598-9914-d2a7c7877d58-image.png\" /><figcaption></figcaption></figure><p>Attempting to grab one of these URLs resulted in a key/value JSON file. The keys in this dataset actually correspond to some of the data in the GET Character response. The Manifest is essentially a database with a massive amount of info on everything in the game.</p><p>Now these files can be rather large (one of the largest I use is about 60MB) and attempting to view them in the browser or VS Code resulted in a crash every time. Its just too much data to render in those apps. So I did a bit of digging and discovered <a href=\"https://dadroit.com/\" target=\"_blank\">Dadroit JSON Viewer</a>.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/b6ae100a-3a8b-4534-950f-3bb4573a51cd-image.png\" /><figcaption></figcaption></figure><p>Using Dadroit, I was able to search the entire manifest for the various <code>hash</code> values to figure out in which manifest file, and where, the info I needed was. If there was info I didn't have a hash for, I also leveraged some of the text found in game to discover the location of that info.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/5ce960d7-171b-462e-908e-dc306247e6c3-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/guardianforge-planning-discovery/8dd71052-6167-4436-8909-8f8495e1b82f-image.png\" /><figcaption></figcaption></figure><p>So after many hours of tracing data around the manifest, I realized that I could totally build an app that does what I needed it to.</p><h2>Infrastructure Planning</h2><p>Now my original goal with this project was to build something on AWS using as many services as I can to make this thing fly. I generally use these projects as opportunities to do something new, and this was no exception.</p><p>So whenever I approach a new app, I split it all up into three categories;</p><ul><li>Front end: Where will the user get access to the app?</li><li>Back end: Where will I make API requests to?</li><li>Storage: Where will all the data be stored?</li></ul><p>I also break out the things I want to user to do;</p><ul><li>Search for Destiny users & select any of their characters</li><li>Create a build, adding notes and other meta info</li><li>View & share builds</li></ul><p>And here is ultimately how the project was broken up:</p><figure><img src=\"/img/n/guardianforge-planning-discovery/110a108c-6501-4929-9554-3b1191c55a96-image.png\" /><figcaption></figcaption></figure><ol><li>When first hits <a href=\"http://guardianforge.net/\" target=\"_blank\">guardianforge.net</a>, they will access a CloudFront CDN instance which returns the Vue application to the user.</li><li>The built Vue app is stored in an S3 bucket, where CloudFront Accesses it.</li><li>Requests to the API go to API Gateway.</li><li>API Gateway forwards those requests to a Lightsail VM, where the main API code is running.</li><li>The API accesses relevant data from a DynamoDB table.</li><li>When a build page is loaded, the Vue app will request the raw JSON file from a separate S3 bucket directly.</li></ol><p>Read on for more info on how each of these components work and how they are used in GuardianForge.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/179664bb-4956-4406-a0f3-6303d239bab3-image.png\" /><figcaption></figcaption></figure><h3>DynamoDB</h3><p>DynamoDB is a NoSQL solution in AWS. I wanted to use GuardianForge as an opportunity to get practice modelling a schema for Dynamo. The main idea was to use Dynamo as more of an index than a full database. I would store build summaries as stubs of the real data. This reduces the amount of data stored in Dynamo, which reduces cost on both storage and egress (data leaving AWS) charges.</p><p>Dynamo is VERY fast if the schema is designed properly. I'll be covering my schema design process in a later entry in this series.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/6c4f42fb-94a5-4898-bf30-816a9586a41d-image.png\" /><figcaption></figcaption></figure><h3>S3</h3><p>I decided to use AWS S3, which is their solution for storing unstructured data (think like files & folders). I'm using it for two functions.</p><p>The first is storing the front end Vue.js application files. S3 has static web hosting feature on its own, but in order to use my own domain name, I ended up connecting to it via a CloudFront CDN Instance (more on that in a moment).</p><p>The second is the actual build data. Each build has a corresponding JSON file that's stored in S3 that contains all of the necessary build data to render it in the Vue app. Now why wouldn't I just store this in Dynamo? Well from a cost perspective, Dynamo generally charges more for storage and less for transmitting data, whereas S3 is the opposite.</p><p>In general, the actual build files will not be hit too frequently (unless one gets really popular), so Id rather pay less for storing this data. In either case, I could put them behind the same CDN as the application files to increase performance and decrease actual hits to the file.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/a0b0b4da-9ada-48cc-bb54-c7247f1ed14f-image.png\" /><figcaption></figcaption></figure><h3>CloudFront CDN</h3><p>Now in the previous section, I mentioned putting the app files behind CloudFront CDN. A CDN is a service that distributes static pages to various endpoints throughout the world. This results in decreased latency when a user hits those pages.</p><p>Using the combination of CloudFront & S3, I can not only optimize performance for my users, but also add my own domain name & acquire a free SSL certificate using Amazon Certificate Manager (ACM).</p><p>I can also setup some pretty fancy routing rules, which I end up doing when implementing social opengraph meta info. More on that in a later entry.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/a158df54-d36e-4f66-90fb-ad9d950a25de-image.png\" /><figcaption></figcaption></figure><h3>Lightsail</h3><p>Lightsail was one of those new services I wanted to try out. From my understanding, it lets you setup databases, servers, and containers at a fixed price in AWS. I'd worked with EC2, which is the server platform in AWS, so this was quote similar.</p><p>Essentially I just had a Linux server that would run the API written in Go. Go was also another thing I wanted out of this project. I hadn't built a real world project with Go up 'til this point. When a Go app is compiled, it produces a native executable binary that can run on the same platform it was built on. So I ended up writing a Systemd Unit file to manage & run the service when the server starts. This was also something I hadn't done before, so it was a neat learning experience.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/66e4fa36-b664-4ee0-8df0-faa87a105991-image.png\" /><figcaption></figcaption></figure><h3>API Gateway</h3><p>Now API Gateway was a pretty late addition to my infrastructure. The main reason (and its not a good one) was to add SSL to my API. This was admittedly just a band-aid instead of a permanent solution. I wanted to buy myself some time to figure out how I wanted to secure the API properly.</p><p>API Gateway acts as an API management platform in AWS. So you get one URL and you can route requests to other URLs or services within AWS. I actually decided to leave API Gateway in the diagram because it becomes much more important later on.</p><h2>Code Storage, Project Management & Continuous Deployment</h2><p>The final pieces of this puzzle are less about the infrastructure and more about managing things. While the obvious thought would be GitHub, I've been a huge fan of Azure DevOps for years. Name aside, it actually has very little to do with Azure.</p><p>Ultimately this is personal preference. I like the UI in Azure DevOps, the more advanced project management tools, and the better ability to create pipelines for deploying and managing builds.</p><p>Azure DevOps has built in support for sprints & breaking down work items into features or epics if needed.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/bc00f5ef-349b-47cc-9849-a8f2407b45c6-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/guardianforge-planning-discovery/75bae27c-55e0-4abd-81fd-d00234bd7441-image.png\" /><figcaption></figcaption></figure><p>Now if I want to somehow link a commit to a work item, all I have to do is include the task number in the commit message and I can trace my comments to whatever tasks I have.</p><figure><img src=\"/img/n/guardianforge-planning-discovery/36df851b-9aa3-4be2-8965-c43693683f29-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/guardianforge-planning-discovery/a74356e5-3d0b-4bab-bb82-f7821ceff724-image.png\" /><figcaption></figcaption></figure><p>Finally I can control my builds/deployments using their Pipelines feature. Pipelines is essentially a glorified task runner. I can pick from predefined step templates, or just write my own scripts to run at certain parts of the process (which I generally lean towards).</p><figure><img src=\"/img/n/guardianforge-planning-discovery/61f4e0c9-ad31-4291-8b7a-69c7f7190e01-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/guardianforge-planning-discovery/a073b25b-99c7-43e4-b080-b0726251af7f-image.png\" /><figcaption></figcaption></figure><h2>Looking Ahead</h2><p>Working on GuardianForge has helped me learn a bunch of interesting little tidbits that I didn't know before. I plan to identify some of the features of GuardianForge and create articles & videos on them, as well as overcoming some issues, in the near future. Happy coding!</p></div>","featuredImage":"/img/n/guardianforge-planning-discovery/be586f29-3774-450b-8a75-0528ec00f144-guardianforge-planning-discovery-featured-image.jpg","excerpt":"Today I'm going to write about something quite different than anything I've done yet. Most of my articles to date hav...","cachedOn":1681394473},{"id":"c4755ec9-b608-40f6-890b-ed805bdd7c17","notion_id":"c4755ec9-b608-40f6-890b-ed805bdd7c17","status":"Published","slug":"building-postman2http-in-go","relation_series":[],"publishOn":"2021-04-01T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/postman2http","seriesOrder":null,"youTubeURL":"https://youtu.be/IVpY9Z8xna0","title":"Building Postman2Http in Go","html":"<div><p>I've been using Postman for a number of years now, basically since I first got into web development. It is indeed a fantastic tool, but one part I hate about it is the proprietary nature of the way it stores its data. In the world of development, there is always a push to keep as many components as possible with the repository. It helps with tracking who did what, and keeps a team on the same page. Postman stores its data differently. Instead of keeping it in files that can be stored with a repo, it stores it on a backend service. Now this isn't necessarily a bad thing as it enables some pretty neat collaboration & automation. BUT if all you're after is an easy way to test your APIs, then it can be overkill, and furthermore frustrating as you are locked into their system.</p><p>Enter the VS Code Rest Client plugin. It's a pretty straightforward plugin on the service that has a decent feature set the more you dig into it. Ultimately it was what I was after. An easy way to write API tests with the ability to store them with my repos, quickly switch environments, and use variables that I could define myself or chain together between requests. Now all I had to do is find a way to migrate all the work Ive done in Postman over the years into this new plugin, which defines its test in a <code>.http</code> file as shown below.</p><pre class=\"language-plain text\"><code>get http://localhost:3000\n\n###\n\nget http://localhost:3000/294688564132708868\n\n###\n\npost http://localhost:3000\nContent-Type: application/json\n\n{\n  \"another\": \"test\"\n}\n\n###\n\nput http://localhost:3000/294688564132708868\nContent-Type: application/json\n\n{\n  \"hello\": \"chicago!\"\n}\n\n###\ndelete http://localhost:3000/294688564132708868\n</code></pre><p>I realized if I exported a Postman collection, it was just a JSON file with the definitions of the requests themselves in there like so.</p><pre class=\"language-json\"><code>{\n\t\"info\": {\n\t\t\"_postman_id\": \"9cf8e206-ea07-4ee9-aae8-19d67537a1d7\",\n\t\t\"name\": \"Test Collection\",\n\t\t\"schema\": \"https://schema.getpostman.com/json/collection/v2.1.0/collection.json\"\n\t},\n\t\"item\": [\n\t\t{\n\t\t\t\"name\": \"Fetch Items\",\n\t\t\t\"request\": {\n\t\t\t\t\"method\": \"GET\",\n\t\t\t\t\"header\": [],\n\t\t\t\t\"url\": {\n\t\t\t\t\t\"raw\": \"localhost:3000\",\n\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\"localhost\"\n\t\t\t\t\t],\n\t\t\t\t\t\"port\": \"3000\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"response\": []\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Get Item\",\n\t\t\t\"request\": {\n\t\t\t\t\"method\": \"GET\",\n\t\t\t\t\"header\": [],\n\t\t\t\t\"url\": {\n\t\t\t\t\t\"raw\": \"localhost:3000/294688583753662980\",\n\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\"localhost\"\n\t\t\t\t\t],\n\t\t\t\t\t\"port\": \"3000\",\n\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\"294688583753662980\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"response\": []\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Create Item\",\n\t\t\t\"request\": {\n\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\"header\": [],\n\t\t\t\t\"body\": {\n\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\"raw\": \"{\\r\\n    \\\"testKey\\\": \\\"testVal\\\"\\r\\n}\",\n\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"url\": {\n\t\t\t\t\t\"raw\": \"localhost:3000\",\n\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\"localhost\"\n\t\t\t\t\t],\n\t\t\t\t\t\"port\": \"3000\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"response\": []\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Update Item\",\n\t\t\t\"request\": {\n\t\t\t\t\"method\": \"PUT\",\n\t\t\t\t\"header\": [],\n\t\t\t\t\"body\": {\n\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\"raw\": \"{\\r\\n    \\\"anotherTest\\\": \\\"anotherVal\\\"\\r\\n}\",\n\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"url\": {\n\t\t\t\t\t\"raw\": \"localhost:3000/294691575403905538\",\n\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\"localhost\"\n\t\t\t\t\t],\n\t\t\t\t\t\"port\": \"3000\",\n\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\"294691575403905538\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"response\": []\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Delete Item\",\n\t\t\t\"request\": {\n\t\t\t\t\"method\": \"DELETE\",\n\t\t\t\t\"header\": [],\n\t\t\t\t\"url\": {\n\t\t\t\t\t\"raw\": \"localhost:3000/294691575403905538\",\n\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\"localhost\"\n\t\t\t\t\t],\n\t\t\t\t\t\"port\": \"3000\",\n\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\"294691575403905538\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"response\": []\n\t\t}\n\t]\n}\n</code></pre><p>As of the writing of this article, I'm practicing with Golang and I was looking for a way to challenge myself with a real world app that I could use. These seem to be the projects that resonate with my learning style the best.</p><ul><li>Define a problem</li><li>Break down the problem</li><li>Solve it with some new fancy utility I can write</li><li>Win!</li></ul><p>So one day, while live streaming on my YouTube channel, I decided to start building. I broke down the workflow of the app to do the following;</p><ol><li>I needed a way to read the contents of a file (the exported Postman collection)</li><li>Once read, I needed a way to convert the JSON into a format that Go would understand</li><li>Then I needed to use that format to create what would essentially be a formatted string.</li><li>And finally, a way to save the string to a file on the disk.</li></ol><p>I ended up writing everything in a single <code>main.go</code> file, which I break down below. I added numbers in the comments for each respective code block to line up with my outline above.</p><pre class=\"language-go\"><code>package main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n)\n\nfunc main() {\n\t// 1.a. Using command line arguments, I'm able to specify the path to the file I want to import.\n\tjsonFile, err := os.Open(os.Args[1])\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n\n\t// 1.b. Read the contents of the file into a byte array\n\tbyteValue, _ := ioutil.ReadAll(jsonFile)\n\tdefer jsonFile.Close()\n\n\t// 2. Create an empty variables and convert the JSON into a Go type that I can work with\n\tvar collection PostmanCollection\n\tjson.Unmarshal(byteValue, &collection)\n\n\t// 3.a. Create an empty string to hold the contents of the final file to be created.\n\toutString := \"\"\n\t// 3.b. Loop over the Items in the collection and append certain pieces to 'outString'\n\tfor idx, item := range collection.Items {\n\t\toutString += item.Request.Method + \" \" + item.Request.Url.Raw + \"\\n\"\n\t\tfor _, header := range item.Request.Headers {\n\t\t\toutString += header.Key + \": \" + header.Value + \"\\n\"\n\t\t}\n\t\tif item.Request.Body.Raw != \"\" {\n\t\t\toutString += \"\\n\" + item.Request.Body.Raw + \"\\n\"\n\t\t}\n\n\t\tif idx+1 != len(collection.Items) {\n\t\t\toutString += \"\\n###\\n\\n\"\n\t\t}\n\t}\n\n\t// 4.a. Create a file on disk to hold the contents of the 'outString'\n\tf, err := os.Create(collection.Info.Name + \".http\")\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\n\t// 4.b. Write 'outString' to that file\n\t_, err = f.WriteString(outString)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tf.Close()\n}\n\n// Below details out the types used to hold the data. Note the annotations to the right\n//    of each field which instruct Go how to read the raw data into the repsective types.\ntype PostmanCollection struct {\n\tInfo  Info   `json:\"info\"`\n\tItems []Item `json:\"item\"`\n}\n\ntype Info struct {\n\tPostmanId string `json:\"_postman_id\"`\n\tName      string `json:\"name\"`\n\tSchema    string `json:\"schema\"`\n}\n\ntype Item struct {\n\tName    string  `json:\"name\"`\n\tRequest Request `json:\"request\"`\n}\n\ntype Request struct {\n\tMethod  string   `json:\"method\"`\n\tHeaders []Header `json:\"header\"`\n\tUrl     Url      `json:\"url\"`\n\tBody    Body     `json:\"body\"`\n}\n\ntype Header struct {\n\tKey   string `json:\"key\"`\n\tValue string `json:\"value\"`\n}\n\ntype Url struct {\n\tRaw string `json:\"raw\"`\n}\n\ntype Body struct {\n\tRaw string `json:\"raw\"`\n}\n</code></pre><p>After an hour of coding, I came away with a very basic MVP. I admit, at this stage it still needs alot of work, but it does exactly what I needed it to. Now I need to feed it more collections in order to iron out any bugs!</p></div>","featuredImage":"/img/n/building-postman2http-in-go/da40e06d-0cd8-4634-9f40-961c26f4eeeb-building-postman2http-in-go-featured-image.jpg","excerpt":"I've been using Postman for a number of years now, basically since I first got into web development. It is indeed a f...","cachedOn":1681394483},{"id":"ee6c7a43-fbdd-4eee-b6f1-6908a93785d3","notion_id":"ee6c7a43-fbdd-4eee-b6f1-6908a93785d3","status":"Published","slug":"easy-crud-with-faunaservice","relation_series":[],"publishOn":"2021-03-10T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/faunaservice-demo","seriesOrder":null,"youTubeURL":null,"title":"Easy CRUD with FaunaService","html":"<div><p>Fauna is an excellent choice for storing data in the cloud, and the <code>fauna</code> NPM package is a full featured adapter around the service. It adapts the Fauna Query Language (FQL) almost directly in JavaScript, but it can be somewhat confusing for developers new to the platform. Thats why I built <code>FaunaService</code>, a simple wrapper around basic CRUD operations for Fauna. The package simplifies these basic operations into simple functions that can be used by any application quickly, without having the learn FQL. In this article, I'll explain how to set up the <code>FaunaService</code> object, give a demo of each function, and end with a sample API written in Express. I'll also assume you have a database & collection already setup. The code shown below will be on a Windows 10 machine with the latest build, but to follow along you should also have the following installed;</p><ul><li>VSCode (or an IDE of your choice)</li><li>NodeJS (The latest LTS version)</li></ul><p>Fauna is free to start, click <a href=\"https://dashboard.fauna.com/accounts/register?utm_source=DevTo&utm_medium=referral&utm_campaign=WritewithFauna_NPMPackage_BMorrison\" target=\"_blank\">here</a> to create an account!</p><h2>Getting Your API Key</h2><p>Before we can start coding, you'll need to create an API key for your database. Open your database in Fauna, click on Security in the bottom left, and create a key. I'm going to select the Server option for my role (so as to not give the key too much access), and give the key a name just so I know what its for in the future.</p><p>Make sure to note your key now as you wont be able to going forward.</p><figure><img src=\"/img/n/easy-crud-with-faunaservice/cb980c98-fcb9-4475-b5fa-d2d5cbeba318-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/easy-crud-with-faunaservice/e2b6a97e-64c3-4471-88d6-13998d0ad545-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/easy-crud-with-faunaservice/25c41084-3710-4001-9b82-559848f4a0e4-image.png\" /><figcaption></figcaption></figure><h2>Start Your Project</h2><p>To start, open a new folder in VSCode and initialize a new project with <code>npm init -y</code> to accept all the defaults. The run <code>npm install </code>@brianmmdev/faunaservice` to install the package and its dependencies.</p><figure><img src=\"/img/n/easy-crud-with-faunaservice/428d69ea-c1fa-4699-8b7a-00d70239ad0a-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/easy-crud-with-faunaservice/053c8121-7e0b-4f6f-8287-c8fd81d643c0-image.png\" /><figcaption></figcaption></figure><p>Now lets start writing some code! Create a file called app.js and import the package. Create a new <code>FaunaService</code> class and pass in your key to initialize it.</p><figure><img src=\"/img/n/easy-crud-with-faunaservice/241c46de-390b-4ae4-aa48-33098fb3254e-image.png\" /><figcaption></figcaption></figure><h2>CRUD Examples</h2><p>The service has five distinct functions that are exported, all of which are asynchronous;</p><ul><li><code>createRecord(collectionName, data)</code></li><li><code>listRecords(collectionName)</code></li><li><code>getRecordById(collectionName, recordId)</code></li><li><code>updateRecord(collectionName, recordId, updates)</code></li><li><code>deleteRecord(collectionName, recordId)</code></li></ul><p>We'll start by creating a few records just to populate our collection with some data. To create records, simply pass in any JavaScript object and it will be added to the collection. Using the <code>createRecord</code> function also returns what was added into the collection along with the <code>id</code> for that record.</p><p>One thing to note about the <code>id</code> field is that it maps directly to the <code>refId</code> inside Fauna. The package simply flattens the object returned directly from the Fauna operation to make it easier to work with.</p><figure><img src=\"/img/n/easy-crud-with-faunaservice/82698a21-0cd4-4d61-b1f4-4e0fa35f38f6-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/easy-crud-with-faunaservice/c9325bf6-c1f1-4fe2-b3a7-8e4c62053bb3-image.png\" /><figcaption></figcaption></figure><p>Now to see all of the data in our collection, we can use the <code>listRecords</code> function like so. The objects will be flattened as above and be returned as an array.</p><figure><img src=\"/img/n/easy-crud-with-faunaservice/bc954032-fc45-473e-8d4f-f0243f69ba04-image.png\" /><figcaption></figcaption></figure><p>To fetch a single record, use the <code>getRecordById</code> function with the collection name and Id of the object. I'm going to grab an Id from another record directly from the Fauna dashboard.</p><figure><img src=\"/img/n/easy-crud-with-faunaservice/02852829-0ac9-4973-a3b3-6609ff5a4429-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/easy-crud-with-faunaservice/0a6c5357-93a8-4f34-b75c-5722143d7a4d-image.png\" /><figcaption></figcaption></figure><p>Now let's say we've completed this specific task and want to mark it as such, we can use the <code>updateRecord</code> function to do so. This function will only update the fields that are passed in, so no need to include the name of the task as well. As with <code>createRecord</code> the data will be returned if needed;</p><figure><img src=\"/img/n/easy-crud-with-faunaservice/1652a377-7720-4f1f-8941-39e4084c44a2-image.png\" /><figcaption></figcaption></figure><p>And finally, we can delete records with <code>deleteRecord</code>. By reviewing the Fauna dashboard, we can see the record is indeed gone.</p><figure><img src=\"/img/n/easy-crud-with-faunaservice/2290d7f4-e50b-43b7-b223-93902570cc36-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/easy-crud-with-faunaservice/402b2bc1-d9e7-426e-8ba1-77ba8784a326-image.png\" /><figcaption></figcaption></figure><h2>Example API</h2><p>Before I close it out, I want to give an example of what an API built with Express might look like. The source code for this is available at <a href=\"https://github.com/bmorrisondev/faunaservice-demo\" target=\"_blank\">https://github.com/bmorrisondev/faunaservice-demo</a>.</p><pre class=\"language-plain text\"><code>const FaunaService = require(\"@brianmmdev/faunaservice\")\nconst bodyParser = require('body-parser')\nconst express = require('express')\nconst app = express()\n\napp.use(bodyParser.json())\n\nconst service = new FaunaService(\"your_fauna_key\")\n\n// Get all tasks\napp.get(\"/tasks\", async (req, res) =&gt; {\n  let records = await service.listRecords(\"TaskData\")\n  res.json(records)\n})\n\n// Get a single task by id\napp.get(\"/tasks/:id\", async (req, res) =&gt; {\n  let record = await service.getRecordById(\"TaskData\", req.params.id)\n  res.json(record)\n})\n\n// Create a task\napp.post(\"/tasks\", async (req, res) =&gt; {\n  let data = req.body\n  let created = await service.createRecord(\"TaskData\", data)\n  res.json(created)\n})\n\n// Update a task\napp.put(\"/tasks/:id\", async (req, res) =&gt; {\n  let updates = req.body\n  let updated = await service.updateRecord(\"TaskData\", req.params.id, updates)\n  res.json(updated)\n})\n\n// Delete a task\napp.delete(\"/tasks/:id\", async (req, res) =&gt; {\n  await service.deleteRecord(\"TaskData\", req.params.id)\n  res.status(200).send()\n})\n\napp.listen(3000, () =&gt; console.log(\"API is running!\"))\n</code></pre><h2>Conclusion</h2><p>While this package doesn't cover the vast capabilities of Fauna, it does indeed give new developers, as well as those looking to create quick demos, an edge over having to learn a new query language by simplifying the most basic operations down to a single line of code! If you are interested in contributing, the package is open source and located at <a href=\"https://github.com/bmorrisondev/faunaservice\" target=\"_blank\">https://github.com/bmorrisondev/faunaservice</a>.</p></div>","featuredImage":"/img/n/easy-crud-with-faunaservice/9b60bffb-5270-45b9-a1fa-bff0e6d666b2-easy-crud-with-faunaservice-featured-image.jpg","excerpt":"Fauna is an excellent choice for storing data in the cloud, and the  fauna  NPM package is a full featured adapter ar...","cachedOn":1681394484},{"id":"bd4052ee-9e00-4722-b7c0-a695813e5ea2","notion_id":"bd4052ee-9e00-4722-b7c0-a695813e5ea2","status":"Published","slug":"working-with-branches-in-git-github","relation_series":["21faed22-0507-4e53-a2a2-39c424faa8ea"],"publishOn":"2021-03-03T00:00:00.000Z","codeURL":null,"seriesOrder":2,"youTubeURL":"https://youtu.be/Jsuldaq1QD8","title":"Working with Branches in Git & GitHub","html":"<div><p>As you add commits to a repository, a timeline of the code is being built as more and more changes are added. This allows us to roll back to previous versions of the code, but what if you want to make some changes with affecting what's been done so far? This is where branches come in.</p><p><strong>Branches</strong> are isolated versions of your code that can be created to allow developers to work on a repository, save and commit their changes, all while leaving the original version unaffected. Considering the timeline example, this would be like creating a complete separate timeline where changes can be made independently of the other timelines. I'll be using a VS Code extension called Git Graph to demonstrate various branching concepts.</p><h2>Working with Branch</h2><p>Before we create a branch, I want to set the stage for the rest of this article. I have a small repo that I created with a few commits. Looking at the repo in Git Graph, we can see the timeline of my commits displayed visually. Note in Git Graph the name of the branch is highlighted by the color represented on the timeline.</p><figure><img src=\"/img/n/working-with-branches-in-git-github/ac1dfc7e-328a-4111-a2d3-9aff8a50689c-image.png\" /><figcaption></figcaption></figure><p>Now lets say I want to modify my styles to give the background a different color, but I'm not sure I really want to keep it. What I can do is create a branch to test my changes out. We can do this using this terminal command. This tells git to create a new branch and switch to it to start working on.</p><pre class=\"language-bash\"><code>git checkout -b testing-styles</code></pre><p>Now you'll have another branch name appear next to the timeline. This is because we haven't made any changes yet. Also note in the bottom left of VS Code, your working branch will be displayed. You can also create a new branch by clicking the name of the working branch.</p><figure><img src=\"/img/n/working-with-branches-in-git-github/40f17dca-39e1-4550-8080-f4cc61d9d0a9-image.png\" /><figcaption></figcaption></figure><p>Now lets add a change to the new branch. I'm going to modify style.css to set a new background color on the page. If you're new, working branch is ahead of the source branch (master in this case), it won't display any different since there are no real differences between the branches, so for the sake of the demonstration, I'm going to modify a file on master and commit that as well. Notice how the colors change and you can visually see the branch break off of the main trunk.</p><figure><img src=\"/img/n/working-with-branches-in-git-github/6cfc56a2-8bdb-4426-b692-36f2540ff975-image.png\" /><figcaption></figcaption></figure><p>Now we can switch between our branches and review the changes independently.</p><figure><img src=\"/img/n/working-with-branches-in-git-github/1021cbfe-8117-4fba-a2ea-1df1901d8cc3-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/working-with-branches-in-git-github/18eb8489-3eb5-458d-bad0-c947dc672035-image.png\" /><figcaption></figcaption></figure><p>Yellow is clearly hard to read, so lets change it to a dark grey.</p><figure><img src=\"/img/n/working-with-branches-in-git-github/0922a7fe-9471-4f11-9922-6ae5c3183e44-image.png\" /><figcaption></figcaption></figure><p>Much better. Now that we're satisfied with the change. Lets get it merged in.</p><h2>Merging</h2><p>Merging is the process of taking all the commits that have been done on one branch and applying them to another. To merge our <code>testing-styles</code> into <code>master</code>, issue the following command in your terminal.</p><pre class=\"language-bash\"><code># Switch to the branch you want to merge into\ngit checkout master\n\n# Specify the branch you want to merge into this one\ngit merge testing-styles</code></pre><p>Now our changes to <strong>style.css</strong> have been brought into <code>master</code> and we can view the changes in the browser. We can also see in Git Graph visually what the merge looks like.</p><figure><img src=\"/img/n/working-with-branches-in-git-github/12f47ff6-06a0-4e78-bf58-97724415c84f-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/working-with-branches-in-git-github/df0b1e77-dac7-41f7-9d33-b98f25d12360-image.png\" /><figcaption></figcaption></figure></div>","featuredImage":"/img/n/working-with-branches-in-git-github/4a1f3cb4-f72b-4b70-a484-dc7a8d4875c4-working-with-branches-in-git-github-featured-image.jpg","excerpt":"As you add commits to a repository, a timeline of the code is being built as more and more changes are added. This al...","cachedOn":1681394485},{"id":"35c81746-1e04-4eb7-839d-b769cca35aab","notion_id":"35c81746-1e04-4eb7-839d-b769cca35aab","status":"Published","slug":"the-basics-of-git-github","relation_series":["21faed22-0507-4e53-a2a2-39c424faa8ea"],"publishOn":"2021-03-02T00:00:00.000Z","codeURL":null,"seriesOrder":1,"youTubeURL":"https://youtu.be/Jsuldaq1QD8","title":"The Basics of Git & GitHub","html":"<div><p>This article kicks off the first in a series on how to use Git & GitHub. Understanding how to use Git is essential to any developer, and GitHub makes it easy and free to store your code in version control, a fancy term meaning you are taking snapshots of your code at certain points in time. This article assumes you have git installed on your computer already, but if not, click <a href=\"https://git-scm.com/downloads\" target=\"_blank\">here</a> to install it. Lets dive in!</p><h2>Creating a Repository</h2><p>Repositories are like storage containers for your code. They separate your project from everyone else's on GitHub. To create a repository, log into GitHub and click the + button in the upper right and then New Repository.</p><figure><img src=\"/img/n/the-basics-of-git-github/9c072bcf-5ac2-4b7c-bf20-2ee36eddf97c-image.png\" /><figcaption></figcaption></figure><p>In the <strong>Create a new repository</strong> view, give your repository a name, all other fields are optional and are relatively self explanatory. One thing I wanted to call out is the options under <strong>Initialize this repository with:</strong> section. If any of these options are checked, you'll need to pull the repository before you are able to sync existing code with it. I personally never use these options, especially if I am looking to push an existing local repository up to GitHub as it could cause issues.</p><figure><img src=\"/img/n/the-basics-of-git-github/5faf3099-5125-4fd9-a52f-57d28856867f-image.png\" /><figcaption></figcaption></figure><h2>Syncing your Code to GitHub</h2><p>There are three methods worth calling out when it comes to syncing code with GitHub; Cloning, Pulling, and Pushing.</p><h3>Cloning</h3><p>Cloning is what you'd do when you want to get an initial sync of code from GitHub to your workstation. It will pull down a copy of whatever code is currently in the repository. Using the example of the repository above, you should see a screen like this when the repository is finished. Note the URL that is used to clone the repo.</p><p>To clone the repository, open up the terminal on your computer and navigate to a folder where you want the code to be stored. The following command will do two things. Firstly, it will create a folder INSIDE your current folder that is the same name as the repository. Second, it will copy all of the code from GitHub into that folder it created.</p><pre class=\"language-bash\"><code>git clone https://github.com/bmorrisondev/my-test-repository.git</code></pre><p>If this is the first time you are cloning from GitHub on your computer, your terminal will prompt you for your GitHub credentials. Otherwise it will use the account you previously signed in as.</p><p>You can also specify the folder if you want to by appending it to the end of the command. Git will then create the new folder with the name you specified. If I were to execute the following command in <code>C:\\Repos</code>, all of my code will end up in <code>C:\\Repos\\my-awesome-repo</code>.</p><pre class=\"language-bash\"><code>git clone https://github.com/bmorrisondev/my-test-repository.git my-awesome-repo</code></pre><h3>Pushing</h3><p>Pushing is the process of pushing your list of new commits up to GitHub. Note that it is the <strong>commits</strong> that will be pushed, so you'll need to make sure to commit any code into your local repository before it will be pushed to GitHub.</p><p>Lets add a file and commit it. I'm going to add a single file called <strong>My New File.txt.</strong> Then when I use <code>git status</code>, we can see the file is currently untracked.</p><figure><img src=\"/img/n/the-basics-of-git-github/48655a46-9551-469a-8bc0-9625ba76f2fc-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/the-basics-of-git-github/40f3e094-1160-405f-836b-b5261e50eeb3-image.png\" /><figcaption></figcaption></figure><p>Lets commit the file with git add . followed by git commit -m \"Your commit message\".</p><figure><img src=\"/img/n/the-basics-of-git-github/c95b6e03-7732-4e10-ae59-353700e3e94b-image.png\" /><figcaption></figcaption></figure><p>Once the file is committed, we can push it up to GitHub. Git uses what's called a <code>remote</code> to inform it where it should be pushed to. You can see a list of remotes on your repository with <code>git remote -v</code>. If you've cloned a repo from GitHub, this will be setup automatically. We can issue the following command to push our changes to GitHub.</p><pre class=\"language-bash\"><code>git push</code></pre><figure><img src=\"/img/n/the-basics-of-git-github/37e6cd4e-e29b-408b-9b59-d34f035145b0-image.png\" /><figcaption></figcaption></figure><p>Now if you are pushing existing code that was not cloned down, you'll need to set up the remote before you can push. To do this, have your terminal open in the same directory as your repository and issue the following commands, replace the repository URL with whatever your git URL is in GitHub.</p><pre class=\"language-bash\"><code># This command registers a remote called 'origin' with the git URL\ngit remote add origin https://github.com/username/reponame\n\n# This command will push your code to the remote origin with the branch named 'branch_name'. Branch name is usually 'master' or 'main'.\n# This will also configure your local repo to 'track' its current branch with 'branch_name'\ngit push --set-upstream origin branch_name\n</code></pre><figure><img src=\"/img/n/the-basics-of-git-github/2d62e579-7576-4ce4-9f7f-fbfae0ee6f04-image.png\" /><figcaption></figcaption></figure><h3>Pulling</h3><p>Pulling is what you'll do when others (or you on another workstation) have pushed commits up to GitHub and you want to pull those new commits down into your local machine. Provided your remote is setup as discussed in the previous section, you can do this simply by issuing the command;</p><pre class=\"language-bash\"><code>git pull</code></pre><p>In this example, I made a change to the text file in my repository directly in GitHub, which is simply used to simulate changes being made somewhere other than your local workstation.</p><figure><img src=\"/img/n/the-basics-of-git-github/ddad2663-ef1c-4fd5-bc2c-2861822717e9-image.png\" /><figcaption></figcaption></figure><p>Now if I compare the commits in GitHub with the commits locally (using <code>git log</code>), you'll see there is one new commit in my GitHub repository that is not in my local repo.</p><figure><img src=\"/img/n/the-basics-of-git-github/df8df3a6-53a8-43d0-9ce2-b8dee7d27a26-image.png\" /><figcaption></figcaption></figure><p>Issuing <code>git pull</code> will bring those changes in as well as update the text file on my computer.</p><figure><img src=\"/img/n/the-basics-of-git-github/9a4bb715-3f5e-4c5e-8014-cc7610461de6-image.png\" /><figcaption></figcaption></figure><p>In the next article, I'll cover branching and how it works.</p></div>","featuredImage":"/img/n/the-basics-of-git-github/aa3ce133-8837-42a9-ae01-0c32504f6376-the-basics-of-git-github-featured-image.jpg","excerpt":"This article kicks off the first in a series on how to use Git & GitHub. Understanding how to use Git is essential to...","cachedOn":1681394492},{"id":"44362d20-bd4a-45e1-80a8-a4c6d91e6b01","notion_id":"44362d20-bd4a-45e1-80a8-a4c6d91e6b01","status":"Published","slug":"automate-deployments-to-aws-with-github-actions","relation_series":["f36d703d-ef13-4706-bc71-0e61c27f6ae7"],"publishOn":"2021-02-23T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2021-02-vue-aws-app-hosting","seriesOrder":3,"youTubeURL":"https://youtu.be/aBJMHcMjhZ4","title":"Automate Deployments to AWS with GitHub Actions","html":"<div><p>In this article, I'm going to cover how you can setup GitHub Actions to automatically deploy changes to AWS using S3 & CloudFront. To follow along, check out the other articles in the series located at the bottom of this page.</p><h2>Setting up a Deploy Account in AWS</h2><p>Before we can setup the Action, we need to setup an account in AWS to allow us to deploy our code. In AWS, head to IAM and create a new user. I named my user GithubDeploy, but you can name yours whatever you prefer. We need to make sure Programmatic access is selected before continuing.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/0c3c1892-5d8d-40e9-beee-4aa1539893a6-image.png\" /><figcaption></figcaption></figure><p>In the next view, select Attach existing policies directly and check AdministratorAccess. Note that this will give the account access to EVERYTHING in our AWS account. In a production scenario, it would be a good idea to create a custom policy with only the required permissions enabled, but thats beyond the scope of this article.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/8b96a784-ee1d-4390-977b-8fcb9c9b7e06-image.png\" /><figcaption></figcaption></figure><p>You can click next all the way to the final view, leaving the defaults as you go. You'll be presented with an Access key ID and Secret access key. Take note of these credentials as we'll need to add them into GitHub when we create our action. You also wont be able to retrieve the secret once you navigate away from this view.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/b90a28e2-36fd-40bc-b747-a08eb80ed588-image.png\" /><figcaption></figcaption></figure><h2>Creating the GitHub Action</h2><p>GitHub Actions are tools that allow you to perform actions with your code. Most commonly they are used to automate building, testing, and deployment of code. If you haven't already, push your code up to a GitHub repo. Then open the repo and select the <strong>Actions</strong> tab. From there, select <strong>set up a workflow yourself</strong>.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/7b684fd6-9c43-4da0-b1f0-019f96bfb0cd-image.png\" /><figcaption></figcaption></figure><p>You'll be presented with a text editor to build your Action. Paste the following code into yours. Note the comments throughout as they explain what each section does.</p><pre class=\"language-yaml\"><code># This is a basic workflow to help you get started with Actions\n\nname: Deploy to AWS\n\non:\n  # Triggers the workflow on push or pull request events but only for the main branch\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n  build:\n    # The type of runner that the job will run on\n    runs-on: ubuntu-latest\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      # Setup NodeJS in our environment\n      - name: Setup Node.js environment\n        uses: actions/setup-node@v2.1.2\n\n      # Runs a set of commands using the runners shell\n      - name: Run a multi-line script\n        run: |\n          npm install\n          npm run build\n\n      # Syncs your 'dist' folder from buildng the Vue app with an S3 bucket\n      - name: S3 Sync\n        uses: jakejarvis/s3-sync-action@v0.5.1\n        with:\n          args: --acl public-read --delete\n        env:\n          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          AWS_REGION: 'us-east-1'\n          SOURCE_DIR: 'dist'\n\n      # Clears the CloudFront cache so new requests will receive the latest version of your app\n      - name: Invalidate Cloudfront\n        uses: chetan/invalidate-cloudfront-action@v1.3\n        env:\n          DISTRIBUTION: ${{ secrets.DISTRIBUTION }}\n          PATHS: '/*'\n          AWS_REGION: 'us-east-1'\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n</code></pre><p>Once done, click on <strong>Start Commit</strong> then <strong>Commit new file</strong>.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/28b3f140-aaf8-4ada-acd1-8d88633ac4e3-image.png\" /><figcaption></figcaption></figure><p>Notice all the <code>${{ secrets.THING }}</code> lines? Those are variables that we can store in the Settings of the repository so they aren't stored with the code. Since storing secrets mixed in with your code is bad practice, this is the route we will go. We've already captured the AWS Key ID & Secret Key, so we just need the S3 Bucket name and CloudFront Distribution ID. You can get these from their respective services as highlighted below.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/d469d264-2e94-4d98-b415-a433eba7636f-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/43f55240-882e-4d08-bfb2-a1706e390786-image.png\" /><figcaption></figcaption></figure><p>Now back in GitHub, head to Settings, then Secrets, and finally add a separate secret for each item we had in our YAML file from earlier. The four we need are.</p><ul><li>AWS_S3_BUCKET</li><li>AWS_ACCESS_KEY_ID</li><li>AWS_SECRET_ACCESS_KEY</li><li>DISTRIBUTION</li></ul><p>Here is an example of me creating one.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/bb9b5363-a62b-455e-ba0b-3163c3443092-image.png\" /><figcaption></figcaption></figure><p>And here is the list once they are all added.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/937ce3ba-119f-4dd5-bb6c-5d1dd5b5a181-image.png\" /><figcaption></figcaption></figure><p>Once thats done, lets test everything by pulling down the new commits from our repo (creating the Action actually adds a commit to our repo) and changing a line of code somewhere. I simply changed the text thats displayed on my home page from \"Welcome to Your Vue.js App\" to \"Welcome to my To Do App!\".</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/5b07cd38-185d-423d-bd9f-f8d8faa2a3cc-image.png\" /><figcaption></figcaption></figure><p>Commit & push your code, then head back to the GitHub Actions tab. You should see your Action running and deploying your code. Once its done, you'll get a green check next to it if everything went according to plan.</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/a7046c5e-7d7a-4502-aa40-3954c9b03a4b-image.png\" /><figcaption></figcaption></figure><p>Now lets check the updated app (you might need to refresh your browser to clear its locally cached version).</p><figure><img src=\"/img/n/automate-deployments-to-aws-with-github-actions/c2f1df80-a873-4973-b4fd-c84d090e554e-image.png\" /><figcaption></figcaption></figure></div>","featuredImage":"/img/n/automate-deployments-to-aws-with-github-actions/27f49f58-3f1b-4903-9f57-807fb8eb7742-automate-deployments-to-aws-with-github-actions-featured-image.jpg","excerpt":"In this article, I'm going to cover how you can setup GitHub Actions to automatically deploy changes to AWS using S3 ...","cachedOn":1681394494},{"id":"7fb76436-2732-4872-be60-0e325ff992c2","notion_id":"7fb76436-2732-4872-be60-0e325ff992c2","status":"Published","slug":"hosting-a-vue-app-in-aws-s3","relation_series":["f36d703d-ef13-4706-bc71-0e61c27f6ae7"],"publishOn":"2021-02-16T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2021-02-vue-aws-app-hosting","seriesOrder":1,"youTubeURL":"https://youtu.be/UkK60BX0giU","title":"Hosting a Vue App in AWS S3","html":"<div><p>This article kicks off the first in a series on how to host Vue apps using AWS services, and how to automate deployment with GitHub Actions. The only hard prerequisite from the AWS standpoint is that you have a free AWS account, but general familiarity with any of these services will certainly help.</p><p>The other things I'll assume you have installed are;</p><ul><li>VSCode</li><li>NodeJS/NPM</li><li>Vue CLI (more <a href=\"https://brianmorrison.me/blog/getting-started-with-vuejs\" target=\"_blank\">here</a> if you need help)</li></ul><h2>Creating a Vue App</h2><p>Lets start by creating a basic Vue app. Open a new folder in VSCode and issue <code>vue create my-todo-app</code> to bootstrap the app. You can select the default options with Vue 2.</p><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/3abaa925-35e0-4ccc-a9ca-9ca71a63a0db-image.png\" /><figcaption></figcaption></figure><p>Once its done, lets issue the npm run build command to build the app into the dist folder.</p><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/935a877c-02bc-4de8-849e-30d7ab19237f-image.png\" /><figcaption></figcaption></figure><h2>Uploading to AWS S3</h2><p>AWS S3 is what's called a BLOB storage service, which means its designed to hold files and other types of unstructured data. The easiest comparison is to your computers file system, although it has other functionality that makes it useful for storing files in the cloud. S3 will be the primary way we can host the files that make up our app.</p><p>To start, head into the AWS console, then to S3, and create a bucket. Buckets in S3 are the top level storage container for the files. You'll need to name it something unique to your app, it cant share the same name as any other existing S3 buckets, even those in other accounts. You'll also want to make sure to check Public Access since our app will need to be publicly accessible.</p><p>Open the S3 console by heading into the Services menu, searching for S3, then selecting it. Once there, click Create Bucket.</p><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/2614ffb9-7ba1-4b6b-8834-0bb9c9c4abf7-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/423c7230-b327-4584-901e-794dd3bd4149-image.png\" /><figcaption></figcaption></figure><p>Set the unique name of the bucket, uncheck the box for disabling public access, acknowledge that change, then scroll to the bottom and click Create.</p><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/7aa9c192-f12a-4270-a265-901a66034af4-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/7f52d409-b0d5-4fa9-a8b1-4b9e27640037-image.png\" /><figcaption></figcaption></figure><p>Once the bucket is created, open it by clicking the name of it. Now we need to upload our files. On your computer, open the <em>dist</em> folder that was created in the previous section and drag the contents of that folder onto the S3 console where it says <strong>Drag and drop files...</strong> to upload them.</p><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/9a4acd5f-c2ce-4921-a329-8d376216da00-image.png\" /><figcaption></figcaption></figure><p>Once you do, you'll be presented with a new screen asking what to do with the files. Scroll down a bit and expand <strong>Additional upload options</strong>, then go to the <strong>Access control list</strong>, section. Enable Read access on objects for Everyone and confirm the warning below. then scroll to the button and click <strong>Upload</strong>. Once the upload is finished, click <strong>Exit</strong> in the upper right.</p><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/c4a40c57-e03d-455b-9d96-f13b4496ebd2-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/0cda2075-efbe-48ed-b2be-0b271e0ab655-image.png\" /><figcaption></figcaption></figure><p>The last thing we need to do is enable the public website feature of S3. Do this by going into <strong>Properties</strong>, scroll to <strong>Static website hosting</strong>, click <strong>Edit</strong>, and <strong>Enable</strong> it. You'll need to specify the <strong>Index</strong> and <strong>Error</strong> documents. These can both be <strong>index.html</strong> and <strong>error.html</strong> respectively. Finally <strong>Save changes</strong>.</p><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/23c0df9d-5b7b-4446-9934-acb626ac2606-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/7e5fa304-0cf3-4f29-af70-e41f4a0d6186-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/706f4ca7-0967-48e1-9598-d97b6e747fff-image.png\" /><figcaption></figcaption></figure><p>Back in the Properties tab, scroll to <strong>Static website hosting</strong> again. You'll be given a URL here that you can use to access your web app. Lets check it out.</p><figure><img src=\"/img/n/hosting-a-vue-app-in-aws-s3/5a7141cd-f023-409d-b5c6-0651fbc00568-image.png\" /><figcaption></figcaption></figure><p>Success! In the next article, I'll cover how we can host our app in CloudFront using a custom domain.</p></div>","featuredImage":"/img/n/hosting-a-vue-app-in-aws-s3/3a75c83c-6a12-4752-b530-932ce0970053-hosting-a-vue-app-in-aws-s3-featured-image.jpg","excerpt":"This article kicks off the first in a series on how to host Vue apps using AWS services, and how to automate deployme...","cachedOn":1681394497},{"id":"941b16ed-663e-4aa8-825a-792affd1096d","notion_id":"941b16ed-663e-4aa8-825a-792affd1096d","status":"Draft","slug":"migrating-my-content-from-wordpress-to-notion","relation_series":[],"publishOn":"2023-04-11T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Migrating my content from WordPress to Notion","html":"<div><p>process</p><ul><li>create new poast</li><li>copy meta fields, including featured image</li><li>copy paste content</li><li>fix code block syntax</li><li>download, reup pics</li></div>","excerpt":"process ","cachedOn":1681394498},{"id":"5ae704dd-dca4-4357-a294-c1ddf8675595","notion_id":"5ae704dd-dca4-4357-a294-c1ddf8675595","status":"Published","slug":"publishing-a-vue-app-to-aws-cloudfront-cdn","relation_series":["f36d703d-ef13-4706-bc71-0e61c27f6ae7"],"publishOn":"2020-02-16T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2021-02-vue-aws-app-hosting","seriesOrder":2,"youTubeURL":"https://youtu.be/UkK60BX0giU","title":"Publishing a Vue App to AWS CloudFront CDN","html":"<div><p>In the previous article of the series, we uploaded a Vue To Do app to AWS S3. Today, we'll learn how to CloudFront to optimize performance of our app. This tutorial can be applied to any S3 bucket though, the process is actually the same.</p><h2>What is CloudFront</h2><p>CloudFront is AWS's Content Distribution Network (or CDN). The purpose is to distribute your app or website to the 200+ distribution points around the world. When a user requests your content, AWS will serve it from the closest distribution point to the user, which results in quicker load times.</p><h2>Connecting CloudFront to S3</h2><p>Lets start by opening CloudFront in the AWS Console. Once there, click Create Distribution. Select Get Started under Web if it asks.</p><figure><img src=\"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/81f33a2c-9578-4e24-9a59-2e08d0403dc0-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/5dbf8d07-ee91-4660-a305-f4dfa36a7820-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/a2d1d8e4-b34e-4fea-9c93-087fbd6b882b-image.png\" /><figcaption></figcaption></figure><p>You'll be presented with a form to complete at this point. Normally you'd need to specify a URL for your Origin (or where your content is originally stored at), but CloudFront integrates nicely with S3 so when you click that box, it will show your current buckets. Select the S3 bucket you wish to publish. Set <strong>Viewer Protocol Policy</strong> it to <strong>Redirect HTTP to HTTPS,</strong> then scroll down and set <strong>Default Root Object</strong> to <strong>index.html.</strong> Finally, click Create Distribution.</p><figure><img src=\"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/c87e23c9-276e-4a2b-9ae1-b61af696b5e0-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/31e2dbdc-5818-4393-8d06-2857600d86dc-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/21dea917-da3a-46fe-bc52-0f2c6af78f70-image.png\" /><figcaption></figcaption></figure><p>You'll be brought back into your list of CloudFront Distributions. It takes a while for AWS to distribute your content, so wait until the Status shows <strong>Deployed</strong>.</p><figure><img src=\"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/fb5f0f37-1354-46cb-9549-a91bf6dccb9d-image.png\" /><figcaption></figcaption></figure><p>Now of we copy the Domain Name listed in the table and paste that into a new tab in our browser, we should see our app again!</p><figure><img src=\"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/05ea2571-a39a-499f-aedb-ac291aa7af98-image.png\" /><figcaption></figcaption></figure><p>In the last article of this series, I'll cover how you can use GitHub actions to automate this process every time changes to your code are made!</p></div>","featuredImage":"/img/n/publishing-a-vue-app-to-aws-cloudfront-cdn/1a1e68ec-ecf2-4baf-8ec8-d2b1d4e713b9-publishing-a-vue-app-to-aws-cloudfront-cdn-featured-image.jpg","excerpt":"In the previous article of the series, we uploaded a Vue To Do app to AWS S3. Today, we'll learn how to CloudFront to...","cachedOn":1681394499},{"id":"b6a3102c-b892-4d6d-b2cf-66bb79aeab92","notion_id":"b6a3102c-b892-4d6d-b2cf-66bb79aeab92","status":"Published","slug":"import-discord-bot-commands-dynamically","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-15T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-7-import-discord-bot-commands-dynamically","seriesOrder":7,"youTubeURL":"https://youtu.be/18euIoOZYjU","title":"Import Discord Bot Commands Dynamically","html":"<div><p>Over the past few weeks, we’ve explored a lot of different ways that you can use a Discord Bot and communicate with it, have it communicate back, build interactive commands, etc. All of this with one interesting issue, we’re putting all of our commands into one file in a giant <code>if</code> statement. It works, but its not pretty.</p><p>Today we’re going to fix that by setting up the <strong>index.js</strong> file to scan a directory and import all of our commands for is. This will provide a clean way to separate our commands into separate files which helps with organization and extending it going forward.</p><h2>Reorganizing Our Commands</h2><p>We’re going to put each command  into a separate file to keep things organized. Create a folder called <strong>commands</strong> and then a file called <strong>hello.js</strong>. You guessed it, were going all the way back to the beginning! But trust me, it will be worth it. We’re going to structure our command file into two parts to be exported; the prefix & the function.</p><p>Here is the <strong>hello.js</strong> file;</p><pre class=\"language-javascript\"><code>module.exports = {\n  // Define the prefix\n  prefix: \"!hello\",\n  // Define a function to pass the message to\n  fn: (msg) =&gt; {\n    msg.reply(\"world!\")\n  }\n}\n</code></pre><h2>Scanning for Commands</h2><p>Back in <strong>index.js</strong>, we need to add the code to scan our <strong>commands</strong> directory for what to import. To do this, we’re going to use <code>fs.readdirSync()</code> to get a list of the filenames, the iterate over the files to parse the prefix & function. Put this code after your other <code>require</code> statements, but before your <code>logon</code> event handler;</p><pre class=\"language-javascript\"><code>const fs = require('fs')\n\n// Create the empty commands object\nconst commands = {}\n// Get the file names from the commands directory\nconst files = fs.readdirSync('./commands')\n// Filter out any non-js files\nconst jsFiles = files.filter(file =&gt; file.endsWith('.js'))\n// Foreach, require the file, check for the right exports, then add to the commands object\njsFiles.forEach(commandFile =&gt; {\n  const command = require(`./commands/${commandFile}`)\n  if (command.prefix && command.fn) {\n    commands[command.prefix] = command.fn;\n  }\n})\n</code></pre><p>So now that our commands have been registered to the <code>commands</code> object, we need to check the incoming command to see if it matches. Lets remove everything from the start of our <code>message</code> handler through the <code>!hello</code> block and replace it with the following code.</p><pre class=\"language-javascript\"><code>// Grab the prefix, which should be the content before the first space\nconst prefix = msg.content.split(' ')[0];\n// Filter out bad commands and bots\nif (commands[prefix] === undefined || msg.author.bot) {\n  return\n}\n\n// Execute the fn of the prefix object, passing in the message\ncommands[prefix](msg);\n</code></pre><p>Now try and issue the <code>!hello</code> command in your server. The bot should respond just the same as it did before.</p><figure><img src=\"/img/n/import-discord-bot-commands-dynamically/7dd1f70d-5504-49d7-84d3-443c8363423a-Untitled.png\" /><figcaption></figcaption></figure><p>Now we’ve run into another issue, none of our other commands work! This is because were checking the input up front and if it doesn’t match a command registered in our <code>commands</code> array, the bot basically quits. The solution is quite simple though, you’ll need reorganize the other commands in the same way. I’ll leave that one to you, but you can always check the GitHub repo associated with this project to guide you along.</p><p>In our next article, I’ll explain how you can use your bot to integrate with third party APIs.</p></div>","excerpt":"Over the past few weeks, we’ve explored a lot of different ways that you can use a Discord Bot and communicate with i...","cachedOn":1681394503},{"id":"ffd81207-4a84-4274-9ba8-c7845f9ed9fa","notion_id":"ffd81207-4a84-4274-9ba8-c7845f9ed9fa","status":"Published","slug":"building-custom-trello-power-ups","relation_series":["c22b85bc-06f6-4aa6-a555-e82dea0444dc"],"publishOn":"2020-12-10T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-12-trello-powerup","seriesOrder":3,"youTubeURL":"https://youtu.be/QbqRd-6bhhk","title":"Building Custom Trello Power-Ups","html":"<div><p>Trello Power-Ups are like plugins for Trello that can extend its existing functionality and integrate external services. Today we're going to build a simple Power-Up, deploy it to Netlify, and add it to a board. Basic knowledge of JavaScript is required to really follow along. I'll be using VSCode as my code editor, but any editor will work.</p><h2>Building the Power-Up</h2><p>Power-Ups are hosted in an IFrame within Trello, so we need to basically create a website with some JavaScript that Trello will use to add the functionality itself. In your editor, create a new HTML file with the following in it, save it to an empty folder as <strong>index.html</strong>.</p><pre class=\"language-html\"><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n  &lt;meta charset=\"utf-8\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n  &lt;script src=\"https://p.trellocdn.com/power-up.min.js\"&gt;&lt;/script&gt;\n  &lt;script&gt;\n    TrelloPowerUp.initialize({\n      // Start adding handlers for your capabilities here!\n      'card-buttons': function (t, options) {\n        return [\n          {\n            icon: \"https://i.ibb.co/6sbknH0/logo-stroke-32pt-style-2-no-stroke.png\",\n            text: 'Hello World!',\n            callback: function (t) {\n              return t.card().then((card) =&gt; alert(\"hello world!\"))\n            }\n          }\n        ];\n      },\n    });\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>Here is what this little Power-Up will do to start.</p><ul><li>Add a button to every card.</li><li>When the button is clicked, it will display a browser alert of \"hello world!\"</li></ul><h2>Deploying to Netlify</h2><p>Deploying a Power-Up to Netlify is just the same as deploying any other site to Netlify. If you don't have an account, you can create one for free at https://app.netlify.com/signup/email. Once logged in, head over to sites, scroll to the bottom, and you'll see a place where you can drop a folder. Go ahead and drag the folder that contains the index.html file we created into this area. You'll then be presented with a new view that lists a URL for your site. This is what we will use to register the Power-Up with.</p><figure><img src=\"/img/n/building-custom-trello-power-ups/65e5d91d-94b1-459f-b233-885ec281c3ed-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-custom-trello-power-ups/4fd7e48b-d1b6-4154-b230-54cef82fef16-image.png\" /><figcaption></figcaption></figure><h2>Creating a Power-Up App</h2><p>To get the Power-Up working in Trello, you'll need to register it. Head to <a href=\"https://trello.com/power-ups/admin\" target=\"_blank\">https://trello.com/power-ups/admin</a> and click on <strong>Create New Power-Up</strong>. Give your Power-Up a name, select a team to use the Power-Up, then paste in the URL from Netlify.</p><figure><img src=\"/img/n/building-custom-trello-power-ups/0e720edb-b8f1-432e-b723-80da48e03b40-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-custom-trello-power-ups/24541055-8a48-4129-ba3e-7265c9e65314-image.png\" /><figcaption></figcaption></figure><p>Once you click Create, you'll be taken to the Power-Up Management Screen. Once there, head over to <strong>Capabilities</strong> on the left. This will show you all the functionality that can be built with Power-Ups. For our little test, we need to enable Card Buttons, so find that in the list and enable it.</p><figure><img src=\"/img/n/building-custom-trello-power-ups/0c9d5a00-eb77-4bdd-8fc0-d5d9cc34a8f1-image.png\" /><figcaption></figcaption></figure><h2>Testing our Power-Up</h2><p>Now that our App is setup, lets add it to the board. You'll need to select a board that is part of the team that the power up was added to. Once your board is open, open the Menu, click on Power-Ups, select Custom, click Add under the name of your Power-Up, and then Add one more time.</p><figure><img src=\"/img/n/building-custom-trello-power-ups/bc703d58-b00c-4a1e-ba4a-5872bfb1a3d5-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-custom-trello-power-ups/cc856725-9ac6-4bd0-991a-8d5dfc87a3d0-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-custom-trello-power-ups/3fd9ba0e-41a2-46b0-8e62-14aba6a9e8b7-image.png\" /><figcaption></figcaption></figure><p>Now open a card and you'll see a new Hello World button on the right side. Click it and you should get an alert pop up, which is exactly what we built and deployed to Netlify.</p><figure><img src=\"/img/n/building-custom-trello-power-ups/7307933c-6747-4690-8d30-46f691bb670b-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-custom-trello-power-ups/876be37d-42f4-45c9-8618-5ae288f5fe28-image.png\" /><figcaption></figcaption></figure><h2>Getting Card Details</h2><p>Now that we have a Power-Up built and working, lets expand it just a bit. We're going to grab the name & id of the card, then add that to our alert box. This is simply to demonstrate how you can get data from the card to use in your code however you need to.</p><p>Lets go back to our <strong>Index.html</strong> and update it like so. Note the changes on the line starting with <code>return t.card(...</code></p><pre class=\"language-html\"><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n  &lt;meta charset=\"utf-8\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n  &lt;script src=\"https://p.trellocdn.com/power-up.min.js\"&gt;&lt;/script&gt;\n  &lt;script&gt;\n    TrelloPowerUp.initialize({\n      // Start adding handlers for your capabilities here!\n      'card-buttons': function (t, options) {\n        return [\n          {\n            icon: \"https://i.ibb.co/6sbknH0/logo-stroke-32pt-style-2-no-stroke.png\",\n            text: 'Hello World!',\n            callback: function (t) {\n              return t.card('id', 'name').then((card) =&gt; alert(`Alert on card ${card.name} (id: ${card.id})`))\n            }\n          }\n        ];\n      },\n    });\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>Now you can redeploy your site to Netlify by going into your site, opening Deploys from the top bar, and dragging your folder in again.</p><figure><img src=\"/img/n/building-custom-trello-power-ups/a4d4d4ef-8000-4515-a1d4-e09c6a78fce6-image.png\" /><figcaption></figcaption></figure><p>Once its redeployed, refresh your Trello page and lets test the button again. You should now get an updated alert box.</p><figure><img src=\"/img/n/building-custom-trello-power-ups/c09c2f77-1928-42f3-a0d6-ad4dfe3097cc-image.png\" /><figcaption></figcaption></figure><h2>Adding a Board Button</h2><p>For one more experiment, lets add a button to the board now. In this updated code, I added another object into <code>TrelloPowerUp.initialize</code> for the board button.</p><pre class=\"language-html\"><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta charset=\"utf-8\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;script src=\"https://p.trellocdn.com/power-up.min.js\"&gt;&lt;/script&gt;\n  &lt;script&gt;\n    TrelloPowerUp.initialize({\n      // Start adding handlers for your capabilities here!\n      'card-buttons': function (t, options) {\n        return [\n          {\n            icon: \"https://i.ibb.co/6sbknH0/logo-stroke-32pt-style-2-no-stroke.png\",\n            text: 'Hello World!',\n            callback: function (t) {\n              return t.card('id', 'name').then((card) =&gt; alert(`Alert on card ${card.name} (id: ${card.id})`))\n            }\n          }\n        ];\n      },\n      'board-buttons': function (t, options) {\n        return [\n          {\n            icon: \"https://i.ibb.co/6sbknH0/logo-stroke-32pt-style-2-no-stroke.png\",\n            text: 'Hello Board!',\n            callback: function (t) {\n              return t.board().then((board) =&gt; alert(\"Hello from the Board!\"))\n            }\n          }\n        ];\n      }\n    });\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>Before you deploy to Netlify, head back to <a href=\"https://trello.com/power-ups/admin\" target=\"_blank\">https://trello.com/power-ups/admin</a>, open your app and enable Board Buttons under Capabilities.</p><figure><img src=\"/img/n/building-custom-trello-power-ups/c9bb4a78-6fd9-480c-b68f-7fb1ad4613b9-image.png\" /><figcaption></figcaption></figure><p>Redeploy to Netlify and refresh your Trello window. You should have a new button on the board. Click it and you should receive an alert from the Board!</p><figure><img src=\"/img/n/building-custom-trello-power-ups/bd5c9559-11a4-4079-8ac0-61d0178a98b0-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-custom-trello-power-ups/b2b8913b-0b63-4ec2-b19e-6cbaeb4d0bdd-image.png\" /><figcaption></figcaption></figure><p>Those are only two of the many ways Power-Ups can be used. If you've built something cool, feel free to let me know on Twitter at <a href=\"https://twitter.com/brianmmdev\" target=\"_blank\">@brianmmdev</a>!</p></div>","featuredImage":"/img/n/building-custom-trello-power-ups/3e17f1c1-3a21-4433-9d32-b77361deb164-building-custom-trello-power-ups-featured-image.jpg","excerpt":"Trello Power-Ups are like plugins for Trello that can extend its existing functionality and integrate external servic...","cachedOn":1681394505},{"id":"afac8c5b-65fa-4b90-8a28-56fbdea9f152","notion_id":"afac8c5b-65fa-4b90-8a28-56fbdea9f152","status":"Published","slug":"third-party-trello-integrations-using-webhooks","relation_series":["c22b85bc-06f6-4aa6-a555-e82dea0444dc"],"publishOn":"2020-12-07T00:00:00.000Z","codeURL":null,"seriesOrder":2,"youTubeURL":"https://youtu.be/79SgNOU_hPo","title":"Third Party Trello Integrations using Webhooks","html":"<div><p>If you've ever used Trello but wanted to integrate it with another app, then the webhooks system used by Trello is exactly what you need.</p><h2>How Webhooks Work</h2><p>Webhooks are automated messages triggered by events that are sent from one system to another, generally over HTTP. If you've ever used a platform like Zapier, IFTTT, or Microsoft PowerAutomate, these platforms are mostly just a GUI built on top of registering and handling webhooks between multiple systems.</p><p>So in the context of Trello, an event can be described as whenever something is changed;</p><ul><li>A new card is created.</li><li>A board is updated.</li><li>A card is added to a list.</li></ul><p>So whenever these events occur, Trello will post a message to an HTTP endpoint of your choosing with the data of whatever model you've created the webhook for. And a model is the <strong>thing</strong> in Trello (card, list, board, etc).</p><p>So to see this in action, lets create a webhook on a Board, which will by default send any and all events that occur on that board.</p><h2>Creating Webhooks</h2><p>Trello does not have any UI around creating a webhook, it needs to be done through their API. If you are not familiar their API, I wrote another article that outlines the basics of using the API <a href=\"https://brianmorrison.me/blog/automation-with-the-trello-api\" target=\"_blank\">here</a>. I'd recommend giving that a read before continuing.</p><p>Besides the key & token, there are two parameters required.</p><ul><li><code>idModel</code> - The unique Id of any entity in Trello (card, list, board, etc).</li><li><code>callbackURL</code> - The URL which Trello should POST messages to.</li></ul><p>For testing, I'm going to use a tool called <a href=\"https://webhook.site/\" target=\"_blank\">https://webhook.site</a> which lets you spin up HTTP endpoints to use for webhook testing. When you go there, you'll automatically be given a URL to use for accepting messages.</p><figure><img src=\"/img/n/third-party-trello-integrations-using-webhooks/8ea6f2b7-c4f9-4810-b353-f5832e5f1783-image.png\" /><figcaption></figcaption></figure><p>Now using Postman, lets create a webhook using the Trello API.</p><pre class=\"language-plain text\"><code>POST https://api.trello.com/1/webooks?key={yourKey}&token={yourToken}&callbackURL={callbackURL}&idModel={idModel}</code></pre><figure><img src=\"/img/n/third-party-trello-integrations-using-webhooks/98ec0844-8d1b-4657-941c-482c2f281461-image.png\" /><figcaption></figcaption></figure><p>Provided you got a 200 response with some info about the webhook, you should be good to go.</p><h2>Testing the Webhook</h2><p>This part is actually really simple. All you have to do is change something on the board, anything will do. Lets rename one of the cards and see what came in on our <a href=\"http://webhook.site/\" target=\"_blank\">webhook.site</a> instance.</p><p>As you can see, there is a ton of info that is sent whenever event data is sent to our test API (the picture does not do it justice). And at this point, its just a matter of figuring out what to do with all this data.</p><figure><img src=\"/img/n/third-party-trello-integrations-using-webhooks/b0f2b26e-0a92-40a4-9839-1933d5fce8e8-image.png\" /><figcaption></figcaption></figure><h2>A Real World Example</h2><p>So how could this be used in the real world? I actually built a sync system between Trello & Todoist using webhooks on both sides and a series of Netlify Functions. This let me sync the status of tasks in Todoist with a checklist inside a card in Trello. I used Trello to track projects & project status, and Todoist to track individual tasks.</p><p>Whenever I closed a task in Trello, it would close in Todoist, and vice versa. Building the entire system took a few hours to make sure it did exactly what I needed, but it saved me a TON of time by not requiring me to manually copy tasks back and forth.</p><p>In the last article of this series, I'll build a small Power Up that can be used to send messages to an API as a way to automate via Trello's UI.</p></div>","featuredImage":"/img/n/third-party-trello-integrations-using-webhooks/55864a8b-3cf2-4fcb-9437-cfc458489d0d-third-party-trello-integrations-using-webhooks-featured-image.jpg","excerpt":"If you've ever used Trello but wanted to integrate it with another app, then the webhooks system used by Trello is ex...","cachedOn":1681394519},{"id":"b98a41bb-1a4d-49e7-b6aa-7083e8b912c0","notion_id":"b98a41bb-1a4d-49e7-b6aa-7083e8b912c0","status":"Published","slug":"automation-with-the-trello-api","relation_series":["cacc9e5f-3d04-4193-97ab-ddbcc2a87a16"],"publishOn":"2020-12-03T00:00:00.000Z","codeURL":null,"seriesOrder":1,"youTubeURL":"https://youtu.be/HvMDbDxnDxs","title":"Automation With the Trello API","html":"<div><p>Trello is an extremely popular Kanban tool that can easily be used to manage a variety of things, from projects to idea boards. One of the most powerful features though is the API, which can be used to modify pretty much anything within Trello. In this article, we're going to touch on some of the different things you can do with the Trello API such as creating & updating cards, but the concepts are exactly the same across all endpoints. Lets dive in.</p><p>Before following along, install Postman if you haven't already. We'll be using this to test a few calls to the API.</p><h2>Exploring the Docs</h2><p>Trello actually has three different APIs. The one we will be using is the Rest API, which is documented at <a href=\"https://developer.atlassian.com/cloud/trello/rest/api-group-actions/\" target=\"_blank\">https://developer.atlassian.com/cloud/trello/rest/api-group-actions/</a>. If you continue developing with Trello, this is a link you'll want to keep handy as it outlines every possible way you can use their API.</p><h2>Getting Your API Credentials</h2><p>In order to authenticate against the Trello API, we'll need to create a Trello App so that we can get an API Token & Key. Head over to <a href=\"https://trello.com/app-key\" target=\"_blank\">https://trello.com/app-key</a>, acknowledge the terms if prompted, and lick Show API Key. You should see a page like this. Save your Key off somewhere since we will need it for all API requests.</p><figure><img src=\"/img/n/automation-with-the-trello-api/8b65a265-869a-46c7-93bc-1a81651beb57-image.png\" /><figcaption></figcaption></figure><p>Next click on Token. You should be redirected to a page saying that Server Token (or something similar) is requesting access to your account. Scroll to the bottom & click Allow. You'll be provided a Token that is required for requests as well.</p><p>Anyone who has your token can access your account, so treat it like a password!</p><figure><img src=\"/img/n/automation-with-the-trello-api/6f1d1398-d22b-4ba1-ae38-33e77243ea86-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/automation-with-the-trello-api/3ba885fa-3a7f-47fd-b2eb-aa11d46c5673-image.png\" /><figcaption></figcaption></figure><h2>API Basics</h2><p>Trello is a bit different than other APIs in the way they receive data. Most calls will not leverage the HTTP body and when you send data, you'll send it as query parameters in the URL like so.</p><pre class=\"language-plain text\"><code>https://api.trello.com/1/boards/{idBoard}?key={yourKey}&token={yourToken}</code></pre><p>While scanning the docs, you'll notice the endpoints are displayed without a domain. To use them, just add <code>[https://api.trello.com](https://api.trello.com)</code> in front of them and you should be good to go.</p><p>Another thing worth noting is that all calls will require the <code>key</code> & <code>token</code> to be sent. This is what authenticates you to Trello, your request will be denied otherwise. Lets test a few API calls now.</p><h2>Testing the API</h2><p>In Trello, Cards are pretty much the base entity of the system. Anything that is tracked in Trello is done via Cards, so we're going to test a few things around Cards. Lets take a look at the URL you can use to create a card;</p><pre class=\"language-plain text\"><code>POST https://api.trello.com/1/cards?key={yourKey}&token={yourToken}&name={cardName}&idList={idList}</code></pre><p>Besides the <code>token</code> and <code>key</code> params, the only other field technically required for this call is <code>idList</code> (which as you can assume, is the unique ID of the list you want the card to be added to), but the card isn't going to be very useful if you don't at least provide the <code>name</code> field as well.</p><p>To find the Id of any list or board, you could use those APIs, but an easier method is to export a board to JSON to see all of the embedded data for that board (which will include lists, cards, etc). To do this, open the board menu, select More, Print and Export, and finally Export as JSON. If you are using Chrome with the <a href=\"https://chrome.google.com/webstore/detail/json-formatter/bcjindcccaagfpapjjmafapmmgkkhgoa/related?hl=en\" target=\"_blank\">JSON Formatter</a> extension, it should just display the JSON in a nicely formatted document that you can navigate to pull out the info you are interested in.</p><figure><img src=\"/img/n/automation-with-the-trello-api/0d849197-3d33-4b7a-a613-36e82a85fda6-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/automation-with-the-trello-api/b8923af9-360d-4db2-a045-92e23ca96e0f-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/automation-with-the-trello-api/a9741794-6869-4d20-9fe6-6c57cc315933-image.png\" /><figcaption></figcaption></figure><p>Now if you scroll down to the bottom, you can see an array of lists. In the starter board, there are already a couple lists, so I'm going to grab the Id of the To Do list.</p><figure><img src=\"/img/n/automation-with-the-trello-api/fb1d5a66-0a15-4db8-8461-56f6aebea474-image.png\" /><figcaption></figcaption></figure><p>So using Postman, I can now create a new card in this list by submitting a POST to the following URL. Note that any data sent in the URL must be properly encoded for Trello to recognize it (which is why the name is <code>My%20new%20card</code> instead of <code>My new card</code>).</p><pre class=\"language-plain text\"><code>https://api.trello.com/1/cards?key={yourKey}&token={yourToken}&name=My%20new%20card&idList=5f98543170234f4baf5e662b\n</code></pre><p>Trello will respond with the data about the new card that was created if it was successful.</p><figure><img src=\"/img/n/automation-with-the-trello-api/f047a21c-3e62-4a40-9b68-ae5935d011c9-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/automation-with-the-trello-api/d6975892-2375-44ba-b656-a58fbbf0e977-image.png\" /><figcaption></figcaption></figure><p>Now lets say we have started working on our new card and we want to move it to the Doing list. To do this, we'll send a PUT request to the cards endpoint with the Id of the card at the end of the url, like so. (I used the same Export function as above to get the cards Id, and the Id of the list I want to send it to)</p><pre class=\"language-plain text\"><code>https://api.trello.com/1/cards/5f9857489a717e25e80760a9?key={yourKey}&token={yourToken}&idList=5f98543170234f4baf5e662c\n</code></pre><p>As you can see we still receive the card data back, but if we check the board, it has moved to the next column.</p><figure><img src=\"/img/n/automation-with-the-trello-api/6158f149-c070-446b-857a-d39608cbf2ea-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/automation-with-the-trello-api/ec0c64af-6556-42f1-9a74-4ea8a37e290e-image.png\" /><figcaption></figcaption></figure><h2>Whats Next</h2><p>Once you understand some of the basics of working with the Trello REST API, those concepts translate across all endpoints and concepts in the API. At this point, its just a matter of deciding what you want to create or modify, finding the endpoint definition in the docs, and writing your HTTP requests to what you need it to.</p><p>In the next article in this series, I'll explore how to create and use webhooks and have Trello automatically notify an external system when certain events happen.</p></div>","featuredImage":"/img/n/automation-with-the-trello-api/c02882a6-2915-4560-bc6d-adc1212d9ee6-automation-with-the-trello-api-featured-image.jpg","excerpt":"Trello is an extremely popular Kanban tool that can easily be used to manage a variety of things, from projects to id...","cachedOn":1681394521},{"id":"ba6fad2e-9654-445f-9619-dd428a1fa064","notion_id":"ba6fad2e-9654-445f-9619-dd428a1fa064","status":"Published","slug":"creating-standalone-apps-in-nodejs-with-pkg-js","relation_series":[],"publishOn":"2020-10-25T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/pkg-starter","seriesOrder":null,"youTubeURL":null,"title":"Creating Standalone Apps in NodeJS with pkg.js","html":"<div><p>You ever want to build a command line app with NodeJS to run on systems that might not have NodeJS installed? Then this article is for you. I'll be explaining how to do exactly that with a library called pkg.js. This article was inspired by a CLI utility I created for my work to send notifications to Microsoft Teams from our build servers.</p><p>The GitHub link for this article links to a starter repo I built to quickly get up and running with pkg.js.</p><h2>Prerequisites</h2><p>Before getting started, make sure you have the following installed;</p><ul><li>VSCode</li><li>NodeJS</li></ul><h2>Project Setup</h2><p>We're going to build a simple app that just logs \"Hello world!\" to the console. Open VSCode, pick a folder for your project, and initialize it by issuing <code>npm init -y</code> in the terminal. Then create a file called <strong>index.js</strong>. Inside the file, just write the following;</p><p><code>console.log(\"Hello world!\");</code></p><p>Test the app by running the following in the terminal.</p><p><code>node app.js</code></p><p>Provided you get the following output, everything is setup as expected.</p><figure><img src=\"/img/n/creating-standalone-apps-in-nodejs-with-pkg-js/ad810093-4f85-42e2-8e18-edca86365555-image.png\" /><figcaption></figcaption></figure><p>Now lets get pkg setup. Run the following in the terminal to install the library globally.</p><p><code>npm install -g pkg</code></p><p>Once the install is finished, head into <strong>package.json</strong> and add a node like so;</p><pre class=\"language-plain text\"><code>\"bin\": \"index.js\",</code></pre><p>What we're doing is telling the pkg library where to look for its entry point.</p><h2>Building Binaries</h2><p>By default, pkg will create binaries for Windows, Mac, and Linux. You can create these by issuing the following command in the terminal;</p><p><code>pkg .</code></p><p>Once finished, you should see three binaries in the root if your project;</p><figure><img src=\"/img/n/creating-standalone-apps-in-nodejs-with-pkg-js/fbff7b53-43c0-4510-b7fa-5dac3b90d9e0-image.png\" /><figcaption></figcaption></figure><p>I'm on windows, so I can run it by typing <code>.\\standalone-apps-pkg-win.exe</code> in the terminal. Lets make sure we get <strong>Hello world!</strong> again;</p><figure><img src=\"/img/n/creating-standalone-apps-in-nodejs-with-pkg-js/c8e41388-1a8f-414a-9fe1-acd57fa541a7-image.png\" /><figcaption></figcaption></figure><h2>Handling Arguments</h2><p>You can actually pass in arguments into your app as well like so;</p><pre class=\"language-bash\"><code>.\\myapp.exe --name='Brian'</code></pre><p>Which is exactly what we'll do now. Lets update <strong>index.js</strong> to look like so;</p><pre class=\"language-javascript\"><code>// Find the argument we're looking for\nlet name = process.argv.find(el =&gt; el.startsWith('--name='))\n\n// Get rid of the name of the argument\nname = name.replace('--name=', '')\n\n// Log the output\nconsole.log(\"Hello \" + name + \"!\")</code></pre><p>Now build your app again using <code>pkg .</code> and pass in the name argument like shown above (feel free to use your name 😉);</p><figure><img src=\"/img/n/creating-standalone-apps-in-nodejs-with-pkg-js/aabae91e-fe26-46dd-937c-58d003e247bd-image.png\" /><figcaption></figcaption></figure><p>The binaries built should be able to run on any system, regardless if Node is installed or not!</p></div>","featuredImage":"/img/n/creating-standalone-apps-in-nodejs-with-pkg-js/5a8aa975-4d6f-46fb-8e53-a540ac17c6c7-creating-standalone-apps-in-nodejs-with-pkg.js-featured-image.jpg","excerpt":"You ever want to build a command line app with NodeJS to run on systems that might not have NodeJS installed? Then th...","cachedOn":1681394522},{"id":"c589f61c-b6fc-42d2-8e4e-d973fe09b685","notion_id":"c589f61c-b6fc-42d2-8e4e-d973fe09b685","status":"Published","slug":"building-websites-with-gridsome-and-wordpress-part-3","relation_series":["8af313c9-2908-4c13-9065-0106fb547cd1"],"publishOn":"2020-10-22T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-10-gridsome-and-wordpress-pt3","seriesOrder":3,"youTubeURL":"https://youtu.be/2x5bh4hN9YM","title":"Building Websites with Gridsome and WordPress (Part 3)","html":"<div><p>WordPress was originally designed as a platform to write blog articles, but over the years, ambitious developers have found new and creative ways to store content in WordPress. Today, we're going to look at a few plugins that can be used to customize the WordPress schema, and we're going to update our Gridsome site to pull in these the new data.</p><h2>Installing the Plugins</h2><p>Before we can start extending the Schema, we'll need to install the following plugins. The first two are in the WordPress Plugin repo, but the second two must be installed via a manual upload.</p><ul><li>Custom Post Type UI - Used to create new Post Types, WordPress's base entity</li><li>Advanced Custom Fields - Used to add custom fields to Post Types</li><li>WPGraphQL CPT UI - Surfaces custom post types through the WPGraphQL plugin (download from (<a href=\"https://github.com/wp-graphql/wp-graphql-custom-post-type-ui/archive/master.zip\" target=\"_blank\">https://github.com/wp-graphql/wp-graphql-custom-post-type-ui/archive/master.zip</a>)</li><li>WPGraphQL ACF - Surfaces custom fields through the WPGraphQL plugin (download from <a href=\"https://github.com/wp-graphql/wp-graphql-acf/archive/master.zip\" target=\"_blank\">https://github.com/wp-graphql/wp-graphql-acf/archive/master.zip</a>)</li></ul><p>Once these are all installed, we can start building a custom post type.</p><h2>Extending the Schema</h2><p>We already have post types for Posts & Pages, but lets say we're going to start creating a training course to sell on our site. To enable this functionality on our main website, we'll create a custom post type with the following schema;</p><ul><li>Description - Rich text block</li><li>Price - Number</li><li>IsInPreview - True/False</li><li>ReleaseDate - Date Picker</li><li>Author - Text</li></ul><p>Lets get started by heading over to CPT UI > Add/Edit Post Types, populate the basic settings, then scroll to the bottom to setup the GraphQL settings. The other fields can be ignored. Once you are done, click Add Post Type to save it.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-3/bace7d96-2ab5-46f2-a783-85e5b6ca2fbb-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-3/0cb8b9b9-7278-4620-bf85-8ae1df86bec0-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-3/a25e88a3-42ba-42f1-a563-583dc350cc25-image.png\" /><figcaption></figcaption></figure><p>On the left nav, you should now have a new entry for Courses.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-3/36638a41-3dbb-4458-bcaa-fa3c3e170cb7-image.png\" /><figcaption></figcaption></figure><p>Now lets add those other fields we need. Head into Custom Fields then Add New. Lets name our group Course Fields. We can use the built in editor for the courses description, but the other fields will need to be added here. Click on Add Field to start adding them. You'll also need to update the Rules in Location to be Course instead of Post. The last thing to do before saving is scrolling down to the bottom and setting the GraphQL settings.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-3/aeeb9dd8-2e24-4279-9e72-3624d603865b-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-3/0f0bf20a-daad-4de9-a477-7145420b939f-image.png\" /><figcaption></figcaption></figure><p>Now lets add a course. Why not Building Websites with WordPress and Gridsome?! Make sure you add some text in the description and populate all the fields we just setup.</p><h2>Pulling into Gridsome</h2><p>Now that we have the data setup, lets fire up Gridsome and head to the explore URL to make sure we can get the data from the GraphQL layer. Execute the following query and you should see the new post type along with the custom fields we setup.</p><pre class=\"language-graphql\"><code>{\n  courses {\n    edges {\n      node {\n        id\n        title\n        content\n        courseFields {\n          isInPreview\n          author\n          releaseDate\n          price\n        }\n      }\n    }\n  }\n}\n</code></pre><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-3/51c4bc83-79d1-4973-828c-88478102621e-image.png\" /><figcaption></figcaption></figure><p>There you go! You now have a system that can be extended to fit pretty much any data structure you need, all using the WordPress publishing tools you might already be familiar with. The next step would be to setup another template just like we did in the previous article to display the data.</p><p>Happy Coding!</p></div>","featuredImage":"/img/n/building-websites-with-gridsome-and-wordpress-part-3/478850f9-fbb1-4972-b7b4-7516d1ad44c3-building-websites-with-gridsome-and-wordpress-part-3-featured-image.jpg","excerpt":"WordPress was originally designed as a platform to write blog articles, but over the years, ambitious developers have...","cachedOn":1681394527},{"id":"f39b11e0-54d4-4c6a-befe-8ec6bc906ec9","notion_id":"f39b11e0-54d4-4c6a-befe-8ec6bc906ec9","status":"Published","slug":"building-websites-with-gridsome-and-wordpress-part-2","relation_series":["8af313c9-2908-4c13-9065-0106fb547cd1"],"publishOn":"2020-10-14T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-10-gridsome-and-wordpress-pt2","seriesOrder":2,"youTubeURL":"https://youtu.be/xjwtnTLXh3s","title":"Building Websites with Gridsome and WordPress (Part 2)","html":"<div><p>In the previous article, we setup WordPress with the WPGraphQL plugin to easily connect Gridsome to it. In this article, I'll cover how to create a Blog Index page, as well as a template to show our post content.</p><h2>Creating a Blog Index</h2><p>The blog index will simply be a list of our posts. Inside the <strong>pages</strong> directory of the project, create a new file called <strong>Blog.vue</strong> and add the following to it. If you've been following along with the series, we're basically repurposing the code we added to Index.vue in the previous article. The only thing we've added is the <code>slug</code> in the query, we're going to use this to link to the post page itself.</p><pre class=\"language-plain text\"><code>&lt;template&gt;\n  &lt;Layout&gt;\n    &lt;h1&gt;Posts&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li v-for=\"{ node } in $static.posts.edges\" :key=\"node.id\"&gt;\n        &lt;router-link :to=\"/blog/${node.slug}\"&gt;{{ node.title }}&lt;/router-link&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/Layout&gt; &lt;/template&gt; &lt;static-query&gt; { posts { edges { node { id title content slug } } } } &lt;/static-query&gt; &lt;script&gt; export default { metaInfo: { title: 'Blog' } } &lt;/script&gt;</code></pre><p>Note how we used the <code><router-link></code> tag. That is an internal Gridsome component that helps with performance by telling Gridsome is it an internal link.</p><p>Now start up your project with <code>npm run develop</code> and enter the URL displayed in your terminal, and add the <code>/blog</code> path to the end. Gridsome will use the file name to determine the best path to use to access the file.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-2/34f245d2-7878-4d67-b141-76777fb43529-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-2/628cd947-b8d8-4927-a16d-0881e263db78-image.png\" /><figcaption></figcaption></figure><p>Now if you try and click on the link, it wont work. We need to setup the template that will be used to generate the pages dynamically. Lets do that now.</p><h2>Creating a Post Template</h2><p>Create a new file in the <strong>templates</strong> directory called <strong>Post.vue</strong>. Add the following content to it.</p><pre class=\"language-javascript\"><code>&lt;template&gt;\n  &lt;Layout&gt;\n    &lt;h1 v-html=\"$context.title\"&gt;&lt;/h1&gt;\n    &lt;div v-html=\"$context.content\"&gt;&lt;/div&gt;\n  &lt;/Layout&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nexport default {\n  metaInfo: function () {\n    return {\n      title: this.$context.title\n    }\n  }\n}\n&lt;/script&gt;\n</code></pre><p><code>$context</code> is a special object used by the build process to inject data into the template. We can setup Gridsome to generate posts for us in <strong>gridsome.server.js</strong>, so lets head there now. You'll notice there are a few template functions setup here, but we aren't going to use them the way that they are. You can safely replace the content of this file with the following.</p><pre class=\"language-javascript\"><code>// gridsome.server.js\nmodule.exports = function (api) {\n  api.createPages(async ({ graphql, createPage }) =&gt; {\n    // Setup our query to get the WordPress data\n    const query = `{\n      posts {\n        edges {\n          node {\n            title\n            slug\n            content\n          }\n        }\n      }\n    }`\n\n    // Execute the query\n    const queryResult = await graphql(query)\n\n    // Pull the data from the query into a variable thats easier to work with\n    let posts = queryResult.data.posts.edges.map(edge =&gt; edge.node)\n\n    console.log('posts', JSON.stringify(posts))\n\n    // For each post, generate a page using the context & template we created\n    posts.forEach(p =&gt; {\n      createPage({\n        path:/blog/${p.slug}, component: './src/templates/Post.vue', context: { title: p.title, content: p.content } }) }) }) }</code></pre><p>I've commented this code to describe what it is doing. In a nutshell, its querying the Gridsome data layer to generate the HTML using the template we just created.</p><p>Now Gridsome will hot-reload many changes made to it, but since generating pages using templates is done at build time, you'll need to restart your process before you can see the changes. Once you've done so, try clicking that link in the blog index.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-2/37f0b01d-9faa-4ed4-b5fe-b3f83c03b01d-image.png\" /><figcaption></figcaption></figure><p>And there is our WordPress data displayed as static HTML using Gridsome! Once last minor change we're going to make is to that menu in the upper right. Gridsome uses Vue components as layouts to keep things consistent, and that menu is setup in <strong>layouts/Default.vue</strong>, so open that file and add another line under the About link like so.</p><pre class=\"language-html\"><code>&lt;template&gt;\n  &lt;div class=\"layout\"&gt;\n    &lt;header class=\"header\"&gt;\n      &lt;strong&gt;\n        &lt;g-link to=\"/\"&gt;{{ $static.metadata.siteName }}&lt;/g-link&gt;\n      &lt;/strong&gt;\n      &lt;nav class=\"nav\"&gt;\n        &lt;g-link class=\"nav__link\" to=\"/\"&gt;Home&lt;/g-link&gt;\n        &lt;g-link class=\"nav__link\" to=\"/about/\"&gt;About&lt;/g-link&gt;\n        &lt;g-link class=\"nav__link\" to=\"/blog/\"&gt;Blog&lt;/g-link&gt; &lt;!-- Here --&gt;\n      &lt;/nav&gt;\n    &lt;/header&gt;\n    &lt;slot/&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n</code></pre><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-2/13b363e7-1af7-4b96-9061-84e35ef75ece-image.png\" /><figcaption></figcaption></figure><p>In my next article, I'll cover how to use WP Custom Post Types and Advanced Custom Fields to create a fully customized schema to use WordPress to store more data than just posts or pages.</p><p>Happy coding! 😎</p></div>","featuredImage":"/img/n/building-websites-with-gridsome-and-wordpress-part-2/482411b4-922b-4b4b-bf31-2dd7abe6aff1-building-websites-with-gridsome-and-wordpress-part-2-featured-image.jpg","excerpt":"In the previous article, we setup WordPress with the WPGraphQL plugin to easily connect Gridsome to it. In this artic...","cachedOn":1681394529},{"id":"da5aecb5-b6e4-4597-b3e0-f054be910c86","notion_id":"da5aecb5-b6e4-4597-b3e0-f054be910c86","status":"Published","slug":"building-websites-with-gridsome-and-wordpress-part-1","relation_series":["8af313c9-2908-4c13-9065-0106fb547cd1"],"publishOn":"2020-10-12T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-10-gridsome-and-wordpress-pt1","seriesOrder":1,"youTubeURL":"https://youtu.be/_Rkbs6Djr4o","title":"Building Websites with Gridsome and WordPress (Part 1)","html":"<div><p>WordPress is one of the most popular website frameworks used today, and it probably isn't going away any time soon. Last I checked, nearly 1/3 websites are built using WordPress! One of the major shortcomings of WordPress though is that without the use of speed optimization plugins, WordPress sites can be slow unless they are on relatively powerful hardware. Even still I've found these plugins don't work well without quite a bit of configuration & tweaking.</p><p>Well one of the best ways to address this (in my opinion) is to use one of the many static site generator (or SSG) frameworks to generate a website using WordPress's API for the data source. This honestly gives you the best of both worlds, the complete flexibility of designing and building your site how you want, all while using WordPress's polished CMS for writing content.</p><h2>Prerequisites</h2><p>You'll need the following before you can follow along;</p><ul><li>A WordPress instance (I'll be using one running on my computer)</li><li>VSCode</li><li>NodeJS & Gridsome CLI</li></ul><h2>Installing the WPGraphQL Plugin</h2><p>We could just use the built in REST API, but the WPGraphQL plugin presents a GraphQL query endpoint that can be easily used with Gridsome to query for data. Unfortunately this plugin is not in the common plugin repository, so you'll need to install it manually. Head over to <a href=\"https://github.com/wp-graphql/wp-graphql/releases\" target=\"_blank\">https://github.com/wp-graphql/wp-graphql/releases</a> and download the latest release. Once its done downloading, head into your WordPress admin and upload the plugin. Once its done, make sure to activate it too.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-1/fd63dae4-35fc-416a-a083-3bc252b9202e-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-1/0907e59f-4e65-4ca9-864f-914a244510f1-image.png\" /><figcaption></figcaption></figure><p>After the plugin has been uploaded & activated, you'll see a new link on the top bar called GraphiQL IDE. You can use this to build and test queries against WordPress</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-1/f28cfc19-3a2c-49ad-ab00-0401595e3704-image.png\" /><figcaption></figcaption></figure><p>If you dont already have permalinks configured, you'll need to do that before proceeding. Selecting anything other than <strong>Plain</strong> should do the trick. I personally like <strong>Day and name</strong>, so thats what I chose here.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-1/a9777b20-9773-48c4-ad07-6019fa595a8e-image.png\" /><figcaption></figcaption></figure><h2>Connecting Gridsome</h2><p>Lets start with a clean site. Create a new Gridsome site with <code>gridsome create <sitename></code>. Then install the <strong>gridsome-source-graphql</strong> plugin with <code>npm install gridsome-source-graphql</code>. Lets then add the following into the <strong>plugins</strong>array in <strong>gridsome.config.js</strong>. Replace the URL with whatever your site is, but its important that the <code>/graphql</code> remains.</p><pre class=\"language-json\"><code>{\n  use: \"gridsome-source-graphql\",\n  options: {\n    url: \"http://localhost:8000/graphql\",\n    fieldName: \"wpGraphQl\",\n    typeName: \"wpGraphQl\"\n  }\n}\n</code></pre><p>Now lets make sure our Gridsome site is talking to WordPress correctly. Start your site with <code>npm run develop</code> and head to the Explore GraphQL data URL shown in the terminal.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-1/bbc4c411-7da8-4bdf-9da4-ce04d10cb20d-image.png\" /><figcaption></figcaption></figure><p>Lets run a query and make sure we get back our WordPress data.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-1/35d8519d-b033-4055-b96c-7c295da8b43e-image.png\" /><figcaption></figcaption></figure><p>Great. Now update our <strong>Index.vue</strong> to display the data. Open the file and add a static query between the <strong>template</strong> and <strong>script</strong> sections. This will fetch our posts and the id, title, and content fields of each one.</p><pre class=\"language-plain text\"><code>&lt;static-query&gt;\n{\n  posts {\n    edges {\n      node {\n        id\n        title\n        content\n      }\n    }\n  }\n}\n&lt;/static-query&gt;\n</code></pre><p>Now in the <strong>template</strong> section, add the following HTML just before the closing <code></Layout></code> tag.</p><pre class=\"language-html\"><code>&lt;p&gt;\n  &lt;h1&gt;Posts&lt;/h1&gt;\n  &lt;ul&gt;\n    &lt;li v-for=\"{ node } in $static.posts.edges\" :key=\"node.id\"&gt;\n      {{ node.title }}\n    &lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/p&gt;\n</code></pre><p>Once you save, Gridsome will automatically rebuild the site. So head to <a href=\"http://localhost:8080/\" target=\"_blank\">http://localhost:8080</a> in your browser to see the site. You should see a list of posts (up to 10 by default) listed.</p><figure><img src=\"/img/n/building-websites-with-gridsome-and-wordpress-part-1/b80b0744-d842-4780-a71f-f87575c81669-image.png\" /><figcaption></figcaption></figure><p>In the next part of this article, I'll outline how to use this data along with page templates to create the actual post pages.</p></div>","featuredImage":"/img/n/building-websites-with-gridsome-and-wordpress-part-1/9f58d48f-5815-4aae-84c4-c79e23f2f948-building-websites-with-gridsome-and-wordpress-part-1-featured-image.jpg","excerpt":"WordPress is one of the most popular website frameworks used today, and it probably isn't going away any time soon. L...","cachedOn":1681394530},{"id":"125cda81-41b8-44bf-becb-8b91623414b3","notion_id":"125cda81-41b8-44bf-becb-8b91623414b3","status":"Published","slug":"running-wordpress-in-docker-for-development","relation_series":[],"publishOn":"2020-10-07T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Running WordPress in Docker for Development","html":"<div><p>Like it or not, WordPress is still an incredible tool used by many business owners and developers throughout the world. Last I checked, nearly 1/3 websites on the internet were running WordPress. If you are ever in a situation where you need to do some WP (shorthand for WordPress) development, setting up a local environment can be a pain if using the traditional LAMP/XAMPP stack. HOWEVER: You can also setup a docker container with one command that will pretty much do all the heavy lifting involved in setting up a local WP environment for development, testing, playing around with, whatever. Lets get into setting this thing up!</p><p>Before we start, you'll need to have Docker installed. You can get it from <a href=\"https://www.docker.com/products/docker-desktop\" target=\"_blank\">https://www.docker.com/products/docker-desktop</a>. Just be warned, you might need to create an account with them before they'll let you download it.</p><h2>Spinning up the Container</h2><p>Docker has two ways to setup containers. The first is manually using the command line to set the containers' configuration, and the second is by using <code>docker-compose</code>, which will use a yaml file that contains all of the configuration needed for one or more containers.</p><p>We'll be using docker-compose with the following yaml file. So create a new folder somewhere on your computer and then create a file in that folder called <code>docker-compose.yaml</code> and paste the following into it;</p><pre class=\"language-yaml\"><code>version: '3.3'\n\nservices:\n   db:\n     image: mysql:5.7\n     volumes:\n       - db_data:/var/lib/mysql\n     restart: always\n     environment:\n       MYSQL_ROOT_PASSWORD: wp_root\n       MYSQL_DATABASE: wordpress\n       MYSQL_USER: wpuser\n       MYSQL_PASSWORD: mysupersecretpassword\n\n   wordpress:\n     depends_on:\n       - db\n     image: wordpress:latest\n     ports:\n       - \"8000:80\"\n     restart: always\n     environment:\n       WORDPRESS_DB_HOST: db:3306\n       WORDPRESS_DB_USER: wpuser\n       WORDPRESS_DB_PASSWORD: mysupersecretpassword\n       WORDPRESS_DB_NAME: wordpress\nvolumes:\n    db_data: {}\n</code></pre><p>Here is a quick breakdown of one the of container definitions;</p><pre class=\"language-yaml\"><code>db: # The name of the container that will be created\n   image: mysql:5.7 # The image that will be used to create the container\n   volumes: # This contains a list of mount points that will be mapped into the container\n     - db_data:/var/lib/mysql\n   restart: always # Make sure the container is always started\n   environment: # Environment variables used by the container\n     MYSQL_ROOT_PASSWORD: somewordpress\n     MYSQL_DATABASE: wordpress\n     MYSQL_USER: wordpress\n     MYSQL_PASSWORD: wordpress\n</code></pre><p>Now open a command line in that folder. In Windows, you can do this by Shift + Right Clicking any of the empty space within the folder and clicking <strong>Open PowerShell window here.</strong> In the PowerShell window, simply type the following to fire up the container.</p><pre class=\"language-bash\"><code>docker-compose up</code></pre><p>You'll first see docker download all the necessary images needed to create the containers, followed by the console output of each running container.</p><figure><img src=\"/img/n/running-wordpress-in-docker-for-development/60a42d30-4234-49cc-bc31-285daf65ca7d-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/running-wordpress-in-docker-for-development/cc44fefb-511e-47ef-a25d-ca6f66e84720-image.png\" /><figcaption></figcaption></figure><p>Now open a browser and head to <strong>http://localhost:8000</strong> and you should be presented with the WordPress setup wizard.</p><figure><img src=\"/img/n/running-wordpress-in-docker-for-development/d0ac8836-37f6-4808-ae51-1ee5132a0fb1-image.png\" /><figcaption></figcaption></figure><p>From this point, its just a process of following along the wizard as it walks you through setting the basic config of WordPress!</p></div>","excerpt":"Like it or not, WordPress is still an incredible tool used by many business owners and developers throughout the worl...","cachedOn":1681394535},{"id":"ec83bd51-8c32-455e-b622-325a1e13dd5a","notion_id":"ec83bd51-8c32-455e-b622-325a1e13dd5a","status":"Published","slug":"getting-started-with-vuejs","relation_series":[],"publishOn":"2020-09-29T00:00:00.000Z","codeURL":null,"seriesOrder":1,"youTubeURL":null,"title":"Getting Started with VueJS","html":"<div><p>Vue is a JavaScript front end framework in line with React and Angular, and it is my preferred front end framework. In this article, ill show you how to install the Vue cli, create your first Vue project, and start up the app so you can browse it on your machine.</p><p>To follow along, you should have the following installed;</p><ul><li>Nodejs</li><li>VSCode</li></ul><p>The Vue CLI was created to make bootstrapping (or setting up) Vue projects dead simple. With a single command, you can create a shell of an application to get in and start building. Install the Vue cli by running the following command in your terminal;</p><p><code>npm install -g @vue/cli</code></p><p>Once installed, create a Vue app with the following command;</p><p><code>vue create my-app</code></p><p>You’ll be prompted by a set of options, pick the default and hit enter.</p><figure><img src=\"/img/n/getting-started-with-vuejs/4f18582c-50b5-4080-8a71-3ed7e96879bf-image.png\" /><figcaption></figcaption></figure><p>Wait a few minutes and your app will be ready to start working on.</p><figure><img src=\"/img/n/getting-started-with-vuejs/3449d0b8-255d-4896-9de2-84c22ba06fbf-image.png\" /><figcaption></figcaption></figure><h2>Running Locally</h2><p>The default app that's created comes with a mini web server that you can use to test changes to your app. You can run the app by issuing the following command in the terminal;</p><p><code>npm run serve</code></p><figure><img src=\"/img/n/getting-started-with-vuejs/2cf29d55-7e92-4984-9884-e01837335f52-image.png\" /><figcaption></figcaption></figure><p>By default, the app will run on port 8080. If something else is running on that port, it will increment up (ie; 8081, 8082, etc) until it finds a port to run on. Make sure to check the terminal to find the port. Assuming we're running on 8080, head over to your browser and enter http://localhost:8080 and you should see the following.</p><figure><img src=\"/img/n/getting-started-with-vuejs/951ea5db-3652-402b-b99f-d34a449f939e-image.png\" /><figcaption></figcaption></figure><p>The web server will also automatically reload your app whenever you make changes. To do this, lets head into <strong>App.vue</strong>and update the title from \"<strong>Welcome to Your Vue.js App</strong>\" to \"<strong>Hello From My Vue App!\"</strong> and save the file. If you have your browser still up, you should notice the page automatically refresh after the changes are made.</p><figure><img src=\"/img/n/getting-started-with-vuejs/3132a7f0-4f05-42b7-add1-a90b3cef0562-image.png\" /><figcaption></figcaption></figure><p>Now you're setup to start building your new Vue app! This is the first of a series of articles I'll be writing on building apps using Vue. Keep an eye out for the next one.</p><p>Happy coding! 😎</p></div>","featuredImage":"/img/n/getting-started-with-vuejs/4ee5cae9-bbea-47f9-8f3c-c578c1ae6f48-getting-started-with-vuejs-featured-image.jpg","excerpt":"Vue is a JavaScript front end framework in line with React and Angular, and it is my preferred front end framework. I...","cachedOn":1681394536},{"id":"5860a2c3-cc96-4a4a-a2a9-012b86f8715c","notion_id":"5860a2c3-cc96-4a4a-a2a9-012b86f8715c","status":"Published","slug":"private-npm-packages-with-github-actions-packages","relation_series":[],"publishOn":"2020-09-17T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":"https://youtu.be/7CNC0QBCY-Y","title":"Private NPM Packages with GitHub Actions & Packages","html":"<div><p>Say there are various snippets of code that you’ve built up over some time that could be useful in multiple projects. You COULD copy and paste this code between the projects, but if you find a bug, that means you have to go into each and fix the bug. A better approach would be to create a package that can be shared across multiple projects. That’s exactly what you’ll learn how to do in this article.</p><p>To demonstrate this, I’m going to create a simple npm package that exports a command that combines a name with <code>Hello</code>, so the result of passing in my name would output <code>Hello Brian!</code></p><p>Here’s the code.</p><pre class=\"language-javascript\"><code>const sayHello = function (name) {\n  console.log(\"Hello \" + name + \"!\")\n}\n\nmodule.exports = {\n  sayHello\n}\n</code></pre><p>You'll also need to add the following to <strong>package.json</strong>. Notice what's called the <strong>scope</strong> at the end of the URL. For your project, replace this with your GitHub username.</p><pre class=\"language-javascript\"><code>\"publishConfig\": {\n  \"registry\":\"https://npm.pkg.github.com/@YOUR_USERNAME\"\n},</code></pre><p>Create an empty repository to hold your package. When you do, make sure you tick the Private radio.</p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/1fbd89ad-9382-43b6-8c3f-7961f0a940f6-image.png\" /><figcaption></figcaption></figure><p>After you've pushed the code to your repo, go into Actions and click \"set up a workflow yourself →\"</p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/7b38e579-6f9f-4b8e-825a-6f1f96bc41db-image.png\" /><figcaption></figcaption></figure><pre class=\"language-yaml\"><code>name: Node.js Package\n\non:\n  push:\n    branches:\n      - master\n\njobs:\n  publish-gpr:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-node@v1\n        with:\n          node-version: 12\n          registry-url: https://npm.pkg.github.com/\n          scope: '@YOUR_USERNAME'\n      - run: npm install\n      - run: npm publish\n        env:\n          NODE_AUTH_TOKEN: ${{secrets.GITHUB_TOKEN}}\n</code></pre><p>Once done, click <strong>Start commit,</strong> then <strong>Commit new file.</strong></p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/e628dcfd-314c-4702-8b0d-e4670bee8f55-image.png\" /><figcaption></figcaption></figure><p>Now head back into Actions and you should see your project building. Once its done and you receive the following, you should be ready to use your package in another project.</p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/e94359f1-a722-4b29-b45c-b55273fe8007-image.png\" /><figcaption></figcaption></figure><p>You can also see the package details by going back to the repository home and clicking on the package on the right sidebar.</p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/c3734177-5521-45ad-b01e-11cbf56cfc3f-image.png\" /><figcaption></figcaption></figure><h2>Adding to a Project</h2><p>Adding to a project is pretty simple. The only thing you'll need to is add a file to the root of your project called <strong>.npmrc</strong>with some info telling your local <strong>npm</strong> where to look for packages for your scope. Before you do, you'll need to generate a personal access token (or PAT) from GitHub that is used to authenticate with private package feeds.</p><p>To get your PAT, click on your profile image on the upper right of GitHub and select <strong>Settings</strong>. Then go to <strong>Developer Settings</strong>, <strong>Personal Access Tokens</strong>, and <strong>Generate New</strong>.</p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/7054ca5b-84ea-4496-bb4f-daa84813ee86-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/c53619d2-9b95-4e34-b3f2-915790defa33-image.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/1f23a0f4-31e9-4081-b63b-05391d80dd14-image.png\" /><figcaption></figcaption></figure><p>In the next window, give your token a name and select the <strong>read:pacakges</strong> scope. Make sure not to check any other boxes as this might give the token too much access to your account. Scroll to the bottom and click <strong>Generate Token</strong>.</p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/5afbaa3d-946e-416e-92c0-8a7f1e9e8b8e-image.png\" /><figcaption></figcaption></figure><p>Once you create the token, you'll be put back into the previous view with the PAT listed. You MUST copy it now as you can't get it again.</p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/4de77ab9-f915-4187-ba41-ae9ac7022bbe-image.png\" /><figcaption></figcaption></figure><p>Tokens should be treated as passwords. Never share them with anyone you don't trust. Furthermore, in the following section, I'll be adding the token directly into the repo. This is generally bad practice and a build variable should be used instead (more <a href=\"https://docs.github.com/en/actions/reference/environment-variables\" target=\"_blank\">here</a>). I'm doing this as a demonstration.</p><p>Now lets get that .npmrc file setup. Create the file in the project you want to use your new package with and add the following content to it. Replace the scope with your username, and the token with your token.</p><pre class=\"language-plain text\"><code>@YOUR_USERNAME:registry=https://npm.pkg.github.com/\n//npm.pkg.github.com/:_authToken=YOUR_TOKEN\n</code></pre><p>Now you can install the package just like you would any other NPM package. Replace the scope & project name with whatever yours is.</p><pre class=\"language-bash\"><code>npm install @learning-brianmmdev/github-actions-packages</code></pre><p>In the project, I created a small file just to test the exported function from the package like so.</p><pre class=\"language-javascript\"><code>let demoPackage = require('@learning-brianmmdev/github-actions-packages')\n\ndemoPackage.sayHello('Brian')\n</code></pre><p>And here is the output of testing this.</p><figure><img src=\"/img/n/private-npm-packages-with-github-actions-packages/01077d01-1f13-46b6-a03c-8b640d579722-image.png\" /><figcaption></figcaption></figure><p>Congratulations! You now have your very own private package that can be reused across projects you build.</p></div>","excerpt":"Say there are various snippets of code that you’ve built up over some time that could be useful in multiple projects....","cachedOn":1681394538},{"id":"adb5aa60-f350-4e30-b7c3-adec1ea39d75","notion_id":"adb5aa60-f350-4e30-b7c3-adec1ea39d75","status":"Published","slug":"lessons-learned-with-netlify-forms","relation_series":[],"publishOn":"2020-09-09T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Lessons Learned With Netlify Forms","html":"<div><p>I decided to throw this together since I've been working on testing & debugging Netlify forms over the weekend. Some of the issues I ran into we're somewhat obvious in hindsight, but here is quick list of what I learned while building a reusable Vue component (to be open sourced in the near future). I'm not going to make this a long, drawn out article, just a few bullet points.</p><ul><li>If you have a <code><label></code> for your input, it will use that when displaying the fields in Netlify, like so;</li></ul><figure><img src=\"/img/n/lessons-learned-with-netlify-forms/698f5bae-dd74-4e5b-aec2-0135d642d38c-image.png\" /><figcaption></figcaption></figure><p>And here's the code.</p><pre class=\"language-html\"><code>&lt;div class=\"form-group\"&gt;\n  &lt;label for=\"name\"&gt;Your Name&lt;span class=\"required\"&gt;*&lt;/span&gt;&lt;/label&gt;\n  &lt;input type=\"text\" class=\"form-control\" name=\"name\" id=\"name\" v-model=\"formData.name\" placeholder=\"Jane Smith\" required&gt;\n&lt;/div&gt;</code></pre><ul><li>It's important that the <code>name</code> in the input attribute matches the key of the actual data being submitted.</li><li>Fun fact, you can actually test form submissions to Netlify with Postman like so;</li></ul><figure><img src=\"/img/n/lessons-learned-with-netlify-forms/4327387c-73d6-4edc-9c18-37ba87e95769-image.png\" /><figcaption></figcaption></figure><ul><li>When testing with Postman, the <code>form-name</code> form key must be present and match the name of the form. That seems to be what tells Netlify how to match up the incoming data. If its missing, you'll get a 404.</li><li>Since you can simply POST the data to the URL, this means that you can actually test submissions locally with JavaScript by swapping out '/' with whatever the domain or subdomain of your Netlify app is like so;</li></ul><figure><img src=\"/img/n/lessons-learned-with-netlify-forms/9c33e0aa-13a8-428e-805c-840823c5a6fb-image.png\" /><figcaption></figcaption></figure><p>Hope this helps someone. Happy coding!</p><p></p></div>","excerpt":"I decided to throw this together since I've been working on testing & debugging Netlify forms over the weekend. Some ...","cachedOn":1681394539},{"id":"537d424d-863b-4134-a357-77c50cda9bbf","notion_id":"537d424d-863b-4134-a357-77c50cda9bbf","status":"Published","slug":"my-gtd-workflow","relation_series":[],"publishOn":"2020-09-07T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"My GTD workflow","html":"<div><p>Many years ago, I started adopting GTD, a framework that makes managing tasks and project simple and straight forward as long as you stick to the process. Ive been complimented by many of my supervisors since about how much work I can get done in such a short amount of time, so the system works if done correctly.</p><p>If you haven't heard of GTD, or need a refresher, here are the basic concepts. There are 5 distinct phases of the workflow;</p><ul><li>Collecting – The process of gathering all your tasks/projects together.</li><li>Processing – Defining what ‘done’ looks like and expanding your projects/tasks based on that.</li><li>Organizing – Sorting your tasks into lists & contexts (physical locations where you need to do the task)</li><li>Reflecting – Reviewing your system & tasks on a regular basis and redefining things as needed</li><li>Doing - …doing the stuff.</li></ul><p>I recently reorganized my system and felt that documenting it for the web might help a few other folks out there. My system might seem complicated, but I used a series of automations & integrations with Microsoft Flow to streamline everything. So strap in, grab a cup of coffee, and learn how to be productive!</p><figure><img src=\"/img/n/my-gtd-workflow/b8bc2738-db6c-4e6c-b2a3-f94895ee7509-Untitled.png\" /><figcaption></figcaption></figure><p></p><h2>Collecting</h2><p>Everything I collect eventually ends up in a <strong>Trello</strong> list on my main board I call Inbox.</p><p>To collect everything, I use three different utilities directly: Microsoft Todo, Pocket, and a tool I wrote called Trello Quick Capture.</p><p>I use <strong>Microsoft Todo</strong> primarily on my iPhone ONLY because it is the easiest way to capture tasks in the least amount of taps. There is a long press feature on the icon that pops up a context menu to add a task, and hitting return on the keyboard adds it. This lets me add a task, return, add a task, return, etc.</p><p>I then have a <strong>Microsoft Flow</strong> flow (?) that takes any task added to Todo, creates a card in a special Inbox list on my main Trello board, and completes the task. (I do all my organizing in Trello).</p><p>I use <strong>Pocket</strong> to capture stuff around the web. On the iPhone, I can share pretty much anything to pocket and it doesn’t ask me any other questions, it just saves it to Pocket. And like above, I have a Flow setup to copy all my Pocket items into the Trello Inbox list.</p><p>The Pocket Chrome extension is stupid simple as well, so I use that to capture pages around the internet I want to go back to review.</p><p><strong>Trello Quick Capture</strong> is really small app written in JavaScript (Electron + Vue) that is bound to a keyboard shortcut (Ctrl + O). It pops up a small dialog window over everything open on my computer that lets me type in a quick task and hit enter to dismiss the window. This creates a card in the same Trello list for processing.</p><p>Finally, if I need to capture an email into a task, there is an Office 365 extension that lets me send emails into Trello.</p><p>The common trend in my collection process is I want to be able to collect anything and everything in the least amount of clicks/taps possible, which just lets me keep moving quickly.</p><h2>Processing & Organizing</h2><p>Organizing is a pretty manual process with very little automation to be honest. GTD has two primary types of items;</p><ul><li>Next Actions – Which are simple, small tasks that you can accomplish without having to necessarily ‘think’ about how to do the action.</li><li>Projects – Multi-step tasks, regardless if there are two steps or two hundred.</li></ul><p>I used to have two boards to track Next Actions & Projects, with Next Actions being the primary board I would see every day, and Projects being a board that I’d come back to whenever I do a review of my entire system.</p><p>In my new setup, I actually moved all my projects into my main board, and moved my next actions to <strong>Todoist</strong>.</p><p>I made this decision so that whenever I organize and process my inbox, I can quickly scan my projects since I'm in the same board, Next Actions lends itself to a traditional list of tasks rather than kanban boards.</p><p>I opted not to use MS Todo because in my experience, its sync engine is not that great (I once ended up with 5 duplicates of some 200+ tasks/projects, which I was not happy about). And although Asana can display tasks as both a list or kanban board, I’m not a huge fan of the interface. I feel like its not the most intuitive and a lot of things require more clicks than I care to give it.</p><p>Remember, this system is all about speed and getting through it with as little effort as possible. No friction!</p><h2>Reviewing & Doing</h2><p>These are pretty self explanatory and I don’t have any fancy tools I use for this. Basically, every few days, I just scan all my projects and compare them against my next actions to determine what’s done or missing, then update accordingly.</p><h2>Archive / Reference</h2><p>Archiving isn’t necessarily a ‘phase’ in GTD, but it’s the result of simply deciding what to keep for later reference. My tool of choice has traditionally been <strong>OneNote</strong> since its pretty much everywhere and the tool does a good job at organizing. I recently started exploring <strong>Notion</strong> because OneNote doesn’t do syntax highlighting, and saving code snippets does look much better with a splash of color.</p><h2>Conclusion</h2><p>This system is the result of many years of trial and error, and it probably wont be my last revision of the entire thing, but what Ive setup does work well and thought some of you out there would benefit from setting up something similar.</p><p>Thank you for joining my TED Talk.</p><p>If you are interested in chatting more about my workflow or just want to nerd out about GTD, feel free to reach out to me on Twitter <a href=\"https://twitter.com/brianmmdev\" target=\"_blank\">@brianmmdev</a> or join my discord at <a href=\"https://fullstack.chat/\" target=\"_blank\">fullstack.chat</a></p></div>","featuredImage":"/img/n/my-gtd-workflow/d35f73d6-3532-4ba8-8923-51ff8adc58ac-my-gtd-workflow-featured-image.png","excerpt":"Many years ago, I started adopting GTD, a framework that makes managing tasks and project simple and straight forward...","cachedOn":1681394541},{"id":"d7a0fe05-7e00-4b72-9efa-1a0c8da741b0","notion_id":"d7a0fe05-7e00-4b72-9efa-1a0c8da741b0","status":"Published","slug":"introducing-bmops-my-custom-cms","relation_series":[],"publishOn":"2020-08-27T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Introducing BMOPS, My Custom CMS","html":"<div><p>So this has been a long time coming. I've had it on the list for years to see what it would take to create a completely custom CMS, and I finally did. I'm calling it BMOPS (Brian Morrison's Operations). Yeah its a silly name, but it rolls off the tongue real nice!</p><p>Quite honestly, this is mainly just a post to test things...</p><h2>How It Started</h2><p>So my website used WordPress as a backend because I really like the content editor. I didn't like the fact that I had to run my own server (or a VPS) to host it, where I'm always pitching serverless solutions as the answer to all the world's problems. I really would have liked to run just the editor component in a serverless environment, but it really looked like it was more work than its worth. So what else should a developer do than come up with their own solution!</p><p>I started playing around with TinyMCE. Turns out it integrates with Vue (my preferred JS framework) quite well. But then I stumbled upon a demo of someone parsing markdown in real time in a browser using Vue, and it got me thinking that this would be the approach to go.</p><h2>And Here We Are</h2><p>So that's exactly what BMOPS is for now. A Vue web app that uses the <code>markdown-it</code> library to render the markdown I type and saves all the data into DynamoDB using Lambda functions. One of the bigger customizations I created was a system where pasting an image into the editor uploads the image to S3 and returns an md-formatted image string into the editor. Like so;</p><figure><img src=\"/img/n/introducing-bmops-my-custom-cms/352747c6-32ad-43b7-a575-b2a0a6eaae87-Untitled.png\" /><figcaption></figcaption></figure><p>I've still got alot of work to do on it but when it's done, it will be the core dashboard where I run all my content creation through, including free and premium content.</p><p>I'll be sharing more over the next few weeks. Be sure to keep an eye on my Twitter account <a href=\"https://twitter.com/brianmmdev\" target=\"_blank\">@brianmmdev</a> for all the updates! Happy coding 🤓</p></div>","excerpt":"So this has been a long time coming. I've had it on the list for years to see what it would take to create a complete...","cachedOn":1681394545},{"id":"44e69f52-d456-4c60-bd07-a547068d4325","notion_id":"44e69f52-d456-4c60-bd07-a547068d4325","status":"Published","slug":"tips-for-job-hunting","relation_series":[],"publishOn":"2020-08-12T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Tips for Job Hunting","html":"<div><p>I was recently involved in a conversation on the <a href=\"https://discord.gg/vM2bagU\" target=\"_blank\">Learn Build Teach Discord</a> run by James Q Quick about how to getting a job and it got me thinking. I’ve done a lot of research on this topic over the years and figured it’d be nice to dump my strategy in an article. Obviously nothing can guarantee you’ll get the job, but hopefully some of these tips will position you better than your competition.</p><h2>Its Your Responsibility</h2><p>I feel like this should go without saying, but ultimately, your career is your responsibility. It doesn’t matter if some recruiter or your college or bootcamp said they will be getting you the job, you shouldn’t wait on them to try and land your shiny new dev job.</p><p>In the end, all of these parties are out for themselves. Recruiters specifically get a nice fat commission for every job they land. If they don’t feel you’re marketable, they aren’t going to waste their time with you. That’s the honest truth. But notice the verbiage I used; “If they don’t think you’re marketable…”. A lot of job hunting is about perception. That’s why undeserving candidates get good jobs while devs that bust their butts get left behind. This leads to my next point:</p><h2>You’re Selling Yourself</h2><p>Hunting for a job is a sales job all in itself. You have to give the perception that you are the ideal candidate for the position you’re after, even if you aren’t. You have to sell yourself in every step of the process;</p><ul><li>Your Resume</li><li>Your Portfolio</li><li>The way you speak to the recruiter/employer</li><li>Your presentation and demeanor during the interview</li><li>How you handle yourself after the interview</li></ul><p>Its all about making yourself attractive to the person (not even necessarily the company) looking to hire you.</p><h2>Put Yourself in Their Shoes</h2><p>Hiring companies are looking to fill a spot with a specific kind of person, and if you really want that gig, you have to BE that person. And they are looking for a person that solves a problem within their company, so everything you do has to revolve around the benefit of the employer.</p><p>I’m not a hiring manager, but I’ve often been involved in the process before and a lot of resumes I’ve reviewed usually showcase their projects like this;</p><ul><li>“I built an application that does the thing using technologies A, B, and C.”</li></ul><p>A list of cool, new, hip tech you worked with might sound cool, but it doesn’t convey <em>value</em> to that cool employer. Instead, I recommend rewriting your resume to follow something like this;</p><ul><li>“I built an application that does X, using technologies Y, and it solved problem Z in the company.</li></ul><p><strong>Z</strong> is the important piece here. Employers hire people to solve problems, and Z conveys the value that you can provide to the employer. The three most common ways a company gets value are;</p><ul><li>Save Time</li><li>Save Money</li><li>Make Money</li></ul><p>Putting your projects & experience in these terms will do wonders for your chances of landing that job.</p><h2>Testimonials</h2><p>If you have past experience, ask former employers to write a quick one or two sentences about you to put on your resume or website. This shows well in multiple ways. First and most obviously, it shows that people you’ve worked under in the past valued you at their company. Secondly, it shows that you took the initiative to even ask for testimonials, which many candidates wont do.</p><h2>Follow Up</h2><p>So you’ve had the interview already and you’re playing the waiting game to hear back from the people you spoke with. Within 24 hours, I always send a thank you note to everyone I met with. Busy people rightfully value their time and they’ve carved some of it out to spend it with you, so telling them you appreciate that is a great gesture that will put you above others.</p><p>This also echos from the section above about it being your responsibility, but if you are waiting for a recruiter to get back to you, get back to them first. Find out if it went well (or not) and figure out whats next. If the recruiter wont give you the employers info, see if you can get the contact info of the people you interviewed with and ask them for feedback directly. Just be careful with that last one, you might burn a bridge with the recruiter, so that’s a decision you’d have to make yourself.</p><h2>Get Feedback</h2><p>If you went through the interview process and know for a fact that you didn’t make the cut, ask for constructive feedback from the folks you interviewed with. You can use the following questions to guide this discussion;</p><ul><li>What did you like about me as a candidate? What didn’t you like?</li><li>Do you have any suggestions for how I can improve?</li><li>After taking these suggestions and improving, would I be eligible to re-interview? And if so, when would that be?</li></ul><p>If you can get to this point, you might even change the mind of the person you’re speaking with because it shows that you can take criticism and work on yourself to better yourself.</p><h2>Network</h2><p>Many times when landing a job, it isn’t what you know but who you know. Story time, my first gig in the tech industry was as a support rep for a big box retailer repairing computers. I was pretty fresh out of high school and had applied for the position several times without ever hearing back. So a few years down the road, I’m in a college calculus class and the class was assigned a team project. Turns out one of my team members was a manager at that store. I told him I had applied there a number of times. He told me to re-apply. Two days later, I got that job.</p><p>Now of course it is total luck (or divine intervention, depending on your beliefs) that I was paired with this individual, but it taught me the valuable lesson of networking with others.</p><h2>Be Extroverted</h2><p>This one is <em>HARD</em> for introverts, of which many tech professionals naturally are. I actually used to be pretty heavily introverted myself (ask my wife, I used to have social anxiety when we’d go to parties and would want to leave as soon as we got there), but I decided that my drive to be successful in the industry was more important about my being nervous talking with others.</p><p>Being extroverted is like a muscle, the more you use it the better it gets. Or to get more techie, the more you practice a specific language or framework, the more natural it becomes to write. After a few years of pushing myself to interact more with others, I can interview well with pretty much anyone because I’ve been practicing for so long. Here are a few tips;</p><ul><li>Join active Discords, specifically ones with voice chats</li><li>Attend meetups (even virtual ones) and make sure to at least introduce yourself to those around you</li><li>Livestream on Twitch or YouTube and interact with the chat</li></ul><p>Now my anxiety wasn’t so intense that I wasn’t able to work through it, so I cant give advice to those who are in that position, but perhaps a quick discussion with your doctor might put you in the right direction.</p><h2>Conclusion</h2><p>I know this was a lot to digest (more than I actually intended on writing) but its all stuff that takes time and practice, like anything else. Hopefully you got something out of this.</p><p>If you are interested in contacting me, I run the <a href=\"https://fullstack.chat/\" target=\"_blank\">fullstack.chat</a> Discord group where we discuss various dev-related (and sometimes not) stuff. Its a friendly environment where all devs are welcome, regardless of background or time in the industry.</p><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\">Be sure to follow me on Twitter at <a href=\"https://twitter.com/brianmmdev\" target=\"_blank\">@brianmmdev</a> and subscribe to my <a href=\"https://www.youtube.com/channel/UCLx9EihBDfoJMncRWSZZoXg/\" target=\"_blank\">YouTube channel</a>.</div></div><p>As always, happy coding! 😎</p><p></p></div>","featuredImage":"/img/n/tips-for-job-hunting/8400852a-e52f-49f6-ae43-2a9a57ba278c-tips-for-job-hunting-featured-image.jpg","excerpt":"I was recently involved in a conversation on the  Learn Build Teach Discord  run by James Q Quick about how to gettin...","cachedOn":1681394552},{"id":"3322ad35-85b3-4430-82bf-e9dbd973ca42","notion_id":"3322ad35-85b3-4430-82bf-e9dbd973ca42","status":"Published","slug":"deploying-a-nodejs-api-to-aws","relation_series":["b699b272-b3fa-4f40-b921-33a6d9e73f22"],"publishOn":"2020-08-03T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-7-deploying-a-nodejs-api-to-aws","seriesOrder":6,"youTubeURL":"https://youtu.be/Xjf2r6TX034","title":"Deploying a NodeJS API to AWS","html":"<div><p>Over the course of the series, we’ve focused on creating a well designed and robust API with features like reading & writing to a database, validating data input, and authentication. But our API does no good running on our computer. We’re going to change that today by deploying our code into AWS using Elastic Beanstalk.</p><p>Elastic Beanstalk (EB for short) provides a streamlined way for you to simply upload a copy of your code and let AWS handle the heavy lifting when it comes to provisioning resources and keeping your code running.</p><h2>Preparing our Code</h2><p>Before we can deploy to EB, we need to add another NPM script. EB will call `npm start` to start the API. Update your <strong>packages.json</strong>‘s script section to look like this.</p><pre class=\"language-plain text\"><code>\"scripts\": {\n    \"start\": \"node index.js\",\n    \"dev\": \"nodemon index.js\"\n  },\n</code></pre><p>Next, use an archive tool to zip up your folder for EB. Make sure that all your files are at the root of the archive. A common mistake is to zip the folder, which will place a nested folder in your archive. EB won’t know what to do with that.</p><h2>Setup Elastic Beanstalk</h2><p>From the AWS Console, click on Services, then Elastic Beanstalk under Compute. Once there, click on Create Application.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/caed8e62-a593-49df-9505-ebdd0101ad16-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/f693618c-2202-495c-b405-101a23af87fe-Untitled.png\" /><figcaption></figcaption></figure><p>I’ll be naming the application ‘<em>nodejs-api</em>’ and giving it the ‘<em>application</em>’ tag of ‘<strong>nodejs-api</strong>’ just to keep things organized.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/cd16bfe4-6581-4094-98d6-96e0687c4243-Untitled.png\" /><figcaption></figcaption></figure><p>Under <strong>Platform</strong>. Select <strong>Node.js</strong>, you can <strong>Platform branch</strong> and <strong>version</strong> as their defaults. Under <strong>Application code</strong>, select <strong>Upload your code</strong>. In the Source code origin section, you’ll need to upload the zipped copy of your code. The version label can be left as is for now.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/6176fb44-66c4-411a-bb08-06e7d9901232-Untitled.png\" /><figcaption></figcaption></figure><p>Once done, click <strong>Create application</strong>. You’ll then be taken to a screen where you can monitor the deployment. After that, if all succeeded, you’ll be presented with a screen like this to show the status of your application.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/6869cb6c-6603-4b05-8afd-aa0260b59b4b-Untitled.png\" /><figcaption></figcaption></figure><p>We need to tweak one setting here. By default, EB spins up a VM (called an EC2 instance in AWS) with an Nginx reverse proxy forwarding traffic from port 80 to 8080. Since the API we built is running on port 3000, we need to tell EB to look at that port instead of 8080. Go into <strong>Configuration</strong>, then <strong>Edit</strong> under <strong>Software</strong>. From there, scroll to the bottom and add an <strong>Environment Property</strong> of <strong>PORT</strong> with a value of <strong>3000</strong>. Click <strong>Apply</strong> and EB will reload your instance with the new settings.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/99f1c046-4474-4867-8848-6a5f606ee5d8-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/6e9a7843-665a-4931-ae1b-46e07c55a4e7-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/1fce6a62-81f2-4ee9-a564-4029f3d951f6-Untitled.png\" /><figcaption></figcaption></figure><p>Once finished, click <strong>Go to environment</strong> on the left. You should see your <em>Hello World!</em> message. Grab the URL from the address bar as that’s the URL for your API in AWS.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/ac08e0e5-eb5a-4d7e-9c23-94873586f31b-Untitled.png\" /><figcaption></figcaption></figure><h2>Testing with Postman Environments</h2><p>Remember back in the first article where I said I’d be exploring environments in Postman? Well here it is! Environments let you quickly swap settings in Postman without changing every little setting in a request. Any environment variable can be referenced throughout the app by using a double curly bracket notation (Ie; <code>{{hostname}}</code>).</p><p>In Postman, click the gear in the Environments Area and then Add to add an environment. First create an environment called Local, add a <code>hostname</code> variable and set the value to `http://localhost:3000`</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/347c86a8-d970-4225-9c2f-07e4aabaea1b-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/c6d6c0e2-0e19-4caf-a9a9-15b3c263f62a-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/1840ce15-6dd1-417c-ab19-df9abfac5774-Untitled.png\" /><figcaption></figcaption></figure><p>Next, add another environment called AWS and set the hostname variable with the value you pulled from the previous section. Make sure there is no trailing slash otherwise you might run into issues when testing.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/1e5d72e3-62f7-4df0-9391-a93249820b86-Untitled.png\" /><figcaption></figcaption></figure><p>Now in your GET /books request, replace <code>http://localhost:3000</code> with <code>{{hostname}}</code>. Drop down where it says <strong>No environment</strong> and select <strong>Local</strong> first. You’ll notice now when you mouse over the <code>{{hostname}}</code> in the address bar that it will show you what its set to.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/df70301d-e70e-4ef0-8243-ae2086bb2465-Untitled.png\" /><figcaption></figcaption></figure><p>Now select AWS and the variable will change.</p><figure><img src=\"/img/n/deploying-a-nodejs-api-to-aws/37ac250e-9d84-4757-8a35-6702650c69c8-Untitled.png\" /><figcaption></figcaption></figure><p>A word of caution: while we did deploy the API to AWS, we didn’t add any any security to the request, so technically if someone was capturing network traffic, they could get the token from the request. I’d encourage you to explore adding HTTPs to the API using an Elastic Load Balancer or API Gateway, but that’s beyond the scope of this article.</p><p>This article concludes the Coding NodeJS API series. If you have any questions, please feel free to hop into my Discord at <a href=\"https://fullstack.chat/\" target=\"_blank\">https://fullstack.chat</a> to discuss. Happy coding! 😁</p></div>","featuredImage":"/img/n/deploying-a-nodejs-api-to-aws/f56fda03-12d6-4901-a697-b11f7bb85bf6-deploying-a-nodejs-api-to-aws-featured-image.png","excerpt":"Over the course of the series, we’ve focused on creating a well designed and robust API with features like reading & ...","cachedOn":1681394553},{"id":"b8c32a73-420d-4a11-b7d9-8fd192503680","notion_id":"b8c32a73-420d-4a11-b7d9-8fd192503680","status":"Published","slug":"adding-authentication-to-a-nodejs-api-using-aws-cognito","relation_series":["b699b272-b3fa-4f40-b921-33a6d9e73f22"],"publishOn":"2020-07-27T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-7-adding-authentication-to-a-nodejs-api-using-aws-cognito","seriesOrder":5,"youTubeURL":"https://youtu.be/SFnOtFk2ou0","title":"Adding Authentication to a NodeJS API using AWS Cognito","html":"<div><p>The API we’ve built so far in this series is only a single route, but that route is pretty robust for what it is. But do you really want everyone in the world to be able to add books to the database? Probably not. This is where authentication comes into play. By adding authentication, we can restrict usage of our API to only the people we want. Today we’ll be adding <strong>AWS Cognito</strong> into our API to prevent unauthorized access of our POST /books handler.</p><h2>Setting Up Cognito</h2><p>In the AWS Console, open up <strong>Service</strong>, then <strong>Cognito</strong> under <strong>Security, Identity, & Compliance</strong>. We need to create a User Pool to let our users sign in. Click on <strong>Manage User Pools</strong>, then <strong>Create a user pool</strong> in the upper right.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/57828604-e32f-4a9b-b4dd-f69eb5f631da-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/22de7890-7c00-402e-a076-37f8c1749d5a-Untitled.png\" /><figcaption></figcaption></figure><p>Give your User Pool a name. I named mine ‘node-api-userpool’. We can leave the other options set as defaults, so click Review defaults, then Create pool.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/120adf4f-0fac-4de5-b7ab-6672dcd89bd2-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/1ecac25f-8dde-4a11-b11c-3576e4d2b64d-Untitled.png\" /><figcaption></figcaption></figure><p>Before moving forward, lets grab the Pool Id and add it to our environment variables. We’re going to need this when setting up the API.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/b43cd6e7-0117-406d-b033-e60414ffde30-Untitled.png\" /><figcaption></figcaption></figure><p>And our current <strong>.env</strong> should look like this.</p><pre class=\"language-plain text\"><code>AWS_ACCESS_KEY_ID = \"AKIA5MOQQQ6WKFDB5WB3\"\nAWS_SECRET_ACCESS_KEY = \"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\"\nAWS_DEFAULT_REGION = \"us-east-1\"\nCOGNITO_USER_POOL_ID = \"us-east-1_S52SSo1f2\"\n</code></pre><p>From here we need to create an App client to identify our API when making auth requests to Cognito. Click on <strong>App clients</strong> in the left side and create a new App client. I named mine ‘<em>nodejs-api</em>‘, you can accept the rest of the defaults and click <strong>Create app client</strong>. On the next view, note the <strong>App client id</strong>. We’re going to need this when creating our sign in URL.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/ca995a4b-3a0a-43ec-bb4a-46875320cd66-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/3193f448-2aca-4e7c-a0fc-8c9141bc0106-Untitled.png\" /><figcaption></figcaption></figure><p>Lets make sure to add these fields to our <strong>.env</strong> file before moving forward.</p><p>Now we need to setup Cognito to provide us the right kind of token when a user signs in. Head to App client settings and do the following;</p><ol><li>Check <strong>Cognito User Pool</strong></li><li>Enter ‘<em>http://localhost</em>‘ in <strong>Callback URLs</strong></li><li>Under <strong>Allowed OAuth Flows</strong>, check <strong>Implicit Grant</strong></li><li>Under <strong>Allowed OAuth Scopes</strong>, check <strong>email</strong> & <strong>openid</strong></li><li><strong>Save</strong> changes</li></ol><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/42a6b649-e05a-4994-9547-b3a67b7d641a-Untitled.png\" /><figcaption></figcaption></figure><p>The next thing we need to do is specify a domain we can use to sign in. Cognito has a set of built in UI forms that will be used by users. Go to <strong>Domain name</strong> and specify a <strong>Domain prefix</strong>. I named mine ‘<em>nodejs-api</em>‘. Take note of what you specify as we’ll need to use this in the next section.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/86353211-30a7-44c3-a763-691f70867fd9-Untitled.png\" /><figcaption></figcaption></figure><p>Finally, lets create a user. Go to <strong>Users and groups</strong> and click <strong>Create user</strong>. Fill out the fields like I’ve demonstrated below.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/b7f5e1b7-d5ed-49d5-aaed-b095d4d9ca91-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/d3f7ada4-a850-4e7e-adae-f02f7bb6d039-Untitled.png\" /><figcaption></figcaption></figure><p>Once your user is created, lets test signing in.</p><h2>Signing In</h2><p>You’ll need to create a URL to sign in based on the values you put into Cognito. If you followed along, it should be something like this.</p><pre class=\"language-plain text\"><code>https://{YOUR_AUTH_DOMAIN}.auth.us-east-1.amazoncognito.com/oauth2/authorize?response_type=token&client_id={YOUR_CLIENT_ID}&redirect_uri=http://localhost\n</code></pre><p>Put that into the browser to be presented with this page. You’ll then enter the credentials for the user you just created.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/57f2dd67-0412-4978-8997-678f97356405-Untitled.png\" /><figcaption></figcaption></figure><p>Sign in and update the password if asked. You might think you did something wrong since you got the <strong>This site can’t be reached</strong> page, but this is because we specified <em>http://localhost</em> as the callback URL. You likely don’t have anything running on there, but don’t worry, the important part is in the address actually. Copy that URL down and break it apart based on the first hash mark and the following ampersands, you should see this.</p><pre class=\"language-plain text\"><code>    http://localhost/\n\n    #id_token=eyJraWQiOiJQQ3NoWGFOOERsbmhTd3lsWmxBXC9kV1VnTENMa1pEaVwvNEpJRVFCcXdoaGs9IiwiYWxnIjoiUlMyNTYifQ.eyJhdF9oYXNoIjoidXNBTE9NMmphNnJNVUhMVW8zUU04QSIsInN1YiI6Ijg4ZTY4MjdlLTFjZjYtNDEyNi1hNmU2LTZiZjA0OGZjMjgxOSIsImF1ZCI6Ijc4c2Y3ZHFrYWpnOTg5MGR1M2hpMnZvcTkwIiwiZXZlbnRfaWQiOiI3NjQ3NGMwZi05YmI0LTQyMDQtYmY1Yy1jYTQ1Mzg2N2EwYTEiLCJ0b2tlbl91c2UiOiJpZCIsImF1dGhfdGltZSI6MTU5MjM1ODg3NCwiaXNzIjoiaHR0cHM6XC9cL2NvZ25pdG8taWRwLnVzLWVhc3QtMS5hbWF6b25hd3MuY29tXC91cy1lYXN0LTFfUzUyU1NvMWYyIiwiY29nbml0bzp1c2VybmFtZSI6Im5vZGVqc2FwaUBibW9ycmlzb24uZGV2IiwiZXhwIjoxNTkyMzYyNDc0LCJpYXQiOjE1OTIzNTg4NzUsImVtYWlsIjoibm9kZWpzYXBpQGJtb3JyaXNvbi5kZXYifQ.a3UyjpKWT3Dyy7m8orMuL0gm7NzJc2x0HRd2yqlf0oRo-nGYyCXchpgt63YWYnY54Dj9ihI6Vz7tixT1UcmFUfFsyMgjt4L1GDtcaJIbcviSVKvV4SFNR42fNKtwWxI_WUzI29V4pOQrLF1ZTOSYKMynzQI8o-Dk0IOjA-Q9MyXTsAbOM1jx19500LFAwjkSonUzA0x0thE5zyaYqjwevW0Z69VC3THssE8h1xpotXTeH2r1y7NQmJ8xRi18nvUPQeJAZB1NiWSf-jQgyGPRVdvN3AMc1szzzclVYZHGwzjqwrA0QnmGJUxXOwKFAbFOHNutsWHIUDW_bYEZieYNLg\n\n    &access_token=eyJraWQiOiJnaHNyRDMySkRQWWRXa1dieURVb0dQeGhYdXlmXC9OWTY3N25rMERvZnJFaz0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiI4OGU2ODI3ZS0xY2Y2LTQxMjYtYTZlNi02YmYwNDhmYzI4MTkiLCJldmVudF9pZCI6Ijc2NDc0YzBmLTliYjQtNDIwNC1iZjVjLWNhNDUzODY3YTBhMSIsInRva2VuX3VzZSI6ImFjY2VzcyIsInNjb3BlIjoib3BlbmlkIGVtYWlsIiwiYXV0aF90aW1lIjoxNTkyMzU4ODc0LCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV9TNTJTU28xZjIiLCJleHAiOjE1OTIzNjI0NzQsImlhdCI6MTU5MjM1ODg3NSwidmVyc2lvbiI6MiwianRpIjoiYzIzYzA4ODQtNDliYi00YmI4LTlmYWItMmZkMjIwNDk1YTQ0IiwiY2xpZW50X2lkIjoiNzhzZjdkcWthamc5ODkwZHUzaGkydm9xOTAiLCJ1c2VybmFtZSI6Im5vZGVqc2FwaUBibW9ycmlzb24uZGV2In0.Znuc_qA3JL5zDVcG0s5CgY4tQyeE826X1L_RJ0Fj4A2iZjddkVMXNkm25Xibg1Sz4R7YXs_VLhzmagrsS5uNFQr5BptgxVDHEzvVFWvYfmH2Z_fdZXXEZlqyomvyMs_zawRI71hVaoQ1Snp1B-PiSskPH_ZqG_HtetF0kmM2ifGEYlSQR4e74BHlZ8DCPsbCbG2fahsXHt-il4AipcuyNhReguLiyToXbHXqfpRmTZY5FYpJztI64S2Vp_6Si9Nt-gz8d1-eYFQFfDxZTU5G8dxq2RGFy4o-Kza1jKZevRTuAP6VecPBhXJhqgsGZ4G6OBc4QmNtQVjZwle82v6DqA\n\n    &expires_in=3600\n\n    &token_type=Bearer\n</code></pre><p>Lets break down what each of these values are used for;</p><ul><li><strong>id_token</strong> – contains identifying information about the user</li><li><strong>access_token</strong> – the authentication token we can use to authetnicate against our API using Cognito</li><li><strong>expires_in</strong> – the time in seconds when these tokens will expire and no longer be valid</li><li><strong>token_type</strong> – the type of token that was returned</li></ul><p>The access_token is what we’re mostly interested in. Using this, we can pass it to our API which will validate it with Cognito. Now that we have our tokens though, we can setup the API to accept them.</p><h2>Adding the Cognito-Express Middleware</h2><p>We’re going to use a package called <strong>Cognito-Express</strong>, which streamlines most of the heavy lifting of validating our tokens with Cognito. Open a terminal and run <code>npm install cognito-express</code>. Once the install is finished, add a new file to the root of the project called <strong>auth.js</strong> and add this code to it.</p><pre class=\"language-javascript\"><code>const CognitoExpress = require('cognito-express')\n\n// Setup CognitoExpress\nconst cognitoExpress = new CognitoExpress({\n  region: process.env.AWS_DEFAULT_REGION,\n  cognitoUserPoolId: process.env.COGNITO_USER_POOL_ID,\n  tokenUse: \"access\",\n  tokenExpiration: 3600\n})\n\nexports.validateAuth = (req, res, next) =&gt; {\n  // Check that the request contains a token\n  if (req.headers.authorization && req.headers.authorization.split(\" \")[0] === \"Bearer\") {\n    // Validate the token\n    const token = req.headers.authorization.split(\" \")[1]\n    cognitoExpress.validate(token, function (err, response) {\n      if (err) {\n        // If there was an error, return a 401 Unauthorized along with the error\n        res.status(401).send(err)\n      } else {\n        //Else API has been authenticated. Proceed.\n        next();\n      }\n    });\n  } else {\n    // If there is no token, respond appropriately\n    res.status(401).send(\"No token provided.\")\n  }\n}\n</code></pre><p>Now go into <strong>books.js</strong> and update the middleware portion of the POST call to contain our auth middleware we created.</p><pre class=\"language-javascript\"><code>// Import the middleware near the top of the file\nconst { validateAuth } = require('../auth')\n\n// Update the middleware to use validateAuth. I can use the spread operator (...) to include the validators as well.\nrouter.post('/', [validateAuth, ...validators.postBooksValidators], async (req, res) =&gt; {\n  // ... code removed for brevity\n}\n</code></pre><h2>Testing with Postman</h2><p>Back in Postman, update the POST /books request and add the authentication header by going to the Authorization tab, changing the type to Bearer Token, and adding your access_token into that field. Now if you check the headers, Postman will have inserted the Authorization header in for us.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/22e4c770-c834-4020-b250-f263ed566ad9-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/4b5caf4c-7391-4c4c-8d9b-fa3c6cf01896-Untitled.png\" /><figcaption></figcaption></figure><p>Send the request and it should work just as it did before. If you remove the token, or mistype it, you’ll get an appropriate response with a 401 Unauthorized status.</p><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/950c4082-b0a1-4b68-8bb0-0bceee437e67-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/1459acc0-9afb-43e2-996d-cc222842eb6a-Untitled.png\" /><figcaption></figcaption></figure><p>Also, since we only added our middleware on the POST handler, GET /books will work without a token at all! Feel free to test that for yourself.</p><p>Remember, the tokens expire after an hour so you’ll need to refresh them manually. This demonstrates at a very basic level how Cognito can be used to generate auth tokens. In a production scenario, you’d want a UI to handle the authentication using the Authorization code grant so you can get refresh tokens which allow you to refresh the users’ access_token after an hour without prompting them to log in again.</p><p>My next article will complete the series by uploading our API into AWS Elastic Beanstalk.</p></div>","featuredImage":"/img/n/adding-authentication-to-a-nodejs-api-using-aws-cognito/16bb2939-c96a-42e5-9242-82707a794f3a-adding-authentication-to-a-nodejs-api-using-aws-cognito-featured-image.png","excerpt":"The API we’ve built so far in this series is only a single route, but that route is pretty robust for what it is. But...","cachedOn":1681394561},{"id":"b8caff70-f102-4719-a40d-99fb4b323a66","notion_id":"b8caff70-f102-4719-a40d-99fb4b323a66","status":"Published","slug":"validating-nodejs-api-request-data","relation_series":["b699b272-b3fa-4f40-b921-33a6d9e73f22"],"publishOn":"2020-07-20T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-7-validating-nodejs-api-request-data","seriesOrder":4,"youTubeURL":"https://youtu.be/KREt-zJMoN4","title":"Validating NodeJS API Request Data","html":"<div><p>Validating the data being sent to an API is important because it filters out bad requests before they attempt to be processed or stored elsewhere. It also provides a way to tell the consumer of the API what they sent incorrectly so they can fix it and try again. We’re going to setup our API to validate our data using a library called <strong>express-validat</strong>or.</p><h2>Setting up a Validator</h2><p>Install the <strong>express-validator</strong> package by opening your terminal and running `npm install express-validator`. Then inside the <strong>books.js</strong> route file, import `check` and `validationResult` functions  of the library like so;</p><pre class=\"language-javascript\"><code>const { check, validationResult } = require('express-validator');</code></pre><p>Now were going to insert the `check` function as a middleware into our POST method handler. Let’s check to make sure the the rating of the book is a number. We also need to check check the results of the validator and send an appropriate response back to the user if the request is bad.</p><pre class=\"language-javascript\"><code>// Define a post method which will be used to accept data into the API\nrouter.post('/', [\n  // Check that the rating is a number\n  check('rating').isNumeric(),\n], async (req, res) =&gt; {\n  // If there are any validation errors, send back a 400 Bad Request with the errors\n  const errors = validationResult(req)\n  if (!errors.isEmpty()) {\n    res.status(400).json({\n      errors: errors.array()\n    })\n  }\n  // ... code removed for brevity\n}\n</code></pre><p>Let’s both a good request and a bad request with postman;</p><figure><img src=\"/img/n/validating-nodejs-api-request-data/95eb1d4b-a13e-42f5-81f0-b5541c383e2d-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/validating-nodejs-api-request-data/53b44436-b11b-41d6-a536-93ad55b30d56-Untitled.png\" /><figcaption></figcaption></figure><p>As you can see, the one with the bad request returns a 400 bad request status along with what was wrong with the request.</p><h2>Custom Validators</h2><p>You can also write custom validators as well. Say you want to check to see if a book exists in the database before saving it. Create a custom validator like so;</p><pre class=\"language-javascript\"><code>router.post('/', [\n  // Check that the rating is a number\n  check('rating').isNumeric(),\n  check('isbn').custom(async value =&gt; {\n    const params = {\n      TableName: 'nodejs-api'\n    }\n    let books = await docClient.scan(params).promise()\n    let existingBook = books.Items.find(b =&gt; b.info.isbn === value)\n    if (existingBook) {\n      return Promise.reject(\"That book already exists\");\n    }\n  })\n], async (req, res) =&gt; {\n  // ... code removed for brevity\n}</code></pre><p>Now if we try to save a book with the same ISBN, we’ll receive an error from the API stating that a book already exists with that number;</p><figure><img src=\"/img/n/validating-nodejs-api-request-data/f992e920-02a6-45a2-bfe8-e5a3a99ae314-Untitled.png\" /><figcaption></figcaption></figure><h2>Organizing Validators</h2><p>Since we need to validate any number of fields, adding them inline with our POST /books handler can make our code somewhat difficult to work with, so I recommend moving them off to their own file. Lets create a new file called <strong>booksValidators.js</strong> next to <strong>books.js</strong> and add the following code to it.</p><pre class=\"language-javascript\"><code>const {\n  check\n} = require('express-validator');\n\nconst AWS = require('aws-sdk');\n\n// Update our AWS Connection Details\nAWS.config.update({\n  region: process.env.AWS_DEFAULT_REGION,\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY\n});\n\n// Create the service used to connect to DynamoDB\nconst docClient = new AWS.DynamoDB.DocumentClient();\n\nexports.postBooksValidators = [\n  // Check that the rating is a number\n  check('rating').isNumeric(),\n  check('isbn').custom(async value =&gt; {\n    const params = {\n      TableName: 'nodejs-api'\n    }\n    let books = await docClient.scan(params).promise()\n    let existingBook = books.Items.find(b =&gt; b.info.isbn === value)\n    if (existingBook) {\n      return Promise.reject(\"That book already exists\");\n    }\n  })\n]</code></pre><p>Since were exporting our array of validators, we can require this file in <strong>books.js</strong> and reference our <code>postBooksValidators</code>instead.</p><pre class=\"language-javascript\"><code>// Add this near the top of the file\nconst validators = require('./booksValidators')\n\n// Replace your validators with the reference\nrouter.post('/', validators.postBooksValidators, async (req, res) =&gt; {\n  // ... code removed for brevity\n}</code></pre><p>Feel free to test your API again to make sure it is setup properly. Next up, I’ll be stepping through how to add authentication to our API using Cognito, an authentication framework from AWS.</p></div>","featuredImage":"/img/n/validating-nodejs-api-request-data/391f66dc-41ac-493e-b86e-6a87f063f99c-validating-nodejs-api-request-data-featured-image.png","excerpt":"Validating the data being sent to an API is important because it filters out bad requests before they attempt to be p...","cachedOn":1681394566},{"id":"dac8bcd0-b553-4ce2-bed1-57bf913f4939","notion_id":"dac8bcd0-b553-4ce2-bed1-57bf913f4939","status":"Published","slug":"saving-data-into-a-database-with-a-nodejs-api","relation_series":["b699b272-b3fa-4f40-b921-33a6d9e73f22"],"publishOn":"2020-07-13T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-7-saving-data-into-a-database-with-a-nodejs-api","seriesOrder":3,"youTubeURL":"https://youtu.be/zQWUVpbiJd8","title":"Saving Data into a Database with a NodeJS API","html":"<div><p>Now that we know how to send and receive data into our API. It’s time to store that data in case our API ever stops. To do this, I’m going to setup a DynamoDB table in AWS.</p><p>DynamoDB is a fully managed NoSQL database service in AWS. There is no underlying database hardware to setup or configure, you simply sign into AWS, create a table, and start storing your data.</p><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\">If you dont have an AWS account, you can setup for a free one by heading to <a href=\"https://aws.amazon.com/free/\" target=\"_blank\">https://aws.amazon.com/free/</a></div></div><h2>Setting up the Table</h2><p>From the AWS Console, drop down <strong>Services</strong> then select <strong>DynamoDB</strong> under <strong>Database</strong>. If this is your first time accessing the DynamoDB service, you’ll be presented with an intro screen with a button that says <strong>Create table</strong> in the middle. If not, it will be towards the top of the screen. Regardless of where it is, go ahead and click that button.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/480472fb-3373-4ca0-b3bb-7f7be4be0bbe-Untitled.png\" /><figcaption></figcaption></figure><p>Specify the name for the table and a primary key. I chose ‘<em>nodejs-api</em>’ and ‘<em>id</em>’ respectively. I’m also going to create a tag called ‘<em>application</em>’ and set the value to ‘<strong>nodejs-api</strong>’. Tags can be used for organization in whatever way you see fit, but they are ultimately optional. Once done, click <strong>Create</strong>.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/2377be60-0d9b-4889-9087-faaa8e0f1a39-Untitled.png\" /><figcaption></figcaption></figure><p>Creating the table should only take a few moments. Once it’s done, we’ll need to create the service account and assign the permissions to access the table.</p><h2>Creating the Service Account</h2><p>Now we need to setup an account we can use to connect to our DynamoDB Table. AWS technically doesn’t have a ‘Service Account’ (they are just standard user accounts) but I’ll be using the term since the account is ONLY going to be used to access our table we created. From the AWS console, drop down <strong>Service</strong> & select <strong>IAM</strong> under <strong>Security, Identity, & Compliance.</strong></p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/618d9033-bcb8-4c70-ad0a-36ae0fc09822-Untitled.png\" /><figcaption></figcaption></figure><p>Select Users from the sidebar and then Add User. In the next screen, give your user a name. I used ‘<em>nodejsApiSvc</em>’ in this example. Make sure you check <strong>Programmatic access</strong> & leave AWS Management Console access unchecked. Click <strong>Next:Permissions</strong>.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/1a4e56e4-f75f-48cb-8960-3815d5c4ea1d-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/71f1a9c4-8174-479f-866b-c4f20d23dbfa-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/6c63d40d-3470-40c5-8c85-f3ec03a12b7d-Untitled.png\" /><figcaption></figcaption></figure><p>It’s generally best practice to assign permissions to a group and add users to those groups so they receive the proper permissions. HOWEVER, since this isn’t a standard user, I’m going to assign the policy directly to the user. So click on <strong>Attach existing policies directly</strong> and then <strong>Create policy</strong>.</p><p>Under <strong>Service</strong>, search for <strong>DyanmoDB</strong> and select it. In the <strong>Actions</strong> section, check all boxes under the <strong>Access Level</strong>. We’re going to give our bot full permissions to the table we just created. Finally, we’ll need to specify the resource (which is our table) we want this policy to access.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/2acad071-6baa-4667-97d9-39e5522508bc-Untitled.png\" /><figcaption></figcaption></figure><p>For each of the 5 items listed under Resources, click on <strong>Add ARN</strong> and populate the table name with the name of the DynamoDB table you created earlier, leave the account number as is, and set the other fields to ‘*’. Your resources should look something like this, albeit with a different account number.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/e57adf07-ee3d-4fe5-b868-b71775741357-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/b60e296d-5ec0-4e4b-85c5-e0b6a5a58d63-Untitled.png\" /><figcaption></figcaption></figure><p>Once finished, click <strong>Review Policy</strong>, then give your policy a name.</p><p>Quick Tip: Naming custom IAM policies in AWS with an underscore (_) in the beginning will bring them to the top of the list 😉</p><p>I’ll name mine `<em>_nodejsApiPolicy</em>`. Add a description if desired and click <strong>Create policy</strong>. Once completed, you can close this tab and head back to the tab we were using to create the user. Click the <strong>refresh</strong> button on screen to refresh the policies and select the newly created policy. Then click <strong>Next: Tags</strong>.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/b0c1ad51-0a55-4e3a-a62d-d07066c6fb63-Untitled.png\" /><figcaption></figcaption></figure><p>I’m going to tag this user just as I did the DynamoDB table.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/f367934a-5ee0-4fe0-8ed2-8d566171fc85-Untitled.png\" /><figcaption></figcaption></figure><p>Click <strong>Next: Review</strong>, then <strong>Create user</strong>. Once the user is created, copy the <strong>Access key ID</strong> and <strong>Secret access key</strong> for the user into another file. NOTE: You cannot get these values again, so make sure to take them down now otherwise you’ll need to create another set of keys. Once done, go ahead and click <strong>Close</strong>.</p><h2>Setting up your API</h2><p>Now we need to setup our API to connect to DynamoDB. I’m going to start by installing two packages into our project;</p><ul><li>`dotenv` – Used to work with environment variables, which we’ll use to store our keys for debugging</li><li>`aws-sdk` – Used to talk to AWS</li></ul><p>Install them both by opening a terminal and issuing the command `npm install dotenv aws-sdk`. Once thats done, create a new file called <strong>.env</strong>. Add the service account keys into the file like so.</p><pre class=\"language-plain text\"><code>AWS_ACCESS_KEY_ID = \"your_key\"\nAWS_SECRET_ACCESS_KEY = \"your_key_secret\"\nAWS_DEFAULT_REGION = \"us-east-1\"</code></pre><p>Next, add the following line into your <strong>index.js</strong> file to import the <strong>.env</strong> variables into your application. After this, any of the variables can be accessed throughout your application by using <code>process.env.MY_DOTENV_KEY</code>.</p><pre class=\"language-javascript\"><code>require('dotenv').config();</code></pre><p>Finally, if you have your project setup in a git repository (which you should), add <code>.env</code> into your <strong>.gitignore</strong> file so it doesn’t save with the repository.</p><h2>Storing Data</h2><p>Now lets update our POST /books method handler to send our data into DynamoDB instead of into the in memory array we did last time. First, add the following code towards the top of your files, after your <code>require</code> statements.</p><pre class=\"language-javascript\"><code>const AWS = require('aws-sdk');\n// Update our AWS Connection Details\nAWS.config.update({\n  region: process.env.AWS_DEFAULT_REGION,\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY\n})\n// Create the service used to connect to DynamoDB\nconst docClient = new AWS.DynamoDB.DocumentClient();</code></pre><p>Then, update your POST handler to be like the following snippet. Note the comments which explain what each of the lines are doing.</p><pre class=\"language-javascript\"><code>// Define a post method which will be used to accept data into the API\nrouter.post('/', async (req, res) =&gt; {\n  // Setup the parameters required to save to Dynamo\n  const params = {\n    TableName: 'nodejs-api',\n    Item: {\n      // Use Date.now().toString() just to generate a unique value\n      id: Date.now().toString(),\n      // `info` is used to save the actual data\n      info: req.body\n    }\n  };\n\n  docClient.put(params, (error) =&gt; {\n    if (!error) {\n      // Send a status of 201, which means an item was created\n      res.status(201).send();\n    } else {\n      // If there was an error, send a 500 (Internal Server Error) along with the error\n      res.status(500).send('Unable to save record, err' + error);\n    }\n  });\n});\n</code></pre><p>Now lets test with Postman.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/072fbedc-f4bd-4e24-9d72-838ee1b1ef17-Untitled.png\" /><figcaption></figcaption></figure><p>Provided you got a 200 response, lets head back into AWS and check to see that the data we sent made it there. You can see your items by going to <strong>DynamoDB</strong>, selecting the table you created, and clicking the <strong>Items</strong> tab.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/fcb2d743-f595-41a5-bb62-3dc8040b80e2-Untitled.png\" /><figcaption></figcaption></figure><p>Great! Now that we have some data in AWS. Let’s update our GET /books method handler to query the data from AWS instead of our in memory data.</p><pre class=\"language-javascript\"><code>router.get('/:id?', async (req, res) =&gt; {\n  // Placing the params here since we may add filters to it\n  const params = {\n    TableName: 'nodejs-api'\n  };\n\n  let responseData;\n\n  // Setup any filters that come in\n  if (req.params.id) {\n    // params.Key is used to filter based on the primary key of the table\n    params.Key = {\n      id: req.params.id\n    }\n  } else {\n    if (req.query.id) {\n      params.Key = {\n        id: req.query.id\n      }\n    }\n  }\n\n  if (!params.Key) {\n    // If there are no params, scan the table and return all records\n    responseData = await docClient.scan(params).promise()\n  } else {\n    // Otherwise use the filted version, which is less costly\n    responseData = await docClient.get(params).promise()\n  }\n\n  // Finally, return the data\n  res.json(responseData)\n});\n</code></pre><p>Note: While this article covers the general concepts of connecting with and using DynamoDB, I would warn against using a scan operation unless absolutely necessary. Scans return every record in the database, which can become costly if your database grows in size. To learn more, here is a link to the official documentation from AWS: <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\" target=\"_blank\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html</a></p><p>Again, lets test with Postman. For the sake of showing the filtering working. I added a couple more books using the POST command from earlier.</p><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/4571862e-8046-463a-80ef-85ccaa769955-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/saving-data-into-a-database-with-a-nodejs-api/bbb580fd-120b-4c7f-9627-d42296476723-Untitled.png\" /><figcaption></figcaption></figure><p>Awesome! Now any time we send data to our API, it will persist regardless of how many times we stop and start it. My next article will cover request validation for data coming into our books API.</p></div>","featuredImage":"/img/n/saving-data-into-a-database-with-a-nodejs-api/36b0a035-6e5e-4fcc-b394-a74cb539ad86-saving-data-into-a-database-with-a-nodejs-api-featured-image.png","excerpt":"Now that we know how to send and receive data into our API. It’s time to store that data in case our API ever stops. ...","cachedOn":1681394567},{"id":"4d4b7256-fc51-4871-95cd-f1cdea085779","notion_id":"4d4b7256-fc51-4871-95cd-f1cdea085779","status":"Published","slug":"sending-and-querying-data-from-a-nodejs-api","relation_series":["b699b272-b3fa-4f40-b921-33a6d9e73f22"],"publishOn":"2020-07-06T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-7-sending-and-querying-data-from-a-nodejs-api","seriesOrder":2,"youTubeURL":"https://youtu.be/GV7LITAFXi8","title":"Sending & Querying Data from a NodeJS API","html":"<div><p>In this article, I’ll be diving into sending and querying data with the API we built in <a href=\"http://brianmorrison.me/blog/hello-world-nodejs-api\" target=\"_blank\">http://brianmorrison.me/blog/hello-world-nodejs-api</a>. I’ll start by adding a new route to request data and explaining the concept of routing in the API. Then we’ll move into using query parameters to filter the data being requested from the API. Finally, I’ll explain how to use a POST along with a package called body-parser to send data to the API for processing.</p><h2>API Request Routing</h2><p>In APIs, Routing refers to handling requests in different ways based on the URL path. In the last article, we sent a request to <code>http://localhost:3000</code> with no path, which means the route is technically <code>/</code>. But you can expand on your API by adding different routes. For example;</p><ul><li><code>http://localhost:3000/books</code> can handle various requests as it relates to the books entity</li><li><code>http://localhost:3000/movies</code> would handle requests related to the movies entity</li></ul><p>A common best practice is to name the route a pluralized version of the specific entity or concept the route relates to.</p><p>Let’s move on with our books concept and add a route for books. I’m going to start by adding some dummy data into our <strong>index.js</strong> file for books. Add the following after the first two lines in the file.</p><pre class=\"language-json\"><code>let books = [{\n    \"id\": 1,\n    \"isbn\": \"9781593275846\",\n    \"title\": \"Eloquent JavaScript, Second Edition\",\n    \"subtitle\": \"A Modern Introduction to Programming\",\n    \"author\": \"Marijn Haverbeke\",\n    \"published\": \"2014-12-14T00:00:00.000Z\",\n    \"publisher\": \"No Starch Press\",\n    \"pages\": 472,\n    \"description\": \"JavaScript lies at the heart of almost every modern web application, from social apps to the newest browser-based games. Though simple for beginners to pick up and play with, JavaScript is a flexible, complex language that you can use to build full-scale applications.\",\n    \"website\": \"http://eloquentjavascript.net/\"\n  },\n  {\n    \"id\": 2,\n    \"isbn\": \"9781449331818\",\n    \"title\": \"Learning JavaScript Design Patterns\",\n    \"subtitle\": \"A JavaScript and jQuery Developer's Guide\",\n    \"author\": \"Addy Osmani\",\n    \"published\": \"2012-07-01T00:00:00.000Z\",\n    \"publisher\": \"O'Reilly Media\",\n    \"pages\": 254,\n    \"description\": \"With Learning JavaScript Design Patterns, you'll learn how to write beautiful, structured, and maintainable JavaScript by applying classical and modern design patterns to the language. If you want to keep your code efficient, more manageable, and up-to-date with the latest best practices, this book is for you.\",\n    \"website\": \"http://www.addyosmani.com/resources/essentialjsdesignpatterns/book/\"\n  },\n  {\n    \"id\": 3,\n    \"isbn\": \"9781449365035\",\n    \"title\": \"Speaking JavaScript\",\n    \"subtitle\": \"An In-Depth Guide for Programmers\",\n    \"author\": \"Axel Rauschmayer\",\n    \"published\": \"2014-02-01T00:00:00.000Z\",\n    \"publisher\": \"O'Reilly Media\",\n    \"pages\": 460,\n    \"description\": \"Like it or not, JavaScript is everywhere these days-from browser to server to mobile-and now you, too, need to learn the language or dive deeper than you have. This concise book guides you into and through JavaScript, written by a veteran programmer who once found himself in the same position.\",\n    \"website\": \"http://speakingjs.com/\"\n  }\n]\n</code></pre><p>Then add the following under <code>app.get</code>. Note that we’re going to use <code>res.json(books)</code> to tell express we want to return JSON instead of plain text.</p><pre class=\"language-javascript\"><code>app.get('/books', async (req, res) =&gt; {\n  res.json(books);\n})\n</code></pre><p>Your <strong>index.js</strong> file should look like this now.</p><pre class=\"language-javascript\"><code>// Import express and create a new express app\nconst express = require('express');\nconst app = express();\n\n// Define some dummy book data, courtesy of https://gist.github.com/nanotaboada/6396437\nlet books = [{\n    \"id\": 1,\n    \"isbn\": \"9781593275846\",\n    \"title\": \"Eloquent JavaScript, Second Edition\",\n    \"subtitle\": \"A Modern Introduction to Programming\",\n    \"author\": \"Marijn Haverbeke\",\n    \"published\": \"2014-12-14T00:00:00.000Z\",\n    \"publisher\": \"No Starch Press\",\n    \"pages\": 472,\n    \"description\": \"JavaScript lies at the heart of almost every modern web application, from social apps to the newest browser-based games. Though simple for beginners to pick up and play with, JavaScript is a flexible, complex language that you can use to build full-scale applications.\",\n    \"website\": \"http://eloquentjavascript.net/\"\n  },\n  {\n    \"id\": 2,\n    \"isbn\": \"9781449331818\",\n    \"title\": \"Learning JavaScript Design Patterns\",\n    \"subtitle\": \"A JavaScript and jQuery Developer's Guide\",\n    \"author\": \"Addy Osmani\",\n    \"published\": \"2012-07-01T00:00:00.000Z\",\n    \"publisher\": \"O'Reilly Media\",\n    \"pages\": 254,\n    \"description\": \"With Learning JavaScript Design Patterns, you'll learn how to write beautiful, structured, and maintainable JavaScript by applying classical and modern design patterns to the language. If you want to keep your code efficient, more manageable, and up-to-date with the latest best practices, this book is for you.\",\n    \"website\": \"http://www.addyosmani.com/resources/essentialjsdesignpatterns/book/\"\n  },\n  {\n    \"id\": 3,\n    \"isbn\": \"9781449365035\",\n    \"title\": \"Speaking JavaScript\",\n    \"subtitle\": \"An In-Depth Guide for Programmers\",\n    \"author\": \"Axel Rauschmayer\",\n    \"published\": \"2014-02-01T00:00:00.000Z\",\n    \"publisher\": \"O'Reilly Media\",\n    \"pages\": 460,\n    \"description\": \"Like it or not, JavaScript is everywhere these days-from browser to server to mobile-and now you, too, need to learn the language or dive deeper than you have. This concise book guides you into and through JavaScript, written by a veteran programmer who once found himself in the same position.\",\n    \"website\": \"http://speakingjs.com/\"\n  }\n]\n\n// Define a basic GET request. The request & response object are passed in\napp.get('/', async (req, res) =&gt; {\n  // Use the request object to send back 'Hello world!'\n  res.send('Hello world!');\n});\n\n// Define another get with a route afterwards\napp.get('/books', async (req, res) =&gt; {\n  // Use res.json() instead of res.send() to tell express we're returning JSON\n  res.json(books);\n})\n\n// Define the port we're going to listen for requests on\nconst port = 3000;\n\n// Tell the app to listen on that port, and log out to the console once its listening.\napp.listen(port);\nconsole.log(`listening on http://localhost:${port}`);\n</code></pre><p>Lets test our work. In Postman, add a folder in the NodeJS API collection named <strong>Books</strong>, and add a new request inside that called <strong>Get Books</strong>. In the address bar, enter <code>http://localhost:3000/books</code> and click Send. You should get the sample data back.</p><h2>Reorganizing our Route</h2><p>Now we have a specific route for books. This is great, but we wont want all of our routes in the <strong>index.js</strong> file. can get out of hand depending on how many routes you have in the API. Let’s take a moment and move this route into a dedicated file. Create a new folder called <code>routes</code> and a file in that folder called <code>books.js</code>. Start but adding the following code to that file;</p><pre class=\"language-javascript\"><code>const express = require('express');\n// Define a new Router object\nconst router = new express.Router();\n\n// Export the router\nmodule.exports = router;\n\nNow move the new code from **index.js** into here. Change \\`app.get\\` to \\`router.get\\`. The completed file should look like this.\n\nconst express = require('express');\n// Define a new Router object\nconst router = new express.Router();\n\n// Define some dummy book data, courtesy of https://gist.github.com/nanotaboada/6396437\nlet books = [{\n    \"id\": 1,\n    \"isbn\": \"9781593275846\",\n    \"title\": \"Eloquent JavaScript, Second Edition\",\n    \"subtitle\": \"A Modern Introduction to Programming\",\n    \"author\": \"Marijn Haverbeke\",\n    \"published\": \"2014-12-14T00:00:00.000Z\",\n    \"publisher\": \"No Starch Press\",\n    \"pages\": 472,\n    \"description\": \"JavaScript lies at the heart of almost every modern web application, from social apps to the newest browser-based games. Though simple for beginners to pick up and play with, JavaScript is a flexible, complex language that you can use to build full-scale applications.\",\n    \"website\": \"http://eloquentjavascript.net/\"\n  },\n  {\n    \"id\": 2,\n    \"isbn\": \"9781449331818\",\n    \"title\": \"Learning JavaScript Design Patterns\",\n    \"subtitle\": \"A JavaScript and jQuery Developer's Guide\",\n    \"author\": \"Addy Osmani\",\n    \"published\": \"2012-07-01T00:00:00.000Z\",\n    \"publisher\": \"O'Reilly Media\",\n    \"pages\": 254,\n    \"description\": \"With Learning JavaScript Design Patterns, you'll learn how to write beautiful, structured, and maintainable JavaScript by applying classical and modern design patterns to the language. If you want to keep your code efficient, more manageable, and up-to-date with the latest best practices, this book is for you.\",\n    \"website\": \"http://www.addyosmani.com/resources/essentialjsdesignpatterns/book/\"\n  },\n  {\n    \"id\": 3,\n    \"isbn\": \"9781449365035\",\n    \"title\": \"Speaking JavaScript\",\n    \"subtitle\": \"An In-Depth Guide for Programmers\",\n    \"author\": \"Axel Rauschmayer\",\n    \"published\": \"2014-02-01T00:00:00.000Z\",\n    \"publisher\": \"O'Reilly Media\",\n    \"pages\": 460,\n    \"description\": \"Like it or not, JavaScript is everywhere these days-from browser to server to mobile-and now you, too, need to learn the language or dive deeper than you have. This concise book guides you into and through JavaScript, written by a veteran programmer who once found himself in the same position.\",\n    \"website\": \"http://speakingjs.com/\"\n  }\n]\n\n// Define another get with a route afterwards\nrouter.get('/', async (req, res) =&gt; {\n  // Use res.json() instead of res.send() to tell express we're returning JSON\n  res.json(books);\n})\n\n// Export the router\nmodule.exports = router;\n</code></pre><p>Finally, back in <strong>index.js</strong>, import the route and register it with the express app. Your <strong>index.js</strong> should look like this. I’ve commented near the line that registers the route.</p><pre class=\"language-javascript\"><code>// Import express and create a new express app\nconst express = require('express');\nconst app = express();\n// Import our new books route file\nconst booksRoute = require('./routes/books')\n\n// Define a basic GET request. The request & response object are passed in\napp.get('/', async (req, res) =&gt; {\n  // Use the request object to send back 'Hello world!'\n  res.send('Hello world!');\n});\n\n// Register the books routes with /books\napp.use('/books', booksRoute);\n\n// Define the port we're going to listen for requests on\nconst port = 3000;\n\n// Tell the app to listen on that port, and log out to the console once its listening.\napp.listen(port);\nconsole.log(`listening on http://localhost:${port}`);\n</code></pre><p>Test the API again to make sure it works.</p><p>If you run into any issues following these guides, feel free to jump into my Discord server at <a href=\"https://fullstack.chat/\" target=\"_blank\">https://fullstack.chat</a> to ask me personally for help.</p><h2>Querying Data</h2><p>Now our books dataset isn’t that big. But if your making a request to a database that has thousands or millions of records, returning all of the data in one shot is not reasonable, so you need a way to filter that data.</p><p>Query parameters are great for this. You can pass in a field and value to filter on by creating a url like so;</p><p><code>http://localhost:3000/books?id=1</code></p><p>The question mark in at the end of the URL is what tells the API that the query parameters are starting. You can chain multiple query parameters together using an ampersand.</p><p><code>http://localhost:3000/books?id=1&pages=472</code></p><p>But of course you’ll need to setup your API to handle this properly. So let’s update our get method in <strong>books.js</strong> to look like this.</p><pre class=\"language-javascript\"><code>// Define another get with a route afterwards\nrouter.get('/', async (req, res) =&gt; {\n  // Create an undefined object to return data with\n  let responseData;\n\n  // Check if the request objects' query includes a param\n  if (req.query.id) {\n    // Filter the data for that parameter.\n    // Note: Query parameters come in as strings, so we need to cast it to a Number for comparing\n    responseData = books.filter(book =&gt; book.id === Number(req.query.id));\n  }\n\n  // If response data is still undefined, there is no query. Return the whole dataset.\n  if (responseData === undefined) {\n    responseData = books\n  }\n\n  res.json(responseData)\n})\n</code></pre><p>Now lets test this out in Postman.</p><figure><img src=\"/img/n/sending-and-querying-data-from-a-nodejs-api/7bb7cd98-8f22-4130-83e1-3aa8bdcfa13e-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/sending-and-querying-data-from-a-nodejs-api/09216a3b-f4c9-4d7b-a192-c3892a69985c-Untitled.png\" /><figcaption></figcaption></figure><p>You can also use parts of the path when querying data as well. A common practice is to use the unique Id of an entity in the URL path to request back a single object:</p><p><code>http://localhost:3000/books/1</code></p><p>This would return a book with the Id of 1. To set this up in your API, update the get method as follows.</p><pre class=\"language-javascript\"><code>// Define another get with a route afterwards\nrouter.get('/:id?', async (req, res) =&gt; {\n  // Create an undefined object to return data with\n  let responseData;\n\n  if (req.params.id) {\n    responseData = books.find(book =&gt; book.id === Number(req.params.id))\n  } else {\n    // Check if the request objects' query includes a param\n    if (req.query.id) {\n      // Filter the data for that parameter.\n      // Note: Query parameters come in as strings, so we need to cast it to a Number for comparing\n      responseData = books.filter(book =&gt; book.id === Number(req.query.id));\n    }\n  }\n\n  // If response data is still undefined, there is no query. Return the whole dataset.\n  if (responseData === undefined) {\n    responseData = books\n  }\n\n  res.json(responseData)\n})\n</code></pre><p>Now test it with Postman.</p><figure><img src=\"/img/n/sending-and-querying-data-from-a-nodejs-api/982e9e10-b7f4-4afe-9248-a57a74ebb9ac-Untitled.png\" /><figcaption></figcaption></figure><p>Also worth noting that since we’re requesting back a single item using the URL parameter, we’re returning a single object using the <code>Array.find()</code> method instead of an array of objects.</p><h2>Sending data to the API</h2><p>What if we want to send data to an API for storing or processing? We can actually use the same route with a different method to do this. We need to add another library into our API to make handling these requests simple. Open terminal and run <code>npm install body-parser</code>. Once that’s done, add this after importing the booksRoute in <strong>index.js</strong>;</p><pre class=\"language-javascript\"><code>// Add body-parser middleware\nconst bodyParser = require('body-parser');\napp.use(bodyParser.json());\n</code></pre><p><strong>body-parser</strong> is whats called middleware in Express. On a very basic level, it’s a function that sits within the overall flow of the request within express. You can add multiple pieces of middleware in Express to modify the behaviour of the API. In this case, body-parser will parse the incoming request body into a JSON object we can more easily work with.</p><p>We’re going to add the following code after the get method in <strong>books.js</strong>;</p><pre class=\"language-javascript\"><code>// Define a post method which will be used to accept data into the API\nrouter.post('/', async (req, res) =&gt; {\n  // Add the request body object into the books array\n  books.push(req.body);\n  // Send a status of 201, which means an item was created\n  res.status(201).send();\n});\n</code></pre><p>Now back in Postman, lets create a new request;</p><ol><li>Create a new request in the Books folder</li><li>Set the address to the books URL, but change the method to POST</li><li>Click the Body tab</li><li>Set the type to Raw</li><li>And the formatting to JSON</li><li>Add the following to the request body</li><li>Click Send</li></ol><pre class=\"language-json\"><code>{\n    \"id\": 4,\n    \"isbn\": \"1234567890123\",\n    \"title\": \"My Cool Book\"\n}\n</code></pre><figure><img src=\"/img/n/sending-and-querying-data-from-a-nodejs-api/68e4d8e7-0912-4367-bc80-51bd2c063677-Untitled.png\" /><figcaption></figcaption></figure><p>Provided you got a 201 back under Status Code, now we can send the get request and see if the data was added to our books data.</p><figure><img src=\"/img/n/sending-and-querying-data-from-a-nodejs-api/7dc3ccc6-6f7a-4a13-a8f7-0b5edee570d8-Untitled.png\" /><figcaption></figcaption></figure><p>Great! Looks like everything is working correctly for now. We have three issues with the POST method at the moment.</p><ul><li>We are saving the data in memory, which means in when the API restarts, that data will be lost.</li><li>We are not validating any incoming data, so literally anything can be sent.</li><li>We are not authorizing any of the requests, which means anyone can send data to the API.</li></ul><p>I’ll be tackling these three issues over the next few articles. The next article in the series will go over how to store data in a database, specifically in AWS using DynamoDB.</p></div>","featuredImage":"/img/n/sending-and-querying-data-from-a-nodejs-api/f9c13afa-ed6b-4251-b579-5068993d7293-sending-and-querying-data-from-a-nodejs-api-featured-image.png","excerpt":"In this article, I’ll be diving into sending and querying data with the API we built in  http://brianmorrison.me/blog...","cachedOn":1681394569},{"id":"2b268056-4bc7-4f47-82a8-522727f6f612","notion_id":"2b268056-4bc7-4f47-82a8-522727f6f612","status":"Published","slug":"hello-world-nodejs-api","relation_series":["b699b272-b3fa-4f40-b921-33a6d9e73f22"],"publishOn":"2020-06-29T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-6-hello-world-nodejs-api","seriesOrder":1,"youTubeURL":"https://youtu.be/j8hhxiO1Ah0","title":"Hello World NodeJS API","html":"<div><p>Welcome to my series on building APIs using NodeJS! This kicks of the first article in the series where I’ll go over the basics on getting up and running with my framework of choice, ExpressJS. Over the course of the next two weeks, I’ll be expanding on this article and we’ll end by deploying a secure, well designed API to AWS.</p><p>In this article, I’ll demonstrate how to create a basic API and setup Postman to test the API. Once were setup and confirm working, I’ll be explaining some basic HTTP concepts as well as how they relate to the various settings in Postman.</p><h2>Prerequisites</h2><p>Before you get started, you’ll need the following installed to follow along with the series;</p><ul><li>VSCode – <a href=\"https://code.visualstudio.com/\" target=\"_blank\">https://code.visualstudio.com/</a></li><li>Postman – <a href=\"https://www.postman.com/downloads/\" target=\"_blank\">https://www.postman.com/downloads/</a></li><li>NodeJS – <a href=\"https://nodejs.org/en/download/\" target=\"_blank\">https://nodejs.org/en/download/</a></li></ul><p>Ill also be doing this in Windows. The process should be similar regardless of what operating system you are using.</p><h2>Setting up your Environment</h2><p>Let’s start by opening our working directory, then the terminal, then initialize the project by entering the following commands;</p><pre class=\"language-bash\"><code>npm init -y\nnpm install express</code></pre><p>If you don’t already have nodemon installed, install it as well with <code>npm install -g nodemon</code>.</p><p>Once that’s done, update your package.json by adding the <code>dev</code> script like so;</p><pre class=\"language-json\"><code>\"scripts\": {\n  \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\",\n  \"dev\": \"nodemon index.js\"\n},</code></pre><h2>Writing the Hello World API</h2><p>Now lets create a main entry point for our API called <strong>index.js</strong> and lets start by importing express and defining the app;</p><pre class=\"language-javascript\"><code>// Import express and create a new express app\nconst express = require('express');\nconst app = express();\n</code></pre><p>Now lets setup the app to respond to a basic get request. Add the following code;</p><pre class=\"language-javascript\"><code>// Define a basic GET request. The request & response object are passed in\napp.get('/', async (req, res) =&gt; {\n  // Use the request object to send back 'Hello world!'\n  res.send('Hello world!');\n});\n</code></pre><p>Now finally, add the following code to start the server</p><pre class=\"language-javascript\"><code>// Define the port we're going to listen for requests on\nconst port = 3000;\n\n// Tell the app to listen on that port, and log out to the console once its listening.\napp.listen(port);\nconsole.log(`listening on http://localhost:${port}`);\n</code></pre><p>That should do its. Start your app by running <code>npm run dev</code>. You should see the following.</p><figure><img src=\"/img/n/hello-world-nodejs-api/f807b52d-a401-4ef0-b725-51e19226777e-Untitled.png\" /><figcaption></figcaption></figure><h2>Testing the API</h2><p>Now open Postman and do the following;</p><ul><li>Create a new collection to store our tests for the series. I named mine ‘NodeJS API’</li><li>Create a new request, name it ‘Hello World’</li><li>Set the address bar to ‘http://localhost:3000` and click Send</li></ul><figure><img src=\"/img/n/hello-world-nodejs-api/1f90af24-71da-4e86-9f57-f1b89799cc0d-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hello-world-nodejs-api/737e4e53-e5df-46af-804c-f0d96b216b8d-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hello-world-nodejs-api/5367e780-daa9-4b61-8ba0-23d4732150f2-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hello-world-nodejs-api/efc9d549-f8f1-497b-9322-79cc522ec151-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/hello-world-nodejs-api/a3304cde-48e1-4b57-9da0-411146512ed4-Untitled.png\" /><figcaption></figcaption></figure><p>You should receive the <code>Hello world!</code> message in your response body. If you did, congrats on getting your first API setup! Now lets explain what the heck is happening under the covers.</p><h2>HTTP Request Basics</h2><p>Before we get specific to the API, its important to understand the basics of HTTP requests. HTTP is shorthand for Hypertext Transport Protocol. It’s a way to send and receive specially crafted messages to and from an HTTP server. What we did is create an HTTP server that will send a message back of <code>Hello world!</code> whenever it receives a request.</p><p>Your browser actually does this every time you request a web page. It will reach out to the server using a request and the server will respond with the code that’s required to render the web page.</p><p>Requests to APIs operate exactly the same way, but instead of sending web pages, they generally send back messages for the client to process.</p><h2>HTTP Request/Response Anatomy</h2><p>In the background, an HTTP message looks something like this;</p><figure><img src=\"/img/n/hello-world-nodejs-api/bf4f708c-798f-4c07-b25f-56b461911d10-Untitled.png\" /><figcaption></figcaption></figure><p>The important parts to consider are the <strong>method</strong>, <strong>headers</strong>, and <strong>body</strong>, and the status in the response. Over the next few articles, I’ll be diving into how each of these are considered when building & consuming an API.</p><h2>Some Postman Basics</h2><p>Postman is a great tool for testing APIs and I’ll be using it extensively in this series. I want to take a moment and explain some Postman basics before we proceed in building APIs.</p><h3>Collections, Folders, & Requests</h3><p>You can configure and save HTTP requests into collections in postman. Collections are ways to organize your requests, and you can create folders within collections for further organization. You can also get fancy and add settings to collections so they are inherited by the requests in them. This will become especially handy when we get into the authentication portion of the series. For now, we’ll be using them mostly to keep organized.</p><figure><img src=\"/img/n/hello-world-nodejs-api/028e52b8-e955-4526-a227-0e1f6f278a9f-Untitled.png\" /><figcaption></figcaption></figure><h3>The Address Bar</h3><p>The address bar is where you’ll put in the endpoint you want to test, along with any query parameters and the method. We’ll get more into how to handle these over the series.</p><figure><img src=\"/img/n/hello-world-nodejs-api/f17b46a0-da70-4dfe-b804-5dfa6ae47f11-Untitled.png\" /><figcaption></figcaption></figure><h3>The Request Area</h3><p>For each request, there are a lot of options you can configure, but you don’t need to configure all of them. The main ones we’ll explore are headers and body. The address, method, and parameters are technically part of the request, they are just placed in a different section of Postman.</p><figure><img src=\"/img/n/hello-world-nodejs-api/d3ab2b45-fda7-4ffc-9262-81c1773ca80e-Untitled.png\" /><figcaption></figcaption></figure><h3>The Response Area</h3><p>The response pane is where you’ll see the response to your request once you send it. This is used to verify that the response received from a request matches what you expect. Note specifically the status code, body, and headers area.</p><figure><img src=\"/img/n/hello-world-nodejs-api/9fd04ea7-db7e-46ed-a88b-b9c8d6ce2ab5-Untitled.png\" /><figcaption></figcaption></figure><h3>Environments Area</h3><p>The last part of Postman worth explaining is the Environments area. You can configure variables here and reference them throughout Postman. This is especially useful when testing different environments like dev, test, and production. I’ll touch on this in the last article in the series when we deploy out to AWS.</p><figure><img src=\"/img/n/hello-world-nodejs-api/e1e0e320-290e-4284-aaf2-74b073b4df00-Untitled.png\" /><figcaption></figcaption></figure><p>If you haven’t signed into Postman, I highly recommend it. It will sync and save your settings in case anything happens to your machine.</p><p>In my next article, I’ll be building on the API we have now by explaining routing and adding some new routes. I’ll be demonstrating how to query data from the API as well as send data to the API for processing using a post method.</p></div>","featuredImage":"/img/n/hello-world-nodejs-api/ca86b4e6-ad71-4c98-8a89-2bf95ca12e27-hello-world-nodejs-api-featured-image.png","excerpt":"Welcome to my series on building APIs using NodeJS! This kicks of the first article in the series where I’ll go over ...","cachedOn":1681394570},{"id":"46a2711c-725e-4f9e-b6ea-1f8cfdff8720","notion_id":"46a2711c-725e-4f9e-b6ea-1f8cfdff8720","status":"Published","slug":"deploy-your-discord-bot-to-aws","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-24T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-11-deploy-your-discord-bot-to-aws","seriesOrder":11,"youTubeURL":"https://www.youtube.com/watch?v=1Cd16JVO6jM","title":"Deploy Your Discord Bot to AWS","html":"<div><p>We’ve covered a lot of ground over the past articles, and we’ve reached a point where we should deploy our bot to somewhere that isn’t our computer so it can be running at all times. In this final article of the series, I’ll be covering how to deploy your bot code out to AWS using Elastic Beanstalk. Elastic Beanstalk (EB for short) provides a streamlined way for you to simply upload a copy of your code and let AWS handle the heavy lifting when it comes to provisioning resources and keeping your code running.</p><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\">If you’ve been following along with this series, you already have a free AWS account from setting up DynamoDB. If not, head over to <a href=\"https://aws.amazon.com/free\" target=\"_blank\">https://aws.amazon.com/free</a> to create the account before continuing.</div></div><h2>Preparing our Code</h2><p>We need to make a few changes to prep our code for running in EB. The first thing we need to change is our npm scripts. Since EB will run `npm start` internally by default, we don’t want it running nodemon since that’s primarily a development tool. So lets change our start script to `node index.js`, and add a dev script to use when running our bot locally. Update your `scripts` section to match the below entries.</p><pre class=\"language-json\"><code>\"scripts\": {\n  \"start\": \"node index.js\",\n  \"dev\": \"nodemon index.js\"\n},\n</code></pre><p>Going forward, you’ll need to run the command <code>npm run dev</code> in your terminal to develop locally with nodemon.</p><p>Next, we’ll need to create a zipped archive to upload to EB. You can use whatever tool you prefer to create zip folders, but the important thing is that you need all of the files to be at the root of the zip file. A common mistake is to zip the folder itself which will nest your code inside that folder when creating the zip file.</p><h2>Setup Elastic Beanstalk</h2><p>From the AWS Console, click on Services, then Elastic Beanstalk under Compute. Once there, click on Create Application.</p><figure><img src=\"/img/n/deploy-your-discord-bot-to-aws/4a4a79b4-134b-4409-b32a-761aa30fd54d-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploy-your-discord-bot-to-aws/ab4a7a89-e36f-411d-9ad7-02f0a9994dd9-Untitled.png\" /><figcaption></figcaption></figure><p>I’ll be naming the application ‘<em>demo-discord-bot</em>’ and giving it the ‘<em>application</em>’ tag of ‘<em>demo-discord-bot</em>’ just to keep things consistent with the last article where we setup DynamoDB to host our data (Read more here: <a href=\"https://brianmorrison.me/blog/storing-info-in-a-database-with-discord-bots\" target=\"_blank\">https://brianmorrison.me/blog/storing-info-in-a-database-with-discord-bots</a>)</p><figure><img src=\"/img/n/deploy-your-discord-bot-to-aws/e9edd010-e6e0-498a-8d52-8179526edd48-Untitled.png\" /><figcaption></figcaption></figure><p>Under <strong>Platform</strong>. Select <strong>Node.js</strong>, you can <strong>Platform branch</strong> and <strong>version</strong> as their defaults. Under <strong>Application code</strong>, select <strong>Upload your code</strong>. In the Source code origin section, you’ll need to upload the zipped copy of your code. The version label can be left as is for now.</p><figure><img src=\"/img/n/deploy-your-discord-bot-to-aws/abcd7bab-25ad-47f1-bcfc-9e4e20be65f3-Untitled.png\" /><figcaption></figcaption></figure><p>Once done, click <strong>Create application</strong>. You’ll then be taken to a screen where you can monitor the deployment. After that, if all succeeded, you’ll be presented with a screen like this to show the status of your application.</p><figure><img src=\"/img/n/deploy-your-discord-bot-to-aws/8312d6c1-9cd1-428b-975c-cf6cbf52586d-Untitled.png\" /><figcaption></figcaption></figure><p>Looks like our EB environment is fully up and running. Lets head back over to Discord and see if we’re up and running.</p><figure><img src=\"/img/n/deploy-your-discord-bot-to-aws/8b7ddac1-2751-4f02-b05f-806b9d24ea2d-Untitled.png\" /><figcaption></figcaption></figure><p>Success! Our bot is now fully running in AWS. Lets test a command.</p><figure><img src=\"/img/n/deploy-your-discord-bot-to-aws/a675c283-96b6-49a4-8709-083293bb49dd-Untitled.png\" /><figcaption></figcaption></figure><p>Everything appears to be working as expected.</p><p>This concludes this series on Coding Discord Bots! Next month, I’ll be exploring NodeJS APIs in Depth. Happy coding 😁</p></div>","excerpt":"We’ve covered a lot of ground over the past articles, and we’ve reached a point where we should deploy our bot to som...","cachedOn":1681394571},{"id":"8710f04d-4963-4aa8-8d1c-bea73595f0da","notion_id":"8710f04d-4963-4aa8-8d1c-bea73595f0da","status":"Published","slug":"storing-info-in-a-database-with-discord-bots","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-22T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-10-storing-info-in-a-database-with-discord-bots","seriesOrder":10,"youTubeURL":"https://youtu.be/LXIz0a7FKCw","title":"Storing Info in a Database with Discord Bots","html":"<div><p>Up until this point, all of the code we’ve written has been for our bot has been used within Discord itself. Bots themselves have no way of persisting data, so what do you do if you need to save data for a later time? You need a place to store data. In this article, we’re going to create a application to collect information from a user and store it in AWS using DynamoDB.</p><p>Fair warning, this is going to be a long one 🙂</p><h2>Setting up Dynamo DB</h2><p>DynamoDB is a low cost and highly available nosql solution from AWS. It is covered by a fairly generous free tier which makes it perfect for development. I’ll give a brief overview on how to configure AWS & DynamoDB for this example, but this will not be an in-depth article on DynamoDB. More on that at another time.</p><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\">If you haven’t already, you can create a free AWS account by heading to <a href=\"https://aws.amazon.com/free\" target=\"_blank\">https://aws.amazon.com/free</a></div></div><p>From the AWS Console, drop down <strong>Services</strong> then select <strong>DynamoDB</strong> under <strong>Database</strong>. If this is your first time accessing the DynamoDB service, you’ll be presented with an intro screen with a button that says <strong>Create table</strong> in the middle. If not, it will be towards the top of the screen. Regardless of where it is, go ahead and click that button.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/c48ed02c-3b50-45d2-9dca-1c1c3efabe9f-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/2ccc69bc-b324-4831-9c54-8c3efbfac78c-Untitled.png\" /><figcaption></figcaption></figure><p>Specify the name for the table and a primary key. I chose ‘<em>demo-discord-bot</em>’ and ‘<em>id</em>’ respectively. I’m also going to create a tag called ‘<em>application</em>’ and set the value to ‘<em>demo-discord-bot</em>’. Tags can be used for organization in whatever way you see fit, but they are ultimately optional. Once done, click <strong>Create</strong>.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/39be56b4-9227-4d26-a757-e8d139b52a35-Untitled.png\" /><figcaption></figcaption></figure><p>Creating the table should only take a few moments. Once it’s done, we’ll need to create the service account and assign the permissions to access the table.</p><h2>Creating the Service Account</h2><p>Now we need to setup an account we can use to connect to our DynamoDB Table. AWS technically doesn’t have a ‘Service Account’ (they are just standard user accounts) but I’ll be using the term since the account is ONLY going to be used to access our table we created. From the AWS console, drop down <strong>Service</strong> & select <strong>IAM</strong> under <strong>Security, Identity, & Compliance.</strong></p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/39ca7f5a-cd4b-4b35-b1c0-871e1bf87b00-Untitled.png\" /><figcaption></figcaption></figure><p>Select Users from the sidebar and then Add User. In the next screen, give your user a name. I used ‘<em>demoDiscordBotSvc</em>’ in this example. Make sure you check <strong>Programmatic access</strong> & leave AWS Management Console access unchecked. Click <strong>Next:Permissions</strong>.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/45b254e5-eab4-45cb-adde-76f46e9499b7-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/67b09204-2d50-459d-b2f4-6cad248f0a50-Untitled.png\" /><figcaption></figcaption></figure><p>It’s generally best practice to assign permissions to a group and add users to those groups so they receive the proper permissions. HOWEVER, since this isn’t a standard user, I’m going to assign the policy directly to the user. So click on <strong>Attach existing policies directly</strong> and then <strong>Create policy</strong>.</p><p>Under <strong>Service</strong>, search for <strong>DyanmoDB</strong> and select it. In the <strong>Actions</strong> section, check all boxes under the <strong>Access Level</strong>. We’re going to give our bot full permissions to the table we just created. Finally, we’ll need to specify the resource (which is our table) we want this policy to access.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/b21b813d-87be-4fd8-a82e-858a0aa1fb5f-Untitled.png\" /><figcaption></figcaption></figure><p>For each of the 5 items listed under Resources, click on <strong>Add ARN</strong> and populate the table name with the name of the DynamoDB table you created earlier, leave the account number as is, and set the other fields to ‘*’. Your resources should look something like this, albeit with a different account number.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/956f1592-909c-44a7-8d9a-3bff860d6bc9-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/c4e6ca84-b65d-4b7d-9c3b-2643b3c83ed3-Untitled.png\" /><figcaption></figcaption></figure><p>Once finished, click <strong>Review Policy</strong>, then give your policy a name.</p><p>Quick Tip: Naming custom IAM policies in AWS with an underscore (_) in the beginning will bring them to the top of the list 😉</p><p>I’ll name mine <code>_demoDiscordBotPolicy</code>. Add a description if desired and click <strong>Create policy</strong>. Once completed, you can close this tab and head back to the tab we were using to create the user. Click the <strong>refresh</strong> button on screen to refresh the policies and select the newly created policy. Then click <strong>Next: Tags</strong>.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/5c25460c-ec9d-4120-a59a-5ce2640d6a10-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/c5958442-2f47-4ae2-bf47-94926c37eccb-Untitled.png\" /><figcaption></figcaption></figure><p>I’m going to tag this user just as I did the DynamoDB table.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/484f4fd3-7de5-4446-8629-987cabdcbd72-Untitled.png\" /><figcaption></figcaption></figure><p>Click <strong>Next: Review</strong>, then <strong>Create user</strong>. Once the user is created, copy the <strong>Access key ID</strong> and <strong>Secret access key</strong> for the user into another file. NOTE: You cannot get these values again, so make sure to take them down now otherwise you’ll need to create another set of keys. Once done, go ahead and click <strong>Close</strong>.</p><h2>Building the Command</h2><p>We’re going to use similar techniques we used to build our survey (Read more here: <a href=\"https://brianmorrison.me/blog/building-a-survey-discord-bot\" target=\"_blank\">https://brianmorrison.me/blog/building-a-survey-discord-bot</a>). Most of the code below is similar to what we have so far. Let’s use the command of <code>!apply</code> in a new command file <strong>apply.js</strong>;</p><pre class=\"language-javascript\"><code>  module.exports = {\n    // Define the prefix\n    prefix: \"!apply\",\n    // Define a function to pass the message to\n    fn: (msg) =&gt; {\n      let application = {}\n      let filter = (msg) =&gt; !msg.author.bot;\n      let options = {\n        max: 1,\n        time: 15000\n      };\n\n      msg.member.send(\"Thanks for your interest in the position! First, what is your name?\")\n        .then(dm =&gt; {\n          // After each question, we'll setup a collector on the DM channel\n          return dm.channel.awaitMessages(filter, options)\n        })\n        .then(collected =&gt; {\n          // Convert the collection to an array & get the content from the first element\n          application.name = collected.array()[0].content;\n          // Ask the next question\n          return msg.member.send(\"Got it, now what's your email address?\")\n        })\n        .then(dm =&gt; {\n          return dm.channel.awaitMessages(filter, options)\n        })\n        .then(collected =&gt; {\n          application.emailAddress = collected.array()[0].content;\n          return msg.member.send(\"Excellent. Finally, tell us why you'd be a good fit.\")\n        })\n        .then(dm =&gt; {\n          return dm.channel.awaitMessages(filter, options)\n        })\n        .then(collected =&gt; {\n          application.pitch = collected.array()[0].content;\n          console.log(application)\n        })\n    }\n  }\n</code></pre><p>You’ll notice one main difference is that we’re actually sending a DM to the user to keep their information private. Here’s what the survey looks like in action so far.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/276f27df-d69b-4c4d-b363-92c0a4c41b59-Untitled.png\" /><figcaption></figcaption></figure><p>And to confirm the data looks they way we expect, we’re just logging out to the console.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/14aa9630-05ee-4dbd-b585-893d3e4135f4-Untitled.png\" /><figcaption></figcaption></figure><h2>Saving to the DynamoDB Table</h2><p>Now that we confirmed our data looks good, we need to setup the code to store that data in the DynamoDB table. We’re going to use an npm package called <strong>aws-sdk</strong> which contains the framework to access AWS resources with NodeJS. Open the terminal and issue the command <code>npm install aws-sdk</code> to add the package to our bot.</p><p>In the previous step, we saved off the our service account access key id & secret. Since this is private info, we’re going to add these values into our <strong>.env</strong> file, along with the default AWS region.</p><pre class=\"language-plain text\"><code>AWS_ACCESS_KEY_ID = \"your_access_key_id\"\nAWS_SECRET_ACCESS_KEY = \"your_secret_key\"\nAWS_DEFAULT_REGION = \"us-east-1\"</code></pre><p>Now we need to add some additional imports to our <strong>apply.js</strong> file. Add the following lines to the top of the file, before the <code>module.exports</code>;</p><pre class=\"language-javascript\"><code>require('dotenv').config();\nconst AWS = require('aws-sdk');\n// Update our AWS Connection Details\nAWS.config.update({\n  region: process.env.AWS_DEFAULT_REGION,\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY\n})\n// Create the service used to connect to DynamoDB\nconst docClient = new AWS.DynamoDB.DocumentClient();\n</code></pre><p>Next, we’re going to replace the <code>console.log()</code> line in the last promise with the following code to save the data to the table.</p><pre class=\"language-javascript\"><code>// Setup the parameters required to save to Dynamo\nconst params = {\n  TableName: 'demo-discord-bot',\n  Item: {\n    // Use Date.now().toString() just to generate a unique value\n    id: Date.now().toString(),\n    // `info` is used to save the actual data\n    info: application\n  }\n}\n\ndocClient.put(params, (error) =&gt; {\n  if (!error) {\n    // Finally, return a message to the user stating that the app was saved\n    return msg.member.send(\"We've successfully received your application. We'll be in touch 😊.\")\n  } else {\n    throw \"Unable to save record, err\" + error\n  }\n})\n</code></pre><p>Now lets test that application form one more time with <code>!apply</code>;</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/0b540bba-e7af-4f5c-ba78-470221af5475-Untitled.png\" /><figcaption></figcaption></figure><p>Now lets check to make sure our application was stored in DynamoDB. You can do this by heading back into the DynamoDB service in AWS going to <strong>Tables</strong>, select your table, then <strong>Items</strong>.</p><figure><img src=\"/img/n/storing-info-in-a-database-with-discord-bots/5af66468-aa59-43cf-86ab-0b8ecc18b51a-Untitled.png\" /><figcaption></figcaption></figure><p>We’re coming down to the last article in the series! The next one will show you how we can host our code in AWS.</p></div>","excerpt":"Up until this point, all of the code we’ve written has been for our bot has been used within Discord itself. Bots the...","cachedOn":1681394572},{"id":"b91ee508-b5b0-49a1-acc7-6f753bc9acc5","notion_id":"b91ee508-b5b0-49a1-acc7-6f753bc9acc5","status":"Published","slug":"working-with-discord-webhooks","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-19T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-9-working-with-discord-webhooks","seriesOrder":9,"youTubeURL":"https://youtu.be/erq5GvmaLYE","title":"Working With Discord Webhooks","html":"<div><p>I have a confession to make. Although this is part of the Discord Bots series, this article actually has nothing to do with Discord bots. Even so, it is extremely useful to know how webhooks work in Discord.</p><p>Webhooks provide a way to create a public HTTP endpoint that allow services to send messages to. Discord will parse those messages and display them in the channel where you created the webhook. This works great when you want to receive notifications from external systems.</p><p>Note that anyone who has your webhook can send messages to the channel, so treat these as secrets (because they are).</p><h2>Create the Webhook</h2><p>Creating webhooks in Discord is very easy. Inside your Discord server, right click on the channel you want the messages to drop and click <strong>Edit Channel</strong>, then <strong>Webhooks</strong>, and finally Create <strong>Webhook</strong>;</p><figure><img src=\"/img/n/working-with-discord-webhooks/eb2e0677-d1f6-494b-84d4-7da73b3f2bbb-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/working-with-discord-webhooks/5f1e2213-5e4c-4279-8306-4d0cd4fc64e1-Untitled.png\" /><figcaption></figcaption></figure><p>We’ll leave the default options for now. You can give your webhook a different name and a profile image, which will show in the channel when the message is displayed as well, but its entirely optional. Copy the <strong>Webhook URL</strong> that is provided and click save.</p><figure><img src=\"/img/n/working-with-discord-webhooks/27ab91ca-5381-4618-b474-3813a00bdcd4-Untitled.png\" /><figcaption></figcaption></figure><h2>Sending a Message using Postman</h2><p>Webhooks receive message using HTTP POST actions. You can simulate this using Postman, which is a very popular API testing client. If you dont have it installed already, you can download it from <a href=\"https://www.postman.com/downloads/\" target=\"_blank\">https://www.postman.com/downloads/</a></p><p>In Postman, do the following;</p><ul><li>Create a new tab for testing the webhook.</li><li>Paste your webhook url in the address bar.</li><li>Just to the left of the address bar, change the method to <strong>POST</strong>.</li><li>Click on the <strong>Body</strong> tab underneath the address bar and select <strong>raw</strong>.</li><li>To the right of those options, click on <strong>Text</strong> and change it to <strong>JSON</strong>.</li><li>Finally, paste this JSON into the body.</li></ul><pre class=\"language-javascript\"><code>{\n\t\"content\": \"Hello from my webhook!\"\n}</code></pre><p>Once you click <strong>Send</strong>, your Postman client should look something like this. You can use the numbered steps that correspond with the directions above, but I also wanted to call out the <strong>Status</strong> in the response pane. You should receive a <strong>204 No Content</strong>. This means the message was sent correctly. If not, you’ll receive an error message in the response body.</p><figure><img src=\"/img/n/working-with-discord-webhooks/2f85c7a6-0b33-43b6-bac9-16f481482f1e-Untitled.png\" /><figcaption></figcaption></figure><p>Back in Discord, you should see your message with your webhook as the sender.</p><figure><img src=\"/img/n/working-with-discord-webhooks/1b7ccaa0-7397-4278-ab62-dfbef4ffb347-Untitled.png\" /><figcaption></figcaption></figure><h2>Sending Embeds</h2><p>You can actually send embeds into Discord using webhooks as well. The method is exactly the same, just the JSON is different. Using our Postman settings above, paste the following JSON into the Postman body and send it.</p><pre class=\"language-javascript\"><code>{\n\t\"embeds\": [{\n\t\t\"title\": \"Webhook Embed\",\n\t\t\"description\": \"Hello from my webhook!\"\n\t}]\n}\n</code></pre><p>And instead of the text we received before, we now have a rich card instead.</p><figure><img src=\"/img/n/working-with-discord-webhooks/50074c76-444d-48fc-8845-b06b24715cdc-Untitled.png\" /><figcaption></figcaption></figure><p>You can get pretty creative with embeds, including images, fields, URLs, etc. Documentation of all available fields can be found here: <a href=\"https://discord.com/developers/docs/resources/channel#embed-object\" target=\"_blank\">https://discord.com/developers/docs/resources/channel#embed-object</a></p><p>Next week, I’ll be going over how to store data in a database as well as deploying our bot to Azure.</p></div>","excerpt":"I have a confession to make. Although this is part of the Discord Bots series, this article actually has nothing to d...","cachedOn":1681394573},{"id":"79b9787c-2159-4e15-9602-8e38cc893329","notion_id":"79b9787c-2159-4e15-9602-8e38cc893329","status":"Published","slug":"integrating-discord-bots-with-other-apis","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-17T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-8-integrating-discord-bots-with-other-apis","seriesOrder":8,"youTubeURL":"https://youtu.be/erq5GvmaLYE","title":"Integrating Discord Bots with Other APIs","html":"<div><p>A while back, there was a movement in the tech industry called Chat Ops. Essentially, it was the idea that chat applications (like Discord) can be used to trigger commands that facilitate parts of the Software Development process. In order to do this, bots are used to communicate with other services to trigger actions, gather data, etc.</p><p>Today, we’re going to explore how to do this using <strong>Axios</strong>, my preferred Node HTTP client.</p><h2>Installing Axios</h2><p>Axios is an HTTP client library for Node. I prefer it over others basically due to its simplicity in setting up requests. Install Axios by opening your terminal and issuing the command `npm install axios`. This will install the library into our package for the bot to use.</p><h2>Pulling Gists with Our Bot</h2><p>An easy way to demonstrate how to interact with external services is using the GitHub api, specifically for Gists. Gists are a way for developers to create public (or private) snippets of code. Not quite full blown repositories, but helpful nonetheless.</p><p>We’re going to add a command called `!gists` that will pull in all public Gists for a user and list the links in our channel. Add the following code into the message handler;</p><pre class=\"language-javascript\"><code>const axios = require('axios')\n\nmodule.exports = {\n  // Define the prefix\n  prefix: \"!gists\",\n  // Define a function to pass the message to\n  fn: (msg) =&gt; {\n    let args = msg.content.split(' ')\n    let username = args[1];\n    // Check to make sure a username is provided\n    if (!username) {\n      msg.reply(\"Please provide a username.\");\n    } else {\n      // Setup our Axios options. Perform a GET request to the url specified\n      let axiosOptions = {\n        method: \"get\",\n        url: `https://api.github.com/users/${username}/gists`\n      }\n\n      // Pass in the options to Axios to make the request\n      axios(axiosOptions)\n        .then(response =&gt; {\n          console.log(response.data)\n        })\n    }\n  }\n}\n</code></pre><p>Now try to issue the command <code>!gists</code> in your server. You should have some console output of the data that was returned from the API request;</p><figure><img src=\"/img/n/integrating-discord-bots-with-other-apis/1bd9b62e-34ee-4ad1-9662-5b9031709638-Untitled.png\" /><figcaption></figcaption></figure><p>Using this, we can determine the structure of the data and pull what we need from it. For simplicity, I’m going to just loop through the <code>description</code> and output it to an Embed.</p><p>Replace the <code>console.log()</code> line with this;</p><pre class=\"language-javascript\"><code>// Map our data to grab only the description\nlet gists = response.data.map(el =&gt; el.description)\n\n// Loop through the descriptions and add a line to the embed\nlet embedDescription = \"\";\ngists.forEach(el =&gt; {\n  embedDescription += `- ${el}\\n`\n});\n\n// Create the embed & send it to the channel\nlet embed = new Discord.MessageEmbed()\n  .setTitle(`Gists for ${username}`)\n  .setDescription(embedDescription)\n  .setURL(`https://www.github.com/users/${username}/gists`)\nmsg.channel.send(embed);\n</code></pre><p>Now when you send the <code>!gists</code> command in your server, you should see a list of gists that GitHub user has created.</p><figure><img src=\"/img/n/integrating-discord-bots-with-other-apis/28c3b256-e929-4c27-bf09-4bd458338b20-Untitled.png\" /><figcaption></figcaption></figure><p>My next article will go over how to use Discord webhooks to receive information from external services and display them in our server.</p></div>","excerpt":"A while back, there was a movement in the tech industry called Chat Ops. Essentially, it was the idea that chat appli...","cachedOn":1681394580},{"id":"41de3c65-3c4f-4ddc-a8cf-18dbda441a6a","notion_id":"41de3c65-3c4f-4ddc-a8cf-18dbda441a6a","status":"Published","slug":"building-a-survey-discord-bot","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-12T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-6-building-a-discord-survey-bot","seriesOrder":6,"youTubeURL":"https://youtu.be/7qL9Is4rDGU","title":"Building a Survey Discord Bot","html":"<div><p>Over the last two weeks, we’ve explored the various ways in which Discord bots can process messages. In this article, we’ll be putting all that together to create a bot that will gather input from a user to create a survey to gather feedback from other users. Strap in, this is gonna be a long one!</p><h2>Using Collectors to Build the Survey</h2><p>To start, lets add a <code>!survey</code> command to start. We wont require any additional arguments here, we’ll jump right into prompting the user for the following info;</p><ul><li>What the survey is answering.</li><li>How long it should run.</li><li>A series of potential answers.</li></ul><p>Add the following code at the end of your message event handler;</p><pre class=\"language-javascript\"><code>if (msg.content.startsWith('!survey')) {\n    // Create an empty 'survey' object to hold the fields for our survey\n    let survey = {};\n    // We're going to use this as an index for the reactions being used.\n    let reactions = [\n      \"1️⃣\",\n      \"2️⃣\",\n      \"3️⃣\",\n      \"4️⃣\"\n    ]\n    // Send a message to the channel to start gathering the required info\n    msg.channel\n      .send(\n        'Welcome to the !survey command. What would you like to ask the community?'\n      )\n      .then(() =&gt; {\n        // After each question, we setup a collector just like we did previously\n        let filter = (msg) =&gt; !msg.author.bot;\n        let options = {\n          max: 1,\n          time: 15000\n        };\n\n        return msg.channel.awaitMessages(filter, options);\n      })\n      .then((collected) =&gt; {\n        // Lets take the input from the user and store it in our 'survey' object\n        survey.question = collected.array()[0].content;\n        // Ask the next question\n        return msg.channel.send(\n          'Great! How long should it go? (specified in seconds)'\n        );\n      })\n      .then(() =&gt; {\n        let filter = (msg) =&gt; !msg.author.bot;\n        let options = {\n          max: 1,\n          time: 15000\n        };\n\n        return msg.channel.awaitMessages(filter, options);\n      })\n      .then((collected) =&gt; {\n        // Adding some checks here to ensure the user entered a number.\n        if (!isNaN(collected.array()[0].content)) {\n          survey.timeout = collected.array()[0].content;\n          // Ask the final question\n          return msg.channel.send(\n            'Excellent. Now enter up to four options, separated by commas.'\n          );\n        } else {\n          throw 'timeout_format_error';\n        }\n      })\n      .then(() =&gt; {\n        let filter = (msg) =&gt; !msg.author.bot;\n        let options = {\n          max: 1,\n          time: 15000\n        };\n\n        return msg.channel.awaitMessages(filter, options);\n      })\n      .then((collected) =&gt; {\n        // Split the answers by commas so we have an array to work with\n        survey.answers = collected.array()[0].content.split(',');\n        console.log(survey)\n      })\n  }\n</code></pre><p>You’ll see that after each message, we’re asking another another question and guiding the user down the path of setting up the information we need. To accomplish this, we can use the <code>awaitMessages</code> version of the collector which will return a <code>Promise</code> like we did in the previous article. This way we don’t need to nest all these into various event callbacks. It makes the code much cleaner and more understandable.</p><p>Here is the code in action;</p><figure><img src=\"/img/n/building-a-survey-discord-bot/95f9be07-76b7-4b2e-9533-7eaa98c571a5-Untitled.png\" /><figcaption></figcaption></figure><p>And the output in our terminal;</p><figure><img src=\"/img/n/building-a-survey-discord-bot/7e961104-ee88-46c3-9738-a148d47c7faa-Untitled.png\" /><figcaption></figcaption></figure><h2>Creating the Reaction Collector</h2><p>Now that we’ve gathered the requirements for the survey, we need a way to display it for the server members to vote on. To do this, I’m going to use an <strong>Embed</strong> which is a rich card that can be used with Discord bots. Embeds are visually different than messages and provide additional formatting options. Lets get rid of that <code>console.log()</code> line in the last <code>.then()</code> of the promise chain and update it so it looks like this;</p><pre class=\"language-javascript\"><code>.then((collected) =&gt; {\n  // Split the answers by commas so we have an array to work with\n  survey.answers = collected.array()[0].content.split(',');\n\n  let surveyDescription = \"\"\n  // Loop through the questions and create the 'description' for the embed\n  survey.answers.forEach((question, index) =&gt; {\n    surveyDescription += `${reactions[index]}: ${question}\\n`;\n  })\n\n  // Create the embed object and send it to the channel\n  let surveyEmbed = new Discord.MessageEmbed()\n    .setTitle(`Survey: ${survey.question}`)\n    .setDescription(surveyDescription)\n  return msg.channel.send(surveyEmbed)\n})\n</code></pre><p>Then add the following block to your promise chain;</p><pre class=\"language-javascript\"><code>.then(surveyEmbedMessage =&gt; {\n  // Create the initial reactions to embed for the members to see\n  for (var i = 0; i &lt; survey.answers.length; i++) {\n    surveyEmbedMessage.react(reactions[i])\n  }\n\n  // Set a filter to ONLY grab those reactions & discard the reactions from the bot\n  const filter = (reaction, user) =&gt; {\n    return reactions.includes(reaction.emoji.name) && !user.bot;\n  };\n\n  // Use the timeout from our survey\n  const options = {\n    time: survey.timeout * 1000\n  }\n\n  // Create the collector\n  return surveyEmbedMessage.awaitReactions(filter, options);\n})\n</code></pre><p>Lets test our survey command one more time and see what the output looks like;</p><figure><img src=\"/img/n/building-a-survey-discord-bot/039ad859-65ba-4e44-a3c6-fc8c04129847-Untitled.png\" /><figcaption></figcaption></figure><p>This an example of an embed. See how it sticks out from a standard message? We’ll get more into embeds when we explore webhooks in Discord next week.</p><h2>Displaying the Results</h2><p>Finally, we want to show the results of our survey. To do that, we’re basically going to get the collected reactions, reduce them down into another embed, and send it to the server to show the total vote count and the number of votes on each option. Add this final <code>then()</code> to the promise chain;</p><pre class=\"language-javascript\"><code> .then(collected =&gt; {\n    // Convert the collection to an array\n    let collectedArray = collected.array()\n    // Map the collection down to ONLY get the emoji names of the reactions\n    let collectedReactions = collectedArray.map(item =&gt; item._emoji.name)\n    let reactionCounts = {}\n\n    // Loop through the reactions and build an object that contains the counts for each reaction\n    // It will look something like this:\n    // {\n    //   1️⃣: 1\n    //   2️⃣: 0\n    //   3️⃣: 3\n    //   4️⃣: 10\n    // }\n    collectedReactions.forEach(reaction =&gt; {\n      if (reactionCounts[reaction]) {\n        reactionCounts[reaction]++\n      } else {\n        reactionCounts[reaction] = 1\n      }\n    })\n\n    // Using those results, rebuild the description from earlier with the vote counts\n    let surveyResults = \"\"\n    survey.answers.forEach((question, index) =&gt; {\n      let voteCount = 0\n      if (reactionCounts[reactions[index]]) {\n        voteCount = reactionCounts[reactions[index]]\n      }\n      let voteCountContent = `(${voteCount} vote${voteCount !== 1 ? 's' : ''})`\n      surveyResults += `${reactions[index]}: ${question} ${voteCountContent}\\n`;\n    })\n\n    // Create the embed and send it to the channel\n    let surveyResultsEmbed = new Discord.MessageEmbed()\n      .setTitle(`Results for '${survey.question}' (${collectedArray.length} total votes)`)\n      .setDescription(surveyResults)\n\n    msg.channel.send(surveyResultsEmbed);\n  })\n</code></pre><p>And here is what the results look like;</p><figure><img src=\"/img/n/building-a-survey-discord-bot/9096f983-45ea-43a8-b9cc-191b2b246f30-Untitled.png\" /><figcaption></figcaption></figure><p>Until now, most of our activity has been within Discord itself. Next week, we’ll explore how to interact with systems outside of Discord for some really cool integrations!</p></div>","excerpt":"Over the last two weeks, we’ve explored the various ways in which Discord bots can process messages. In this article,...","cachedOn":1681394587},{"id":"858ca735-9a23-4557-8f59-48412174002d","notion_id":"858ca735-9a23-4557-8f59-48412174002d","status":"Published","slug":"discord-bot-reaction-collectors","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-10T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-5-discord-bot-reaction-collectors","seriesOrder":5,"youTubeURL":"https://youtu.be/ISsgd5Jjl_k","title":"Discord Bot Reaction Collectors","html":"<div><p>In our last article, we explored how we can use Collectors to prompt a user for additional input. Reaction Collectors work in a similar way, but collect Reactions or Emojis to a particular message. This technique is used in a lot of bots where input might need to be gathered from multiple people, such as a voting system.</p><h2>Creating a Reaction Collector</h2><p>In this example, we’re going to add a basic reaction collector to respond to two possible reactions: 👍 and 👎. Depending on the reaction, our bot will respond in different ways. Add the following code to your message handler;</p><pre class=\"language-javascript\"><code>if (msg.content.startsWith('!react')) {\n    // Use a promise to wait for the question to reach Discord first\n    msg.channel.send('Which emoji do you prefer?').then((question) =&gt; {\n      // Have our bot guide the user by reacting with the correct reactions\n      question.react('👍');\n      question.react('👎');\n\n      // Set a filter to ONLY grab those reactions & discard the reactions from the bot\n      const filter = (reaction, user) =&gt; {\n        return ['👍', '👎'].includes(reaction.emoji.name) && !user.bot;\n      };\n\n      // Create the collector\n      const collector = question.createReactionCollector(filter, {\n        max: 1,\n        time: 15000\n      });\n\n      collector.on('end', (collected, reason) =&gt; {\n        if (reason === 'time') {\n          msg.reply('Ran out of time ☹...');\n        } else {\n          // Grab the first reaction in the array\n          let userReaction = collected.array()[0];\n          // Grab the name of the reaction (which is the emoji itself)\n          let emoji = userReaction._emoji.name;\n\n          // Handle accordingly\n          if (emoji === '👍') {\n            msg.reply('Glad your reaction is 👍!');\n          } else if (emoji === '👎') {\n            msg.reply('Sorry your reaction is 👎');\n          } else {\n            // This should be filtered out, but handle it just in case\n            msg.reply(`I dont understand ${emoji}...`);\n          }\n        }\n      });\n    });\n  }\n</code></pre><p>There are a few things you’ll notice about this code. The first is that instead of replying directly to the user, we’re sending messages directly into the channel. Sending a message to the channel actually returns a <code>Promise</code> so we can chain another function with the output of the promise, which contains info about the message we just sent.</p><p>Once the promise returns, we can take that message and react to it directly. Then we create a reaction collector with <code>createReactionCollector</code> with a filter and options just like we did for the standard collector. We handle the <code>end</code> event pretty much like we did with the standard collector as well. Here are the results;</p><figure><img src=\"/img/n/discord-bot-reaction-collectors/9df2bbe4-5ce7-46f9-b62b-9053c9fc2892-Untitled.png\" /><figcaption></figcaption></figure><p>In the next article, we’ll put everything we’ve learned so far into creating a command that allows users to create surveys!</p></div>","excerpt":"In our last article, we explored how we can use Collectors to prompt a user for additional input. Reaction Collectors...","cachedOn":1681394588},{"id":"ab5146db-e37c-4dbc-acfe-a9cac6357454","notion_id":"ab5146db-e37c-4dbc-acfe-a9cac6357454","status":"Published","slug":"prompting-for-input-with-discord-bot-collectors","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-08T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-4-discord-bot-collectors","seriesOrder":4,"youTubeURL":"https://youtu.be/ISsgd5Jjl_k","title":"Prompting for Input with Discord Bot Collectors","html":"<div><p>Say you want to setup a command to reach out to an external service. Sure you can use arguments to gather multiple pieces of info, but if the user doesn’t properly put in all the required info, how can you make sure they do? That’s where Collectors come in. Collectors are a way that you can setup your bot to gather additional input from a user in Discord.</p><p>Whenever a command is triggered that creates a new collector, the collector will wait for additional messages and “collect” them for processing once its done. After the collector has ended, you can do what you need to with the necessary messages.</p><h2>Creating a Collector</h2><p>To get started, we need to setup yet another command prefix so our bot knows to prompt the user. Lets go with <code>!collector</code>. Add the following code into your “message” event handler;</p><pre class=\"language-javascript\"><code>if (msg.content.startsWith('!collector')) {\n  // Filters define what kinds of messages should be collected\n  let filter = (msg) =&gt; !msg.author.bot;\n  // Options define how long the collector should remain open\n  //    or the max number of messages it will collect\n  let options = {\n    max: 2,\n    time: 15000\n  };\n  let collector = msg.channel.createMessageCollector(filter, options);\n\n  // The 'collect' event will fire whenever the collector receives input\n  collector.on('collect', (m) =&gt; {\n    console.log(`Collected ${m.content}`);\n  });\n\n  // The 'end' event will fire when the collector is finished.\n  collector.on('end', (collected) =&gt; {\n    console.log(`Collected ${collected.size} items`);\n  });\n\n  msg.reply('What is your favorite color?');\n}\n</code></pre><p>The <code>createMessageCollector()</code> function accepts two parameters; a filter and the collector options.</p><p>Filters are a way to inform the collector what kinds of messages it should take in. You can create a filter around almost anything. The filter should return either true or false, depending on what your requirements are.</p><p>One thing worth noting about collectors is they are outside the standard flow of the way our bot receives messages. Meaning that it <em>will</em> collect messages from bots, so we used our filter to prevent it from collecting responses even from our bot.</p><p>The collector options tell the collector how to behave. In the options, you’ll generally specify either how long the collector should remain open (<code>time</code>) or how many messages it should take in before ending (<code>max</code>).</p><p>Here’s how the code behaves in our server.</p><figure><img src=\"/img/n/prompting-for-input-with-discord-bot-collectors/05e3bf92-4d87-4fd6-a895-48a60b66a3f9-Untitled.png\" /><figcaption></figcaption></figure><p>And you can see it will log out the collected message in the terminal, as well as the number of collected messages when the “end” event is fired.</p><figure><img src=\"/img/n/prompting-for-input-with-discord-bot-collectors/0beb3881-999c-43a6-83bc-4a0d80a60501-Untitled.png\" /><figcaption></figcaption></figure><h2>Arguments with Collectors</h2><p>You see how Collectors can be very powerful in helping our bot guide a user through how to use a particular command. We’re going to expand upon this a bit to show how you can use Arguments and Collectors together.</p><p>Lets say we’re working on a bot that requires 2 pieces of input per command. What happens if the user only enters one? This is where we can add a collector to prompt a user for the additional input required.</p><p>Add the following code to your message event handler.</p><pre class=\"language-javascript\"><code>if (msg.content.startsWith('!gimme')) {\n  // Split the arguments\n  const args = msg.content.split(' ');\n\n  // Check the first argument (skipping the command itself)\n  if (args[1] === 'smiley') {\n    if (args.length &lt; 3) {\n      // Filter out any bot messages\n      let filter = (msg) =&gt; !msg.author.bot;\n      // Set our options to expect 1 message, and timeout after 15 seconds\n      let options = {\n        max: 1,\n        time: 15000\n      };\n      let collector = msg.channel.createMessageCollector(filter, options);\n\n      collector.on('end', (collected, reason) =&gt; {\n        // If the collector ends for 'time', display a message to the user\n        if (reason === 'time') {\n          msg.reply('Ran out of time ☹...');\n        } else {\n          // Convert the collection to an array and check the content of the message.\n          //   Repsond accordingly\n          switch (collected.array()[0].content) {\n            case 'happy':\n              msg.reply('😀');\n              break;\n            case 'sad':\n              msg.reply('😢');\n              break;\n            default:\n              msg.reply('I dont know that smiley...');\n              break;\n          }\n        }\n      });\n\n      msg.reply('What kind of smiley do you like? (happy or sad)');\n    } else {\n      // If all arguments are already there, respond with the requested item\n      switch (args[2]) {\n        case 'happy':\n          msg.reply('😀');\n          break;\n        case 'sad':\n          msg.reply('😢');\n          break;\n        default:\n          msg.reply('I dont know that smiley...');\n          break;\n      }\n    }\n  }\n\n  if (args[1] === 'circle') {\n    if (args.length &lt; 3) {\n      let filter = (msg) =&gt; !msg.author.bot;\n      let options = {\n        max: 1,\n        time: 15000\n      };\n      let collector = msg.channel.createMessageCollector(filter, options);\n\n      collector.on('end', (collected, reason) =&gt; {\n        if (reason === 'time') {\n          msg.reply('Ran out of time ☹...');\n        } else {\n          switch (collected.array()[0].content) {\n            case 'red':\n              msg.reply('🔴');\n              break;\n            case 'blue':\n              msg.reply('🔵');\n              break;\n            default:\n              msg.reply('I dont know that color...');\n              break;\n          }\n        }\n      });\n\n      msg.reply('What color circle would you like? (blue or red)');\n    } else {\n      switch (args[2]) {\n        case 'red':\n          msg.reply('🔴');\n          break;\n        case 'blue':\n          msg.reply('🔵');\n          break;\n        default:\n          msg.reply('I dont know that color...');\n          break;\n      }\n    }\n  }\n}\n</code></pre><p>Each section is commented to explain what exactly it does. Essentially, the <code>!gimme</code> command does the following though;</p><ul><li>The user has two different paths the code can follow, either a smiley or a circle.</li><li>For each path, there are two options available.</li><li>If the user enters both pieces of input, the bot will respond accordingly. If not, the bot will prompt the user to add the second piece.</li><li>Finally, we’ve also added some additional code to inform the user in case the collector times out.</li></ul><p>Let’s see how it behaves now;</p><figure><img src=\"/img/n/prompting-for-input-with-discord-bot-collectors/413a1fc2-408d-4552-a2ba-612365fa9bf8-Untitled.png\" /><figcaption></figcaption></figure><p>Our next article will outline how to use reactions and collectors together.</p><p></p></div>","excerpt":"Say you want to setup a command to reach out to an external service. Sure you can use arguments to gather multiple pi...","cachedOn":1681394590},{"id":"b252aa39-9b6a-42fe-a52c-bb19a04e9089","notion_id":"b252aa39-9b6a-42fe-a52c-bb19a04e9089","status":"Published","slug":"streamline-your-bot-dev-environment","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-05T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-3-streamline-bot-dev-env","seriesOrder":3,"youTubeURL":"https://youtu.be/VgWloMi6OHY","title":"Streamline Your Bot Dev Environment","html":"<div><p>In this article, we’re going to tweak our environment to make development a little easier and setup our project for hosting later on.</p><h2>Setting up Nodemon</h2><p>To prevent us from having to restart our bot for every little change we make, we’re going to install <strong>Nodemon</strong> and configure our project to use it. Nodemon is a great tool for local development. It monitors your files & folders for changes, and will automatically restart your project if it detects any changes. This can save quite a bit of time by automating that process.</p><p>To install Nodemon, open your terminal and run the command <code>npm install -g nodemon</code> and wait for it to finish installing. Once done, open up your <strong>package.json</strong> file and modify (or add) the <code>start</code> script like below;</p><pre class=\"language-json\"><code>\"scripts\": {\n  \"start\": \"nodemon index.js\",\n  \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n},</code></pre><p>Now going forward, we can issue the command <code>npm start</code> and Nodemon will start monitoring our project for changes.</p><h2>Setting up Environment Configs</h2><p>The next thing we’re going to do is setup <strong>dotenv</strong> which is a package that allows you to parse the <strong>.env</strong> (Environment) configuration files. This file will contain sensitive information that we don’t necessarily want to commit to our repository. And since the project will already expect these secure values to be part of the environment config, whatever service you decide to host your bot on will support these values.</p><p>To install the package, open your terminal and enter <code>npm install dotenv</code>. This will install the package into your repository. Next create a file in the root of the project and name it <strong>.env</strong>. This is the default file that the package will import when it starts.</p><p>Now we need to make some changes to <strong>index.js</strong>. The first thing we need to do is require & configure the package we just installed. To do this, add this line to the first line in <strong>index.js</strong>;</p><pre class=\"language-javascript\"><code>require(‘dotenv’).config()</code></pre><p>And finally, we want to move our Bot’s token into the <strong>.env</strong> file and replace it with <code>process.env.DISCORD_BOT_TOKEN</code> like so.</p><pre class=\"language-plain text\"><code>DISCORD_BOT_TOKEN = \"your_token_here\"</code></pre><p>Here is the full <strong>index.js</strong> up til this point;</p><pre class=\"language-javascript\"><code>require('dotenv').config();\nconst Discord = require('discord.js')\nconst client = new Discord.Client();\n\nclient.on('ready', () =&gt; {\n  console.log(`Logged in as ${client.user.tag}!`);\n})\n\nclient.on('message', async msg =&gt; {\n  if(msg.author.bot) {\n    return\n  }\n\n  if(msg.content.startsWith(\"!hello\")) {\n    msg.reply(\"world!\")\n  }\n\n  if(msg.content.startsWith(\"!dm\")) {\n    let messageContent = msg.content.replace(\"!dm\", \"\")\n    msg.member.send(messageContent)\n  }\n\n  if(msg.content.startsWith(\"!args\")) {\n    const args = msg.content.split(' ')\n    let messageContent = \"\"\n    if(args.includes(\"foo\")) {\n      messageContent += \"bar \"\n    }\n    if(args.includes(\"bar\")) {\n      messageContent += \"baz \"\n    }\n    if(args.includes(\"baz\")) {\n      messageContent += \"foo \"\n    }\n    msg.reply(messageContent)\n  }\n\n})\n\nclient.login(process.env.DISCORD_BOT_TOKEN);\n</code></pre><p>Now to ensure our token never makes it into our repository, add <code>.env</code> to your <strong>.gitignore</strong> file. This will prevent the config from ever getting committed and keep your commits secure.</p></div>","excerpt":"In this article, we’re going to tweak our environment to make development a little easier and setup our project for h...","cachedOn":1681394591},{"id":"51c24af7-a0fc-4e76-a4ad-598c9421857d","notion_id":"51c24af7-a0fc-4e76-a4ad-598c9421857d","status":"Published","slug":"ways-a-discord-bot-can-respond-to-commands","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-03T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-2-responding-to-commands","seriesOrder":2,"youTubeURL":"https://youtu.be/VgWloMi6OHY","title":"Ways a Discord Bot Can Respond to Commands","html":"<div><p>Now that you have a basic bot built, we’ll want to explore the different ways you can respond using the bot.</p><h2>Direct Messages</h2><p>Bots will parse any message they have access to, this actually includes direct messages to the bot itself. If you want to setup direct messages, you’ll actually need the bot to message you first. In this example, we’ll use the command <code>!dm</code> to get the bot to message you to start. Add the following code into your message handler.</p><pre class=\"language-javascript\"><code>if(msg.content.startsWith(\"!dm\")) {\n  let messageContent = msg.content.replace(\"!dm\", \"\")\n  msg.member.send(messageContent)\n}\n</code></pre><p>Your <strong>index.js</strong> should look like this now;</p><pre class=\"language-javascript\"><code>// Import discord.js and create the client\nconst Discord = require('discord.js')\nconst client = new Discord.Client();\n\n// Register an event so that when the bot is ready, it will log a messsage to the terminal\nclient.on('ready', () =&gt; {\n  console.log(`Logged in as ${client.user.tag}!`);\n})\n\n// Register an event to handle incoming messages\nclient.on('message', async msg =&gt; {\n  // This block will prevent the bot from responding to itself and other bots\n  if(msg.author.bot) {\n    return\n  }\n\n  // Check if the message starts with '!hello' and respond with 'world!' if it does.\n  if(msg.content.startsWith(\"!hello\")) {\n    msg.reply(\"world!\")\n  }\n\n  if(msg.content.startsWith(\"!dm\")) {\n    let messageContent = msg.content.replace(\"!dm\", \"\")\n    msg.member.send(messageContent)\n  }\n\n})\n\n// client.login logs the bot in and sets it up for use. You'll enter your token here.\nclient.login('your_token_here');\n</code></pre><p>Start your bot if it isn’t already (or restart it), and issue the <code>!dm</code> command we just added. You should receive a direct message from the bot.</p><figure><img src=\"/img/n/ways-a-discord-bot-can-respond-to-commands/0891fa42-23d6-4192-b3b4-554177037cf1-Untitled.png\" /><figcaption></figcaption></figure><p>Any command we’ve setup the bot to handle so far can now be issued using the new chat. If you’re coming from my last post, you can issue the <code>!hello</code> command and it should respond in the chat.</p><figure><img src=\"/img/n/ways-a-discord-bot-can-respond-to-commands/b46dc76f-52c8-4831-891e-aff8a1645614-Untitled.png\" /><figcaption></figcaption></figure><h2>Processing Arguments</h2><p>So far, we’ve been using very basic commands to get our bot to respond, but you can also parse out the entire content of the message sent and handle it differently based on the full input. Let’s add some additional code to parse the message to respond differently, even if the command prefix is the same.</p><pre class=\"language-javascript\"><code>if (msg.content.startsWith(\"!args\")) {\n  const args = msg.content.split(' ')\n  let messageContent = \"\"\n  if (args.includes(\"foo\")) {\n    messageContent += \"bar \"\n  }\n  if (args.includes(\"bar\")) {\n    messageContent += \"baz \"\n  }\n  if (args.includes(\"baz\")) {\n    messageContent += \"foo \"\n  }\n  msg.reply(messageContent)\n}\n</code></pre><p>Now if we issue the <code>!args</code> command with a different message to follow, our bot will respond in different ways.</p><figure><img src=\"/img/n/ways-a-discord-bot-can-respond-to-commands/529307da-43ad-4f28-bd0d-7e77cb8895f1-Untitled.png\" /><figcaption></figcaption></figure><p>This is how many bots actually work. The bots are coded to use a common prefix (I.e.; <code>!args</code>) and perform different actions based on the content of the message they receive.</p><p>If you are enjoying this content, please feel free to stop by my Discord (<a href=\"http://fullstack.chat\" target=\"_blank\">fullstack.chat</a>) to hang out with other developers, including me!</p><p></p></div>","excerpt":"Now that you have a basic bot built, we’ll want to explore the different ways you can respond using the bot. Bots wil...","cachedOn":1681394593},{"id":"5436bbe4-d31f-454b-a45e-a6df4036cf88","notion_id":"5436bbe4-d31f-454b-a45e-a6df4036cf88","status":"Published","slug":"building-a-hello-world-discord-bot","relation_series":["6ad92aef-a29e-4781-94b5-8a473268826a"],"publishOn":"2020-06-01T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/2020-06-1-hello-world-discord-bot","seriesOrder":1,"youTubeURL":"https://youtu.be/VgWloMi6OHY","title":"Building a Hello World Discord Bot","html":"<div><p>Welcome to June’s series: Building Discord Bots. In this article I’ll outline how to get your very own Discord bot setup and responding to messages.</p><p>Discord Bots are used every day by thousands of servers all over the world. They are used for all sorts of things such as creating registrations forms, integrating with third party APIs, and working with voice. In this series, I’ll be covering some of the basic things you’ll need to know to create your own Discord bot to do whatever you need!</p><h2>Register with Discord</h2><p>The first thing you need to do is register your bot with Discord. Head over to their developer portal (<a href=\"https://discordapp.com/developers\" target=\"_blank\">https://discordapp.com/developers</a>) and create a new application by clicking <strong>New Application</strong> in the upper right and give your application a name.</p><figure><img src=\"/img/n/building-a-hello-world-discord-bot/34a506fc-1127-49a3-9d5e-d101bc1efecb-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/building-a-hello-world-discord-bot/8bca78fb-8acd-4766-9e4b-8fe78e18bae8-Untitled.png\" /><figcaption></figcaption></figure><p>Once you do that, add the bot capabilities to your application by going to the <strong>Bot</strong> tab and clicking <strong>Add Bot</strong>. Copy the token and save it for later. Then head over to the OAuth2 tab, check the Bot box under Scopes, and the Administrator box under Bot Permissions. Then copy the link and put it into your browser to add the bot to your server.</p><figure><img src=\"/img/n/building-a-hello-world-discord-bot/4b8642a9-743f-4199-a953-d141a54e0b2c-Untitled.png\" /><figcaption></figcaption></figure><p>A few notes:</p><ul><li>We’re setting the bot up as an Administrator for this tutorial only. You should only add the permissions your bot actually needs to operate within a server.</li><li>Also, you can only register a bot with servers that you administer. Luckily, Servers are free, so create one if you don’t already own one.</li></ul><h2>Setting Up Your Environment</h2><p>My preferred IDE is VSCode so I’ll be using that in this series but you can use whatever IDE you prefer. First create a new folder on your computer and initialize it with <code>npm init -y</code> which will accept all the defaults. Then install the <code>discord.js</code> library with <code>npm install discord.js</code>. Create <code>index.js</code> and add the following code to the file. I’ve commented each block to explain what exactly it does.</p><pre class=\"language-javascript\"><code>    // Import discord.js and create the client\n    const Discord = require('discord.js')\n    const client = new Discord.Client();\n\n    // Register an event so that when the bot is ready, it will log a messsage to the terminal\n    client.on('ready', () =&gt; {\n      console.log(`Logged in as ${client.user.tag}!`);\n    })\n\n    // Register an event to handle incoming messages\n    client.on('message', async msg =&gt; {\n\n      // Check if the message starts with '!hello' and respond with 'world!' if it does.\n      if(msg.content.startsWith(\"!hello\")) {\n        msg.reply(\"world!\")\n      }\n    })\n\n    // client.login logs the bot in and sets it up for use. You'll enter your token here.\n    client.login('your_token_here');\n</code></pre><h2>Testing your Bot</h2><p>Now we’ll want to test to make sure our bot is running and responding to applications. The first indicator that things are working is in your terminal, you should see a message that the bot is running & signed in. You can also look on the sidebar of Discord to see that the bot is Online. And finally, drop a message into the general channel with `!hello` and your bot should respond.</p><figure><img src=\"/img/n/building-a-hello-world-discord-bot/000bdc06-18b5-472d-81aa-d7293543ccbe-Untitled.png\" /><figcaption></figcaption></figure><h2>A Warning about How Bots Work</h2><p>Bots will monitor all channels of your server they have permissions to monitor. This powerful but can cause some issues. This is why we’re going to want to prevent our bot from responding to other bots (including itself), so add the following line to the beginning of the message handler. This will prevent this exact scenario;</p><pre class=\"language-javascript\"><code>    // Import discord.js and create the client\n    const Discord = require('discord.js')\n    const client = new Discord.Client();\n\n    // Register an event so that when the bot is ready, it will log a messsage to the terminal\n    client.on('ready', () =&gt; {\n      console.log(`Logged in as ${client.user.tag}!`);\n    })\n\n    // Register an event to handle incoming messages\n    client.on('message', async msg =&gt; {\n      // This block will prevent the bot from responding to itself and other bots\n      if(msg.author.bot) {\n        return\n      }\n\n      // Check if the message starts with '!hello' and respond with 'world!' if it does.\n      if(msg.content.startsWith(\"!hello\")) {\n        msg.reply(\"world!\")\n      }\n    })\n\n    // client.login logs the bot in and sets it up for use. You'll enter your token here.\n    client.login('your_token_here');\n</code></pre><p>Congratulations! You now have your every own Discord bot. I’ll be expanding on this bot over the next few weeks.</p></div>","excerpt":"Welcome to June’s series: Building Discord Bots. In this article I’ll outline how to get your very own Discord bot se...","cachedOn":1681394595},{"id":"319c5c45-bb94-4823-9e5f-27d28a17cef4","notion_id":"319c5c45-bb94-4823-9e5f-27d28a17cef4","status":"Published","slug":"reviving-enviari-on-mixer","relation_series":[],"publishOn":"2020-04-30T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Reviving Enviari On Mixer","html":"<div><p>Enviari is the evolution of a program I started building several years ago when my wife owned a small business where she was shopping lots of little products out of our house. Over the last few weeks, I’ve started exploring the idea of kicking it off with a fresh start, building it on Azure, with the goal of bringing it to the market in the coming months. This post will detail the core features of the app, as well as touch on the history of it.</p><h2>Goals</h2><p>So the main idea of Enviari is to provide a seamless, easy way for small businesses & solopreneurs to be able to ship their product all over the country (potentially the world) at a fraction of the cost of standard postage. It will start by providing USPS as the primary carrier, and synchronize customers in from the Square platform. Users will also be able to save shipping profiles & default preferences to make things a bit easier. I also plan to build in a system by which Dymo label printers will be able to generate physical labors when the shipment is created.</p><p>The app will be built on Azure and integrate directly into some of its core features. Here is what I plan to use so far;</p><ul><li>Azure Functions</li><li>Service Bus</li><li>Cosmos DB</li><li>Azure B2C</li><li>Azure Storage/CDN</li></ul><p>More services may come, but these are the ones I know for certain I will need to use.</p><h2>So Why Azure?</h2><p>I originally targeted AWS when I started the web version of this app last year, but decided to switch focus to using Azure primarily because that’s what I use at work and I can learn cool new things that I can transfer both ways.</p><h2>App History</h2><p>The app actually used to be called QuickerShipper, and it started as a basic PowerShell script that I would run manually when my wife need to ship something. That evolved into a Windows-based WPF application that I built out fully and started selling on my website. My wife actually still uses it to this day!</p><p>The issue came when I tried to sell out to other people in her little community. Turns out most people wanted to use it on their iPhones and Macs, which is what triggered me to start learning web frameworks & cloud tech so I could make it a universal application using JavaScript.</p><p>So when I started live streaming last year, that’s when I started building this thing on my Twitch channel, and at that point it was already the third version of the application.</p><p>All of those sessions are actually live on my YouTube channel <a href=\"https://www.youtube.com/channel/UCLx9EihBDfoJMncRWSZZoXg\" target=\"_blank\">here</a>.</p><h2>So Why Mixer?</h2><p>I’m honestly not tied to any one streaming platform. I chose Twitch because it was (and still is) the market leader. But recently I’ve been interested in switching just to explore the other platforms and see what they have to offer. Someone in my Discord suggested Mixer, so I decided why the hell not.</p><p>As of now, I’ll be streaming these sessions every Thursday at 8:30pm Central. Feel free to check it out here: <a href=\"https://mixer.com/brianmmdev\" target=\"_blank\">mixer.com/brianmmdev</a></p><p>As always, happy coding! 😁</p><p></p></div>","excerpt":"Enviari is the evolution of a program I started building several years ago when my wife owned a small business where ...","cachedOn":1681394597},{"id":"98d5c867-4d1f-45d0-8d3b-8b919f5a339b","notion_id":"98d5c867-4d1f-45d0-8d3b-8b919f5a339b","status":"Published","slug":"embracing-imposter-syndrome","relation_series":[],"publishOn":"2020-04-16T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Embracing Imposter Syndrome","html":"<div><p>If you’ve been writing code for any length of time, you’re probably not unfamiliar with Imposter Syndrome. Wikipedia defines Imposter Syndrome as;</p><blockquote>**…**a psychological pattern in which one doubts one’s accomplishments and has a persistent internalized fear of being exposed as a “fraud”.</blockquote><p>A big part of why most developers (especially newbies) experience this is because our industry is uniquely deep. In the sense that the more you learn, the more you realize you don’t know, and the more you fear that you should know it and that others will view you as somewhat inferior to them. Most folks are terrified by this. I’m going to present a different twist and encourage you to embrace it.</p><h3>Imposter Syndrome as a Good Thing</h3><figure><img src=\"/img/n/embracing-imposter-syndrome/60f31bc6-1110-4fdf-ba7b-f9e83a32dcf8-image.png\" /><figcaption></figcaption></figure><p>It might sound crazy, but hear me out. If you are worrying about not knowing enough, that means that you care enough to want to grow. Whether that’s to impress others or yourself. Desired to grow in any positive area of your life is never a bad thing! I’d encourage you to acknowledge that the reason you’re scared at all is because you KNOW there is something you don’t fully understand.</p><p>Learn to acknowledge these cues and do something about it. Try to identify what exactly it is about a specific topic that’s causing it, and do what you need to do in order to understand the topic fully. Then try and engage in intelligent conversations with others in the field. If you’re worried about speaking with the ones you work with, there is no shortage of people on the internet who would be willing to chat about these things.</p><p>Side note: I host a Discord community & we would love to chat with you about any and all things coding 😉. – <a href=\"http://fullstack.chat\" target=\"_blank\">fullstack.chat</a> </p><p>Putting this concept into practice will do wonders for both your knowledge and confidence as a developer.</p><p>And as always, happy coding! 😁 💻</p><p></p></div>","excerpt":"If you’ve been writing code for any length of time, you’re probably not unfamiliar with Imposter Syndrome. Wikipedia ...","cachedOn":1681394598},{"id":"7cd29757-4c1e-4cf8-a962-0bc6e3f33d99","notion_id":"7cd29757-4c1e-4cf8-a962-0bc6e3f33d99","status":"Published","slug":"nodist-running-multiple-version-of-nodejs","relation_series":[],"publishOn":"2020-04-11T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Nodist: Running multiple version of NodeJS on Windows","html":"<div><p>Ever work on different projects that depend on different versions of Node? I recently ran into an issue where I had one project that depended on Node 10, and another (this website actually) depend on Node 12. Put me in an interesting situation. I wasn’t about to go and uninstall/reinstall versions of Node every time I switched projects. So I did a little digging and discovered a project called Nodist.</p><p>Nodist allows you to quickly swap out the active version of Node on your box. Here are some of the commands I personally use. To get started, head over to the releases page of the Nodist project <a href=\"https://github.com/nullivex/nodist/releases\" target=\"_blank\">here</a> and install the latest version.</p><h2>Setting your Active Node Version</h2><p>In Nodist, there is the concept of the global version & the local version. The global version is the node version that’s used by default. I set mine to the latest (12 at the time of this writing) by issuing the following command in my terminal;</p><pre class=\"language-bash\"><code>nodist global 12</code></pre><p>One of the neat things about Nodist is you don’t need to install a version manually. When you set the version, Nodist will automatically download and install the latest dot release of Node for that major release. You can also set the entire version if you want;</p><pre class=\"language-bash\"><code>nodist 12.16.2\n</code></pre><p>If you’re working on a project that requires a different version, you can set it on the directory level using;</p><pre class=\"language-bash\"><code>nodist local 10\n</code></pre><p>Now whenever you issue Node & NPM commands in that directory, it will use the version specified instead of the global version.</p><h2>Setting the NPM Version</h2><p>If you want to use different NPM versions, you can also use Nodist to set that as needed;</p><pre class=\"language-bash\"><code>nodist npm global 12\nnodist npm local 10</code></pre><p>You can also set NPM to whatever version matches the version of Node you have set;</p><pre class=\"language-bash\"><code>nodist npm global match</code></pre><p>Using Nodist makes swapping out versions of Node & NPM much easier than trying to tackle it manually. Head over to the repo to view the full documentation <a href=\"https://github.com/nullivex/nodist\" target=\"_blank\">here</a>.</p></div>","excerpt":"Ever work on different projects that depend on different versions of Node? I recently ran into an issue where I had o...","cachedOn":1681394600},{"id":"82dd8ba0-41d3-425a-aba7-6bee23a1f91d","notion_id":"82dd8ba0-41d3-425a-aba7-6bee23a1f91d","status":"Published","slug":"cloud-challenge-build-a-pipeline-with-azure-devops","relation_series":[],"publishOn":"2020-03-24T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Cloud Challenge: Build a Pipeline with Azure DevOps","html":"<div><p>This challenge will be actively discussed on the Discord (<a href=\"http://fullstack.chat\" target=\"_blank\">fullstack.chat</a>). Participants are encouraged to collaborate with others on the Discord as well as publish their GitHub repositories ether in Discord or in the comments. Happy coding!</p><blockquote>📅 Active Dates: 3/23 – 3/27/2020</blockquote><h2>Objective</h2><ol><li>In the framework of your choice, create a basic front end application and build it. You can use the artifacts from the previous challenge.</li><li>Connect your GitHub repo up to a project in Azure DevOps.</li><li>Create a Release Pipeline and Deploy your application to Azure CDN.</li></ol><h2>Tips & Approaches</h2><p>Here are some resources to get started;</p><ul><li>What is Azure Pipelines?: <a href=\"https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines</a></li><li>Create your first pipeline: <a href=\"https://docs.microsoft.com/en-us/azure/devops/pipelines/create-first-pipeline\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/devops/pipelines/create-first-pipeline</a></li><li>Pipeline run sequence: <a href=\"https://docs.microsoft.com/en-us/azure/devops/pipelines/process/runs\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/devops/pipelines/process/runs</a></li></div>","excerpt":"This challenge will be actively discussed on the Discord ( fullstack.chat ). Participants are encouraged to collabora...","cachedOn":1681394603},{"id":"c57514ec-2c9d-4eb4-824c-562341290873","notion_id":"c57514ec-2c9d-4eb4-824c-562341290873","status":"Published","slug":"cloud-challenge-deploy-a-frontend-app-to-azure-cdn","relation_series":[],"publishOn":"2020-03-15T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Cloud Challenge: Deploy a Frontend App to Azure CDN","html":"<div><p>This challenge will be actively discussed on the Discord (<a href=\"http://fullstack.chat\" target=\"_blank\">fullstack.chat</a>). Participants are encouraged to collaborate with others on the Discord as well as publish their GitHub repositories ether in Discord or in the comments. Happy coding!</p><blockquote>📅 Active Dates: 3/15 – 3/19/2020</blockquote><h2>Objective</h2><ol><li>In the framework of your choice, create a basic front end application and build it.</li><li>Upload the artifacts to an Azure Storage Account with the public website feature setup.</li><li>Configure Azure CDN with your Storage Account site as the origin.</li></ol><h2>Tips & Approaches</h2><p>Here are some resources to get started;</p><ul><li>Create an Azure Storage Account: <a href=\"https://docs.microsoft.com/en-us/azure/storage/common/storage-account-create\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/storage/common/storage-account-create</a></li><li>Static Website Hosting with Azure Storage Account: <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website</a></li><li>Integrate an Azure storage account with Azure CDN: <a href=\"https://docs.microsoft.com/en-us/azure/cdn/cdn-create-a-storage-account-with-cdn\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/cdn/cdn-create-a-storage-account-with-cdn</a></li></ul><p></p></div>","excerpt":"This challenge will be actively discussed on the Discord ( fullstack.chat ). Participants are encouraged to collabora...","cachedOn":1681394614},{"id":"1b8e371c-4f71-46d6-abc1-d567a8567444","notion_id":"1b8e371c-4f71-46d6-abc1-d567a8567444","status":"Published","slug":"cloud-challenge-deploy-a-docker-container-on-azure-aks","relation_series":[],"publishOn":"2020-03-09T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Cloud Challenge: Deploy a Docker Container on Azure AKS","html":"<div><p>This challenge will be actively discussed on the Discord (<a href=\"https://brianmm.chat/\" target=\"_blank\">brianmm.chat</a>). Participants are encouraged to collaborate with others on the Discord as well as publish their GitHub repositories ether in Discord or in the comments. Happy coding!</p><blockquote>📅 Active Dates: 3/9 – 3/13/2020</blockquote><h2>Objective</h2><ol><li>In the language of your choice, build a small API that returns some data.</li><li>Then take that API and create a DOCKERFILE that will build a container.</li><li>Publish that container to the docker registry of your choice.</li><li>Deploy that container to AKS.</li><li>Test with Postman.</li></ol><h2>Tips & Approaches</h2><p>Here are some resources to get started;</p><ul><li>Microsoft Sample AKS Container: <a href=\"https://hub.docker.com/r/microsoft/sample-aks-helloworld\" target=\"_blank\">https://hub.docker.com/r/microsoft/sample-aks-helloworld</a></li><li>Deploy an AKS cluster using the Azure CLI: <a href=\"https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough</a></li><li>Dockerfile Fundamentals: <a href=\"https://dev.to/marcbeaujean/dockerfile-step-by-step-creating-a-custom-container-2ccm\" target=\"_blank\">https://dev.to/marcbeaujean/dockerfile-step-by-step-creating-a-custom-container-2ccm</a></li></ul><h2>Solution</h2><p>This will be a two part solution. I ran into some issues due to my lack of understanding of Kubernetes, but the follow up will be in next weeks video!</p><p><strong>Repository</strong>: <a href=\"https://github.com/bmorrisondev/weekly-cloud-challenges\" target=\"_blank\">https://github.com/bmorrisondev/weekly-cloud-challenges</a></p><p><em><strong>This stream is no longer available...</strong></em></p></div>","excerpt":"This challenge will be actively discussed on the Discord ( brianmm.chat ). Participants are encouraged to collaborate...","cachedOn":1681394615},{"id":"fe8e852e-cf3b-4bcb-9885-a8c0d13df6b4","notion_id":"fe8e852e-cf3b-4bcb-9885-a8c0d13df6b4","status":"Published","slug":"cloud-challenge-build-an-api-in-azure","relation_series":[],"publishOn":"2020-02-28T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Cloud Challenge: Build an API in Azure","html":"<div><p>This challenge will be active on the Discord (<a href=\"http://fullstack.chat\" target=\"_blank\">fullstack.chat</a>) between 3/1 & 3/7/2020. Participants are encouraged to collaborate with others on the Discord as well as publish their GitHub repositories ether in Discord or in the comments. Happy coding!</p><h2>Objective</h2><p>Create an API hosted in Azure that returns a random text when called. Use the language of your choice.</p><h2>Tips & Approaches</h2><p>Here are some services to explore to get started;</p><ul><li>API Management can be used to expose any backend services.</li><li>Azure Functions can be executed from requests either directly or through API Management.</li><li>An API can be hosted as an App Service if using .NET Framework & Web API.</li><li>A Docker container can be hosted in an Azure VM.</li></ul><h2>Solution</h2><p>Check out the live stream below to follow along on how I built three different APIs in Azure.</p><p><strong>Repository</strong>: <a href=\"https://github.com/bmorrisondev/weekly-cloud-challenges\" target=\"_blank\">https://github.com/bmorrisondev/weekly-cloud-challenges</a></p><p><em><strong>This stream is no longer available...</strong></em></p></div>","excerpt":"This challenge will be active on the Discord ( fullstack.chat ) between 3/1 & 3/7/2020. Participants are encouraged t...","cachedOn":1681394618},{"id":"59a2a51b-66d4-4c93-81e1-e4436bd5186d","notion_id":"59a2a51b-66d4-4c93-81e1-e4436bd5186d","status":"Published","slug":"script-scheduler-a-cron-based-nodejs-script-runner","relation_series":[],"publishOn":"2020-02-03T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Script-Scheduler: A cron based NodeJS script runner","html":"<div><p>One of the things that I do on a fairly regular basis is try to find ways to automate various processes in my life. To do this, I’ve used a combination of PowerShell scripts running on my server, Microsoft Flow (now called Power Automate), and IFTTT. Well Microsoft has started increasing Flow prices (and they made the HTTP connector Premium), and deploying PowerShell scripts sometimes can be a pain to do since I’m using the Windows Task Scheduler to manage them. I was looking for an easier way to port some of this functionality to something I can run in a Docker container and deploy using standard CI/CD tools.</p><p>My solution is what I’ve come to name <strong>script-scheduler</strong>. It’s a way to have small NodeJS scripts to run a set interval using cron expressions. What the framework does is recursively pull in a directory of specially formatted scripts and schedules them to run based on their cron expression. Here is a sample of what I am currently using in one of my projects;</p><pre class=\"language-plain text\"><code>// Import the package.\nconst scheduler = require('@brianmmdev/script-scheduler')\n\n// This is the directory where all my scripts live.\nconst baseDir = `${__dirname}/scripts/`\n\n// Pass it into the scheduler.\nscheduler.run(baseDir)\n</code></pre><p>A basic script is composed of three main components;</p><ul><li>A cron expression.</li><li>A function.</li><li>An optional enabled flag.</li></ul><p>Here is an example of a script that could be run;</p><pre class=\"language-plain text\"><code>module.exports = {\n    enabled: true,\n    schedule:  \"*/1 * * * *\",\n    fn: function () {\n        var date = new Date();\n        console.log('This script will run every minute!', date.toLocaleString());\n    }\n}</code></pre><p>There is plenty more polish that can be added to this thing, but it is in a working state so I figure why not publish it for others to use. Check it out at the links below;</p><ul><li>Github Repo: <a href=\"https://github.com/bmorrisondev/script-scheduler\" target=\"_blank\">https://github.com/bmorrisondev/script-scheduler</a></li><li>NPM Package: <a href=\"https://www.npmjs.com/package/@brianmmdev/script-scheduler\" target=\"_blank\">https://www.npmjs.com/package/@brianmmdev/script-scheduler</a></li></ul><p>Feel free to comment (or better yet, contribute) as you see fit. Happy coding!</p></div>","excerpt":"One of the things that I do on a fairly regular basis is try to find ways to automate various processes in my life. T...","cachedOn":1681394619},{"id":"0b52db6c-5a88-4f10-95a9-09b322c9ec2d","notion_id":"0b52db6c-5a88-4f10-95a9-09b322c9ec2d","status":"Published","slug":"deploy-a-website-to-aws-cloudfront","relation_series":[],"publishOn":"2020-01-26T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"Deploy a website to AWS CloudFront","html":"<div><p>In this article, I’ll be demonstrating how to configure AWS S3 and AWS CloudFront to host a static website.</p><h2>Setting up S3</h2><p>To start, I’ll need to create an S3 bucket to store the content of the website. S3 is object-based cloud storage hosted in AWS. You can store any kind of file here, but you can also host a website using their built-in Static Site Hosting. Head over to S3 and click Create bucket;</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/55a432ab-e100-444a-8552-9e5750746778-Untitled.png\" /><figcaption></figcaption></figure><p>You’ll need to name the bucket something that DNS can resolve to. So lowercase characters, periods, dashes, etc. I named this <strong>wordpress-static-site-test</strong> during this. Later in the series, I’ll likely create a new bucket once I uncover any caveats that go along with this process. On the second page of the Create bucket wizard, I unchecked Block all public access and acknowledged the warning that the contents of the bucket will be public. I left everything else as defaults. If you are doing a lot of these, tagging them would be a good way to figure out what’s what for billing or organizational purposes.</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/b91b4a52-81a2-4cc2-b136-2e112f0923a8-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/0a624c34-aef9-4fbb-8f9c-c14d4d42027a-Untitled.png\" /><figcaption></figcaption></figure><p>Once the bucket is created, click on it to manage it. Then head over to <strong>Properties</strong>, and enable <strong>Static website hosting</strong>. I set the <strong>Index document</strong> to <em>index.html</em> (I have a sample file I’m using for testing). Note the <strong>Endpoint</strong> URL as well since we’ll need to test this shortly.</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/b7e08ef5-ce2c-4771-a13b-09ac6e484a4e-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/06-2020-01-26_8-40-17-1024x58.png\" /><figcaption></figcaption></figure><p>Now we need to upload the sample <em>index.html</em>. On the second page of the wizard, change the dropdown to <strong>Grant public read access to this object</strong>.</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/6511b995-6d8c-48ef-bfdf-c6084d0139d3-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/6ac2bd15-f26b-4722-8094-3111aa0ad11c-Untitled.png\" /><figcaption></figcaption></figure><p>Now open a browser window and enter in the <strong>Endpoint</strong> URL you noted earlier. You should get the contents of the <em>index.html</em> file you uploaded. If you do, we’re looking good so far.</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/cf1ea82d-82ba-4260-9296-839546281bb9-Untitled.png\" /><figcaption></figcaption></figure><h2>Creating a CloudFront Distribution</h2><p>Next were going to deploy this site out to CloudFront, AWS’s CDN. If you are unfamiliar with a CDN, it is essentially a network of endpoints all around the world that is used to distribute your content closer to your users. For instance, if someone accesses my website in Europe, they’ll access it from an AWS data center close to them.</p><p>So head over to CloudFront and click <strong>Create Distribution</strong>. If you click on the Origin Domain Name, you should see a list of S3 buckets in your account. Select the one we created earlier.</p><p>An <strong>Origin</strong> is where the CDN will pull the content from. You can see here that if you wanted to publish an existing website out to a CDN, you’d just need to set the URL here.</p><p>Then scroll down to the bottom and set the <strong>Default Root Object</strong> to <em>index.html.</em> We’re telling CloudFront that it will be the default object returned to the requestor if there is no other path or filename afterwards.</p><p>Finally, click <strong>Create Distribution</strong> at the bottom.</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/9db6b3e1-6368-4c35-bb97-7aec5d9b1281-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/ed342b47-213e-486d-bb80-868290592614-Untitled.png\" /><figcaption></figcaption></figure><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/4ae80494-0755-49ab-89f2-3155a7f6c59f-Untitled.png\" /><figcaption></figcaption></figure><p>You’ll be returned back to the list of CloudFront Distributions and your new Distribution should show a deployment status of <em>In Progress</em>. Wait for it to say <em>Deployed</em> before going any further. This process can take up to 10 minutes in my experience.</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/94353f3b-4737-4e71-a31e-38127b2c2d1d-Untitled.png\" /><figcaption></figcaption></figure><p>Once the status is Deployed, click on the Deployment ID to grab the URL. It’s listed under the General tab as <strong>Domain Name</strong>.</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/8f97f4a2-aa3f-408a-b5ea-8fe545d08f9e-Untitled.png\" /><figcaption></figcaption></figure><p>Enter that URL into your browser and you should receive the same web page you did earlier, but it will be coming from CloudFront instead of directly from S3.</p><figure><img src=\"/img/n/deploy-a-website-to-aws-cloudfront/f2ce2b27-e676-4112-be55-a6b78b996e15-Untitled.png\" /><figcaption></figcaption></figure><p>That’s it! You now have a (rather basic) static website hosted in AWS endpoints all over the world! In the next part of this series, I’ll be looking at requesting a certificate with Amazon Certificate Manager and setting a CNAME in the CloudFront distribution so we can use our own URL.</p></div>","excerpt":"In this article, I’ll be demonstrating how to configure AWS S3 and AWS CloudFront to host a static website. To start,...","cachedOn":1681394620},{"id":"d8430c4f-0e7f-444f-9d73-3828d31bc061","notion_id":"d8430c4f-0e7f-444f-9d73-3828d31bc061","status":"Published","slug":"how-i-learn-new-stuff-in-tech","relation_series":[],"publishOn":"2022-09-16T00:00:00.000Z","codeURL":null,"seriesOrder":null,"youTubeURL":null,"title":"How I learn new stuff in tech","html":"<div><p>I was listening to a <a href=\"https://www.youtube.com/watch?v=qe033p3RUPM\" target=\"_blank\">recording of a Twitter Space </a>by <a href=\"https://twitter.com/colbyfayock\" target=\"_blank\">Colby Fayock</a> on developer education while looking for resources to help me get better at my new job in the field. It was comprised of a group of other highly skilled developers who also use their skills to help educate others in the field. Since the field of software development is also one of never-ending learning, one of the topics was how to learn new things. It got me thinking about my approach to learning new technologies.</p><p>It’s worth noting that this system is NOT for brand new developers, but for those who have at least a basic understanding of programming and can build a project of their own. That said, once you get the basics of development down and understand how to build something, you can easily make this work.</p><h2>Set up a ‘common’ project</h2><p>Pick something to build and something you can build over and over again. I call it a ‘common’ project not because it's something everyone has done, but because it is something you can continue to work on over and over again. It helps if it's about a topic you love since you’ll be more inclined to work on it and have a closer personal connection to it.</p><p>That said, it doesn't have to be complicated. Even the simplest note-taking app or to-do app when built properly has a lot of moving components, and you can make them as simple or complex as you want. Running with the concept of a note-taking app, here is how potentially complicated you can make it:</p><ul><li>Build an API to store data in a relational database.</li><li>Use OAuth to enable multi-user functionality.</li><li>Build a front end with React and deploy to <a href=\"https://www.netlify.com/\" target=\"_blank\">Netlify</a>.</li><li>Build a native mobile app for Android or iOS.</li><li>Set up real-time UI updates with WebSockets or gRPC.</li><li>Set up asynchronous automations with queueing and serverless functions.</li></ul><p>It doesn't matter what the app does, as long as you understand what it should do. Then you get to fill in the blanks with whatever framework/language/etc you want.</p><h2>Understand where ‘the thing’ fits</h2><p>I use ‘the thing’ to refer to any new piece of tech you wish to explore. They all belong to one of four general categories:</p><ul><li>Frontend</li><li>Backend</li><li>Storage</li><li>Conduit</li></ul><p>I like to think of the general system flow from left to right like so.</p><figure><img src=\"/img/n/how-i-learn-new-stuff-in-tech/c37c9fbc-7a86-4650-b4ff-c2da6ccf9cdc-Untitled.png\" /><figcaption></figcaption></figure><p>Let’s break each of these down. I’ll be using the term “data” to represent information as it traverses the system. It could be a text file, an image, or a JSON structure. It doesn’t matter what it is, but it moves through the system and uses various services.</p><h3>Frontend</h3><p>This is the pretty stuff that shows up on screen and is the part users interact with most. This includes web frameworks, mobile apps, etc.</p><h3>Backend</h3><p>This is where the data goes to be processed in some capacity before it is stored in the storage (or data) layer, or retrieved from the storage layer and returned to the frontend system. This is why it sits between storage & front end. It’s the piece that connects them and makes them talk. This layer also often handles checking requests from the front end to make sure they are valid (ie; user authentication).</p><h3>Storage</h3><p>This is where data is stored. This could be a relational database, a NoSQL data store, or a file storage service. There isn’t much more to say about this layer, but that doesn’t make it simple. Each type of storage service has its own conventions and ways to store and retrieve data.</p><h3>Conduit</h3><p>This is a tricky one to place. I consider it to be in the Conduit category if it involves protocols that are used for services to communicate with one another. It could be low-level protocols like TCP and HTTP, but I also group things like GraphQL and queuing services here.</p><h2>A real-world example</h2><p>If you’ve followed me at all over the last two years, you’ll know I built an <a href=\"https://github.com/GuardianForge/guardianforge.net\" target=\"_blank\">open-source</a> web service for Destiny 2 players called <a href=\"https://guardianforge.net/\" target=\"_blank\">GuardianForge</a>. I built this service using mostly AWS services but intermingled third-party services like <a href=\"https://www.algolia.com/\" target=\"_blank\">Algolia</a>and <a href=\"https://stripe.com/\" target=\"_blank\">Stripe</a>. Here is a diagram of the architecture of the site, categorized based on the definitions above, and described in detail below.</p><figure><img src=\"/img/n/how-i-learn-new-stuff-in-tech/647d5953-6c32-492b-ad2b-4296acde286a-Untitled.png\" /><figcaption></figcaption></figure><ol><li>The front end is comprised of 3 apps. They are built with <strong>React</strong> and hosted <strong>AWS S3</strong> with <strong>CloudFront</strong> serving up the files to the user.</li><li>When an API request is made (a user creates a build), the front end sends the request through <strong>AWS API Gateway</strong>to an <strong>AWS Lambda</strong> function that processes the request.</li><li>Lambda will create a summary record in <strong>DynamoDB</strong>.</li><li>Lambda will also create a raw JSON file for the front end to render and reduce the load on the database.</li><li>A message is sent to <strong>AWS SQS</strong> to further process the build, this results in a faster response to the user since the post-build logic doesn't necessarily need to be done immediately.</li><li>The post-build Lambda function picks up the message and creates an OpenGraph social image to be shared on Twitter and sends a message to the GuardianForge <strong>Discord</strong> server (not shown).</li><li>The Lambda from <code>6</code> will also send a message to <strong>Algolia</strong>, which powers the search on GuardianForge.</li><li>Using Algolia libraries, the front end can interact with Algolia to search for builds.</li><li>When a user signs up to support GuardianForge, the Lambda API interacts with <strong>Stripe</strong> to set up the subscription and flag the user in Dynamo that they are a premium user.</li><li>Finally, the front end can also interact directly with the JSON files from <code>4</code>.</li></ol><h2>How I used this process</h2><p>When I joined PlanetScale, I was tasked with building something on the service. Since I already had GuardianForge setup, and I had a separate environment to play with new stuff that doesn't affect production, all I had to do was update the Lambdas that wrote to DynamoDB to now write my PlanetScale database. I essentially swapped out one of the boxes in my diagram.</p><figure><img src=\"/img/n/how-i-learn-new-stuff-in-tech/a7776801-2f6a-4ae6-ad20-5b5effa1c6a6-Untitled.png\" /><figcaption></figcaption></figure><p>I did all of this in a separate branch in the repo, which is <a href=\"https://github.com/GuardianForge/guardianforge.net/tree/dev/planetscale-poc\" target=\"_blank\">still public to this day</a>. I’ve also redone the frontend several times (it originally was a Vue project) and because I use this approach, I only had to replace the green parts in my diagram and everything else stayed as is.</p><h2>Closing thoughts</h2><p>If you want to make this work for you, set up a project of your own that you can continue to use. Start as simple as you need, or over-engineer the hell out of it to fit any category or service you want. If you can, make it on a topic you are passionate about. Then, when the next shiny thing comes out that you HAVE to try, figure out which box it fits in and swap out that component in your stack.</p></div>","featuredImage":"/img/n/how-i-learn-new-stuff-in-tech/d5066483-ffe6-495a-970b-958816b5331a-how-i-learn.jpg","icon":"/img/n/how-i-learn-new-stuff-in-tech/6ff44c7e-e999-42a3-a9c2-ac8dd84a295f-nerd.png","excerpt":"I was listening to a  recording of a Twitter Space  by  Colby Fayock  on developer education while looking for resour...","cachedOn":1681394622},{"id":"e7f565fd-1f93-4fa7-bf09-925fb3b55792","notion_id":"e7f565fd-1f93-4fa7-bf09-925fb3b55792","status":"Published","slug":"first-experience-with-framer-motion","relation_series":[],"publishOn":"2022-10-04T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/framer-motion-scratch","seriesOrder":null,"youTubeURL":null,"title":"First experience with Framer Motion","html":"<div><p>I've wanted a good strategy to build animated diagrams to use within my videos for quite some time now. I've explored things like Adobe After Effects or DaVinci Fusion, but they felt like overkill to me. When I got the idea of using SVGs and code in some way to accomplish this, my research turned me on to Framer Motion. Today is the first day I've used this library and so far IT'S AWESOME! Here's what I was able to pull off in about 2-3 hours of using it.</p><div class=\"callout\"><div class=\"callout-icon\">⚠️</div><div class=\"callout-content\">This code is NOT PRODUCTION QUALITY, it's just here as a reference.</div></div><h2>General overview</h2><p>This is by no means an elaborate explanation of how to use Framer Motion, just how I interpret it. When you import <code>motion</code> from the library, you can prepend any element with it. So <code>div</code> becomes <code>motion.div</code>, then you can use the custom props that come with the library. These are as follows:</p><ul><li><code>initial</code> - The initial state of the object.</li><li><code>animate</code> - The final state of the object.</li><li><code>transition</code> - Attributes around how the object behaves (duration, easing animation, etc)</li></ul><p>The first two in the list are apparently controlled with CSS attributes and the <code>transition</code> prop has its own object to work with.</p><h2>Making my first box</h2><p>In infrastructure diagrams, boxes are pretty commonplace to represent components within the system, so it felt like an easy place to start. I first tried creating a <code>div</code> with a border to animate. Framer Motion seems to work off of CSS attributes, so I could animate scale and opacity, but not actually drawing the box on screen. FM did however support drawing paths in SVGs, so I created a custom Box component in React that I could use to animate the path when it gets dropped on the screen. This let me animate the border of the box being drawn, which is pretty good progress overall.</p><figure><img src=\"/img/n/first-experience-with-framer-motion/181a7700-2483-4d23-8968-4c062d3ffa00-CleanShot-2022-10-04-at-16.49.37-1.gif\" /><figcaption></figcaption></figure><p>Notice in the code that it has the <code>motion</code> prefix on the SVG elements, as well as those three attributes from earlier on the <code>motion.rect</code> element:</p><pre class=\"language-plain text\"><code>&lt;motion.svg viewBox={viewBox} style={{ position: 'absolute', top: 0, left: 0}}&gt;\n  &lt;motion.rect width={width - stroke} height={height - stroke} rx=\"15\" fill=\"transparent\" stroke=\"red\"\n    strokeWidth={stroke}\n    initial={{\n      pathLength: 0\n    }}\n    animate={{\n      pathLength: 1.1\n    }}\n    transition={{\n      duration: 2,\n      ease: 'easeInOut',\n    }}\n    /&gt;\n&lt;/motion.svg&gt;</code></pre><h2>Creating a canvas</h2><p>Again, my primary goal is to build some kind of tool that lets me create animated diagrams for my videos. Preferably something I could control using my keyboard or a mouse click. I have an ultrawide monitor and record a section of my monitor that equates to 1080p, so I needed something that I could set the size to match as well. I built a canvas component that is hard set to 1920x1080 and has relative positioning so I can place child components at specific X/Y coordinates.</p><figure><img src=\"/img/n/first-experience-with-framer-motion/94702ed3-e8c0-4853-beab-607515439422-CleanShot-2022-10-04-at-16.51.46.png\" /><figcaption></figcaption></figure><p>My <code>Box</code> component was updated to take in X/Y coordinates, height, width, and stroke size. There are some calculations done with those values to ensure the box is drawn properly within the viewbox of the SVG element, but the result is that I can place a box on the canvas using the following code:</p><pre class=\"language-plain text\"><code>&lt;Box3 height={200} width={200} stroke={3} x={200} y={200} /&gt;</code></pre><h2>Animating connectors</h2><p>In diagramming software, its pretty easy to place connectors between various elements, but I wanted to take it a step further and create dotted, animated connectors that I can use to highlight certain parts of the diagram when I'm explaining what is going on with the diagram. I was able to animate an SVG <code>line</code> element by messing with the <code>stroke-dashoffset</code>like so:</p><pre class=\"language-plain text\"><code>&lt;motion.line x1=\"400\" y1=\"300\" x2=\"600\" y2=\"300\" stroke=\"black\" strokeDasharray=\"10\"\n  initial={{\n    opacity: 0,\n    strokeDashoffset: 60\n  }}\n  animate={{\n    strokeDashoffset: 0\n  }}\n  transition={{\n    duration: 1,\n    repeat: Infinity,\n    ease: \"linear\"\n  }}\n/&gt;</code></pre><h2>Controlling the animations</h2><p>Framer Motion has a React Hook called <code>useAnimationControls()</code> which lets you control when transitions should fire. Using a sequence number held in a state object along with <code>useEffect()</code>, I was able to wire up clicking on the canvas wrapper to iterate a number and control the animations based on that sequence number.</p><pre class=\"language-plain text\"><code>const controls = useAnimationControls()\nconst textControls = useAnimationControls()\nconst connectorControls = useAnimationControls()\nconst veryNiceControls = useAnimationControls()\nconst [sequenceNum, setSequenceNum] = useState(0)\n\nuseEffect(() =&gt; {\n  async function handleSequence(sequence: number) {\n    if(sequence === 1) {\n      controls.start({\n        opacity: 1\n      })\n    }\n    if(sequence === 2) {\n      connectorControls.start({\n        strokeDashoffset: 0,\n        transition: {\n          duration: 1,\n          repeat: Infinity,\n          ease: \"linear\"\n        }\n      })\n      connectorControls.start({ opacity: 1 })\n    }\n    if(sequence === 3) {\n      veryNiceControls.start({\n        width: 500,\n        height: 566\n      })\n    }\n  }\n  handleSequence(sequenceNum)\n}, [sequenceNum])</code></pre><h2>Putting it all together</h2><p>After putting all of these things together, I was able to create a very basic animated diagram to represent the model I use to learn new stuff and build services. Here is the project so far:</p><figure><img src=\"/img/n/first-experience-with-framer-motion/288fd278-ed0d-4d38-bf1f-01041076b3cd-CleanShot-2022-10-04-at-16.33.40.gif\" /><figcaption></figcaption></figure><p>My next step is to build on this project to create a framework for building animated diagrams for video content. Stay tuned for updates!</p></div>","featuredImage":"/img/n/first-experience-with-framer-motion/a1b52453-5075-4e94-8079-a0dbcf915d85-first-exp-framer-motion.jpg","icon":"/img/n/first-experience-with-framer-motion/bfb997e4-170b-4a00-8a8b-bfd89e622821-framer-motion-300x290.webp","excerpt":"I've wanted a good strategy to build animated diagrams to use within my videos for quite some time now. I've explored...","cachedOn":1681394623},{"id":"b540412c-3122-443c-a183-ed0678e55819","notion_id":"b540412c-3122-443c-a183-ed0678e55819","status":"Published","slug":"building-a-mastodon-scheduler-insipration-and-tech-stack","relation_series":["cacc9e5f-3d04-4193-97ab-ddbcc2a87a16"],"publishOn":"2022-11-24T00:00:00.000Z","codeURL":"https://github.com/tweetyah/tootahead","seriesOrder":1,"category":"open saas","youTubeURL":null,"title":"Building a Mastodon Scheduler: Insipration and Tech Stack","html":"<div><p>A few weeks ago, I decided to start building a Twitter scheduler. Considering the platform is a bit of a mess, I switched to focus the application for Mastodon instead. In this series, I’ll provide the outline of the project components, interesting findings, and my vision for this project.</p><h2>Inspiration</h2><p>MANY years ago, I hacked together a tweet scheduling system using a combination of a Trello board and Microsoft Power Automate (then called Flow). A flow in Power Automate would run every few hours and pick a Trello card at random and share it. I had built logic in so the same card wouldn't be shared too frequently, and it worked pretty well for the time being.</p><figure><img src=\"/img/n/building-a-mastodon-scheduler-insipration-and-tech-stack/b78c640b-c9d8-4645-a056-341e785d34b9-image.jpeg\" /><figcaption></figcaption></figure><p>My Marketing Automation board in Trello.</p><p>Fast forward to just last year, I rebuilt some of the same logic into a Notion database along with a scheduler service built with Go. The service would handle the process of picking something from Notion and sending it out, along with any images attached to the note in Notion. I built a custom UI with React and hosted it in Netlify that had a neat date & time picker that I really liked using.</p><figure><img src=\"/img/n/building-a-mastodon-scheduler-insipration-and-tech-stack/16ab20be-63da-4ba1-9e31-36bb2ef5e506-image.jpeg\" /><figcaption></figcaption></figure><p>My custom Notion Tweeter app.</p><p>This lead me to take on the challenge of turning this into a Saas to build another revenue stream.</p><h2>The tech stack</h2><p>The first thing I really wanted to dive into is the tech stack I’m using for the project. This is mainly focused on hosting, automation, languages, frameworks, etc.</p><h3>Front end</h3><p>I’ve built many tools over the years using Vue and React, but I really wanted to try something different since I’ve been in a bit of a rut lately. That’s why I chose to try out Svelte, and I absolutely love it.</p><figure><img src=\"/img/n/building-a-mastodon-scheduler-insipration-and-tech-stack/cd1477e1-efde-41bf-9096-925d39f6a100-image.png\" /><figcaption></figcaption></figure><p>The components are small and concise, which is great. But one of the things I love most is that the framework really just kinda gets out of the way. For example, I built my own accordion component for TootAhead using a lot of vanilla JavaScript, something I’ve always found to be challenging in Vue or React.</p><p>I also decided to really dive into Tailwind for the first time, and again, fell in love with it. Not a super fan of the massive number of classes in a single tag, but when components are properly separated, it’s not all bad.</p><p>I also love the custom theming support. It makes creating my own colors to apply SUPER easy. I feel like I’m just scratching the surface of Tailwind and can’t wait to learn more.</p><h3>The backend</h3><figure><img src=\"/img/n/building-a-mastodon-scheduler-insipration-and-tech-stack/7377565d-4cfa-4076-a9fa-68fb173e2eee-Untitled.png\" /><figcaption></figcaption></figure><p>The backend of the service is built entirely with Go but is split into two distinct parts: the API and the Scheduler.</p><p>The API is a set of Serverless functions that are built to run on Netlify. These are used to save and schedule posts, as well as handle the OAuth logic so users can sign in with their Mastodon account.</p><p>The Scheduler is a simple Go binary that runs in a loop, checking for new posts to send every minute. When a post is discovered, it will pull the necessary tokens from the database for that user and submit the post using the Mastodon API. Any errors are caught and stored with that post record in the database.</p><h3>Data storage</h3><figure><img src=\"/img/n/building-a-mastodon-scheduler-insipration-and-tech-stack/39a62145-52c7-4e2a-be83-d5b5cb8ed74a-image.jpeg\" /><figcaption></figcaption></figure><p>While I may eventually need to set up some kind of file storage server, I’ve only got a database at the time of this writing. All of the data for TootAhead is stored in a PlanetScale database. While it’s true that my working there had a hand in it, I am honestly a fan of the platform.</p><p>As someone who has a decent background working with relational databases, I’ve waited years for a truly Serverless relational database system, and PlanetScale delivers that and so much more. I’ll break the schema down a bit later in this series as there is a decent amount to cover before it will really make sense.</p><h3>Hosting</h3><figure><img src=\"/img/n/building-a-mastodon-scheduler-insipration-and-tech-stack/ab5b566b-c80d-4611-83c2-3f44fa32b7e3-Untitled.png\" /><figcaption></figcaption></figure><p>Netlify is the east winner here for me. The fact that I can have a DevOps pipeline for my app and website wired to a GitHub repository with little work is unbeatable (and I’m no stranger to building pipelines).</p><p>As mentioned earlier, I’m using the Netlify Functions feature to run the API as well, and that’s all rolled into the service. HTTPS works out of the box with LetsEncrypt, and the certificates automatically renew, so that’s one less thing I have to worry about.</p><p>I actually have two Netlify sites that run the service. One is dedicated to the web app itself at <a href=\"https://app.tootahead.com/\" target=\"_blank\">app.tootahead.com</a>, and the second runs a SvelteKit website that’s only a single page now at <a href=\"https://tootahead.com/\" target=\"_blank\">tootahead.com</a>.</p><h2>Conclusion</h2><p>I'll be exploring the project structure & automation pipelines in the next article. If you are interested in exploring the project, feel free to check out the source at <a href=\"https://github.com/tweetyah/tootahead\" target=\"_blank\">github.com/tweetyah/tootahead</a>.</p></div>","featuredImage":"/img/n/building-a-mastodon-scheduler-insipration-and-tech-stack/8973a516-970f-43e2-a946-dac1d4ef12e3-background-3.jpg","icon":"/img/n/building-a-mastodon-scheduler-insipration-and-tech-stack/5882f87b-051a-4c4a-887c-d53433375a74-logo.png","excerpt":"A few weeks ago, I decided to start building a Twitter scheduler. Considering the platform is a bit of a mess, I swit...","cachedOn":1681394625},{"id":"9bed83f6-cf69-4814-8dab-452833e2a644","notion_id":"9bed83f6-cf69-4814-8dab-452833e2a644","status":"Published","slug":"build-a-responsive-modal-with-svelte-and-tailwind","relation_series":[],"publishOn":"2022-11-29T00:00:00.000Z","codeURL":null,"seriesOrder":null,"category":"front end","youTubeURL":null,"title":"Build a responsive modal with Svelte and Tailwind","html":"<div><p>I'm working on a new Saas and decided to use some new tech that I haven't used before, specifically Svelte & Tailwind. More details on that Saas are found in another recent post, but I just wrapped up building an interactive modal from scratch and figured I'd share how I did it.</p><p>Here is the end result:</p><figure><img src=\"/img/n/build-a-responsive-modal-with-svelte-and-tailwind/87781c61-a4ee-41b7-9eba-36783a66b108-CleanShot-2022-11-29-at-09.51.02.gif\" /><figcaption></figcaption></figure><p>I'm going to make this a bit more generic than my use case. I started by creating a component that used a <code>if</code> block to check if the prop <code>open</code> is true or not to display the modal. I also used the <code>transition</code> prop on the main <code>div</code> to apply the smooth animation as it is rendered/removed. The Tailwind classes applied will make this <code>div</code> take up the entire screen, and center any content within.</p><pre class=\"language-plain text\"><code>&lt;script lang=\"ts\"&gt;\n  import { sineIn } from \"svelte/easing\";\n  import { fade } from \"svelte/transition\";\n\n  export let open: boolean\n&lt;/script&gt;\n\n{#if open}\n  &lt;div transition:fade={{ duration: 150, easing: sineIn }}\n    class=\"absolute top-0 bottom-0 left-0 right-0 grid justify-center items-center\"&gt;\n\n  &lt;/div&gt;\n{/if}</code></pre><p>Next I'll add some content so when the modal is open, there is actually something on the screen. Here is a breakdown of the important Tailwind classes:</p><ul><li><code>relative</code> - Keeps this div with the parent, since the parent is <code>absolute</code>.</li><li><code>w-screen sm:w-[80vw] sm:max-w-[800px]</code> - Makes the div take up the full width of the screen, UNLESS its at or above the <code>sm</code> breakpoint, by which it will take up 80% of the screen width up to a max of 800px.</li><li><code>h-screen sm:h-auto</code> - Similar to above, take up the full height of the screen, or only as much as needed if at the <code>sm</code> breakpoint or above</li></ul><pre class=\"language-plain text\"><code>&lt;script lang=\"ts\"&gt;\n  import { sineIn } from \"svelte/easing\";\n  import { fade } from \"svelte/transition\";\n\n  export let open: boolean\n&lt;/script&gt;\n\n{#if open}\n  &lt;div transition:fade={{ duration: 150, easing: sineIn }}\n    class=\"absolute top-0 bottom-0 left-0 right-0 grid justify-center items-center\"&gt;\n    &lt;div class=\"relative rounded shadow-sm w-screen sm:w-[80vw] sm:max-w-[800px] h-screen sm:h-auto bg-bglight\" style=\"z-index: 1000;\"&gt;\n      Main content goes here!\n    &lt;/div&gt;\n  &lt;/div&gt;\n{/if}</code></pre><p>Now instead of adding a background to the top-most container, I needed to create a secondary <code>div</code> that was clickable so when the user clicks on the backdrop, it will close the modal. I also added the <code>onClose</code> prop so my modal can signal to the parent when that backdrop is clicked. The <code>z-index</code> values are important between the backdrop and the main content. Since the content div has a higher <code>z-index</code> value than the backdrop, it will visually be placed on top of the backdrop.</p><pre class=\"language-plain text\"><code>&lt;script lang=\"ts\"&gt;\n  import { sineIn } from \"svelte/easing\";\n  import { fade } from \"svelte/transition\";\n\n  export let open: boolean\n  export let onClose: Function\n&lt;/script&gt;\n\n{#if open}\n  &lt;div transition:fade={{ duration: 150, easing: sineIn }}\n    class=\"absolute top-0 bottom-0 left-0 right-0 grid justify-center items-center\"&gt;\n    &lt;div on:click={() =&gt; onClose()} class=\"absolute top-0 bottom-0 left-0 right-0 bg-gray-900/90\" style=\"z-index: 100;\" /&gt;\n    &lt;div class=\"relative rounded shadow-sm w-screen sm:w-[80vw] sm:max-w-[800px] h-screen sm:h-auto bg-bglight\" style=\"z-index: 1000;\"&gt;\n      Main content goes here!\n    &lt;/div&gt;\n  &lt;/div&gt;\n{/if}</code></pre><p>Finally, the parent component that is using the modal should have a prop that signals whether the modal is open or not, as well as the <code>onClose</code> wired up to set that value to false:</p><pre class=\"language-plain text\"><code>&lt;script lang=\"ts\"&gt;\n  import Modal from \"./Modal.svelte\";\n  let isModalOpen: boolean = false;\n&lt;/script&gt;\n\n&lt;div&gt;\n  &lt;Modal open={isModalOpen} onClose={() =&gt; isModalOpen = false} /&gt;\n&lt;/div&gt;</code></pre><p>Finito!</p></div>","featuredImage":"/img/n/build-a-responsive-modal-with-svelte-and-tailwind/214b693f-b04c-4f1c-b7a0-11e528c40384-background-1.jpg","icon":"/img/n/build-a-responsive-modal-with-svelte-and-tailwind/531b3b99-f51d-4234-9f0d-e63fc7aa8e00-svelte-logo-E3497608CB-seeklogo.com.png","excerpt":"I'm working on a new Saas and decided to use some new tech that I haven't used before, specifically Svelte & Tailwind...","cachedOn":1681394627},{"id":"69aca527-45be-48b1-877d-5b60e4522859","notion_id":"69aca527-45be-48b1-877d-5b60e4522859","status":"Published","slug":"building-a-mastodon-scheduler-project-structure-and-deployments","relation_series":["cacc9e5f-3d04-4193-97ab-ddbcc2a87a16"],"publishOn":"2022-12-01T00:00:00.000Z","codeURL":"https://github.com/tweetyah/tootahead","seriesOrder":2,"category":"open saas","youTubeURL":null,"title":"Building a Mastodon scheduler: project structure & deployments","html":"<div><p>In the second entry of the series, I'll cover how I structured the project and how automated deployments to Netlify are set up. Some of the code samples refer to the original project name (TweetYah), so you'll have to ignore some of that.</p><h2>General project structure</h2><p>I'm a big fan of the monorepo approach on solo projects. It keeps everything related to the project in one place instead of having to hop between multiple repositories for frontend, backend, API, etc. That said, there are realistically 3 different things in play here. The first is the front end that's built with Svelte. Next is the API written in Go as Netlify Functions. And the third is the scheduler that scans the database every minute for posts to dispatch. You might consider the front end and API to be the same project, but the code bases are completely different.</p><h2>Directory breakdown</h2><p>Below is an in-depth breakdown of each component, as well as some of my reasoning behind why I did certain things.</p><h3>App</h3><p>This folder contains both the front-end Svelte code as well as the Go Netlify Functions. This is primarily because of the way Netlify handles the deployments, as well as testing the app locally. By running <code>npm run dev</code>, the Netlify CLI is triggered to build and monitor the front-end, AND monitor the functions directory for changes.</p><p>This lets me hit a URL in my browser and configure any API calls from the front end to hit <code>/.netlify/functions/{handler}</code>, which stays consistent even when deployed to Netlify. Since I'm just using the path instead of a completely separate domain, CORS doesn't become an issue with this approach.</p><h3>Db</h3><p>This directory contains the structure of my PlanetScale database schema in an HCL file that can be used by Atlas CLI (a new favorite tool of mine) to deploy schema changes to the database. Essentially it allows me to version control the database structure and store it along with the repository. There is a simple script in that calls the Atlas CLI to deploy those changes, something I will be utilizing more down the line.</p><pre class=\"language-bash\"><code># apply.sh\n\natlas schema apply -u mysql://localhost/tweetyah -f schema.hcl</code></pre><p>I haven't fully set up all of the pipelines I want, but the end goal will be to use something like GitHub Actions to automate the process of updating the database schema for me. When the file changes in the <code>dev</code> branch, I'll have an action automatically apply those changes to the <code>dev</code> branch of my PlanetScale database. When a PR goes to the <code>main</code> branch in GitHub, another Action will be set up to merge the database changes in PlanetScale. This helps me keep a nice history of the changes that can be applied directly to my database. This will be an article all in itself once I get to that point.</p><h3>Lib</h3><p>Lib is a shared Go module that's used between the Scheduler and the Netlify Functions. If you are familiar with decoupled application structures (Onion, Hexagonal, etc), this is the \"business logic\" portion of that. This is essentially the core of the backend code. Using Go's <code>replace</code> directive in the <code>go.mod</code> files, I'm able to tell the system to reference this directory for that dependency instead of hitting a public URL like so:</p><pre class=\"language-plain text\"><code>module github.com/tweetyah/scheduler\n\ngo 1.17\n\nreplace github.com/tweetyah/lib =&gt; ../lib // &lt;= here\n\nrequire (\n    github.com/go-sql-driver/mysql v1.6.0\n    github.com/joho/godotenv v1.4.0\n)\n\nrequire (\n    github.com/aws/aws-lambda-go v1.34.1 // indirect\n    github.com/aws/aws-sdk-go v1.44.131 // indirect\n    github.com/bmorrisondev/go-utils v1.0.1 // indirect\n    github.com/golang-jwt/jwt v3.2.2+incompatible // indirect\n    github.com/jmespath/go-jmespath v0.4.0 // indirect\n    github.com/pkg/errors v0.9.1 // indirect\n    github.com/tweetyah/lib v0.0.0-20221105205022-03b042823748 // indirect\n)\n</code></pre><h3>Scheduler</h3><p>The Scheduler is a Go project that compiles to a single binary and is deployed via Docker to my home lab for the time being. For that deployment, I use Azure DevOps with a  <code>makefile</code> to handle the logic of deploying the app. I'll eventually move this workflow to GitHub Actions, but I already have a deploy agent running on my home lab so this was just a shortcut.</p><pre class=\"language-bash\"><code># makefile\nbuild:\n    GOOS=linux GOARCH=amd64 go build -o dist/scheduler .\n    docker build --tag tweetyah-scheduler:latest .\n\nstart: build\n    docker rm -f tweetyah-scheduler\n    docker run -d --name tweetyah-scheduler tweetyah-scheduler:latest\n\n</code></pre><p>Its job is to check the database every minute for posts that need to be sent, specifically the <code>posts</code> table in the database. It filters posts to only those that are in a \"scheduled\" <code>status</code> with a <code>send_at</code> value earlier than the current time. This is run in an infinite loop that has a 1-minute delay between iterations.</p><pre class=\"language-go\"><code>func main() {\n    godotenv.Load()\n\n    for {\n        // do stuff..\n        time.Sleep(1 * time.Minute)\n    }\n}</code></pre><p>The query that's run also joins out to a few other tables to grab user tokens and the mastodon domain where the post was scheduled.</p><pre class=\"language-go\"><code> query := `\n        select\n            p.id,\n            p.text,\n            p.send_at,\n            p.retweet_at,\n            p.is_thread,\n            p.thread_count,\n            p.id_user,\n            p.thread_parent,\n            p.thread_order,\n            ut.access_token,\n            ut.refresh_token,\n            ut.access_token_expiry,\n            p.service,\n            ut.mastodon_domain\n        from\n            posts p\n        left join\n            user_tokens ut on p.id_user = ut.user_id\n        where\n            status = 0 and send_at &lt; NOW()`</code></pre><h3>Website</h3><p>The Website directory holds a SvelteKit project that's essentially just a single landing page, deployed to h<a href=\"https://tootyah.com/\" target=\"_blank\">ttps://tootyah.com</a> via Netlify. Not a whole lot of complexity going on here, just a reiteration of the App directory. The key takeaway here is that I don't have it hooked up for deployment using Netlify's automatic system. Instead, I use an NPM script to trigger deployments manually.</p><h2>Conclusion</h2><p>Next up I think I'll break down the database structure in detail and show my methods for working with SQL in Go without an ORM.</p></div>","featuredImage":"/img/n/building-a-mastodon-scheduler-project-structure-and-deployments/24e05d62-4d9d-4ab9-8265-005c96627ff3-background.jpg","icon":"/img/n/building-a-mastodon-scheduler-project-structure-and-deployments/d5e3fcf7-6522-4ea8-ad19-f3b5e462d109-logo.png","excerpt":"In the second entry of the series, I'll cover how I structured the project and how automated deployments to Netlify a...","cachedOn":1681394629},{"id":"8372f9dd-4cb4-4261-b30f-6c284ac4abfe","notion_id":"8372f9dd-4cb4-4261-b30f-6c284ac4abfe","status":"Published","slug":"my-notion-note-taking-automations","relation_series":[],"publishOn":"2023-02-03T00:00:00.000Z","codeURL":"https://github.com/bmorrisondev/bmo-cloud","seriesOrder":null,"category":"notion","youTubeURL":null,"title":"My Notion note-taking automations","html":"<div><h2>My Notion Note Taking Automations</h2><p>After attending THAT conference a few weeks ago, I sat in a group that discussed ongoing journalling. I decided to really get back into it using my Notion Notes database after being inspired during that talk. I used to keep a daily note in my OneNote notebooks way back in the day but it’s been a while since I have. Since then, I’ve moved to have a weekly note, which I prefer since it’s just a running list of the things I want to jot down. Beyond that, I’ve decided to automate some things with code. Here’s a quick and dirty rundown.</p><h2>One note per week</h2><p>I have a database called Notes that I use to keep misc notes in. Every week, I create a note that spans the entire week on a calendar, using a Date property with start and end dates. This is actually an automated process since I have a script that runs on Monday around 4AM to set this up, but it’s not so difficult that I couldn’t do it myself.</p><figure><img src=\"/img/n/my-notion-note-taking-automations/1726545b-e7dc-4f3f-8e19-3e4ef7492372-Untitled.png\" /><figcaption></figcaption></figure><h2>Daily setup</h2><p>I actually don’t use any templates here, but instead, I recently built an automation using serverless functions on Netlify that uses the Notion API to grab the latest page in that document and append my daily task list (which honestly needs to be expanded on) and adds an inspirational quote just to put me in a good space mentally.</p><figure><img src=\"/img/n/my-notion-note-taking-automations/d8082b3d-dc31-4bf3-816d-3f35198e5d0e-Untitled.png\" /><figcaption></figcaption></figure><p>So here is the serverless function in Netlify.</p><pre class=\"language-javascript\"><code>const { Client } = require(\"@notionhq/client\")\nconst Quote = require('inspirational-quotes');\n\nconst dailyTodos = [\n  \"review email\",\n  \"review github issues\",\n  \"review todos\"\n]\n\nexports.handler = async function (event, context) {\n\n  notion = new Client({\n    auth: process.env.NOTION_KEY\n  })\n\n  // Get the latest note in that db\n  let params = {\n    database_id: process.env.NOTES_DBID,\n    sorts: [\n      {\n        property: 'Date',\n        direction: 'descending'\n      }\n    ]\n  }\n  let res = await notion.databases.query(params)\n\n  let latestNote = res.results[0]\n\n  // this is a template to add the blocks I want\n  params = {\n    block_id: latestNote.id,\n    children: [\n      {\n        type: \"divider\",\n        divider: {}\n      },\n      {\n        \"heading_2\": {\n          \"rich_text\": [\n            {\n              \"text\": {\n                \"content\": (new Date()).toLocaleDateString()\n              }\n            }\n          ]\n        }\n      },\n      {\n        \"type\": \"toggle\",\n        \"toggle\": {\n          \"rich_text\": [\n            {\n              \"type\": \"text\",\n              \"text\": {\n                \"content\": \"daily tasks\",\n              }\n            }\n          ],\n          \"color\": \"default\",\n          \"children\":[]\n        }\n      }\n    ]\n  }\n\n  // this block adds my daily todos from an array\n  dailyTodos.forEach(todo =&gt; {\n    params.children[2].toggle.children.push({\n      \"type\": \"to_do\",\n      \"to_do\": {\n        \"rich_text\": [{\n          \"type\": \"text\",\n          \"text\": {\n            \"content\": todo,\n            \"link\": null\n          }\n        }],\n        \"checked\": false,\n        \"color\": \"default\"\n      }\n    })\n  })\n\n  // and add the inspirational quote\n  let quote = Quote.getQuote()\n  params.children.push({\n    \"type\": \"quote\",\n    \"quote\": {\n      \"rich_text\": [{\n        \"type\": \"text\",\n        \"text\": {\n          \"content\": quote.text + \"\\n•\" + quote.author,\n        },\n      }],\n      \"color\": \"default\"\n    }\n  })\n\n  await notion.blocks.children.append(params)\n\n  return {\n    statusCode: 200,\n  }\n}</code></pre><p>And I’m using a GitHub Action workflow with a cron trigger to do this once a day.</p><pre class=\"language-yaml\"><code>on:\n  workflow_dispatch:\n  schedule:\n    - cron:  '0 4 * * 1-5'\n\njobs:\n  execute:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Execute\n      uses: fjogeleit/http-request-action@v1\n      with:\n        url: '${{ secrets.NETLIFY_URL }}/.netlify/functions/add-daily-note-section'\n        method: 'GET'</code></pre><h2>Import tasks into a separate db</h2><p>One thing I really wanted was a way to scan through the list of to dos at the top level and automatically create those entries in a separate Notion database for me to plan. I initially tried to do this in another serverless function, but it took longer than the allotted 10 seconds for it, so I instead decided to put it in a container on my Kubernetes server. In any case, using the <code>node-cron</code> library, I could essentially schedule it to happen once per day as well. This would help me setup for my daily review as soon as I sit down at the computer:</p><pre class=\"language-javascript\"><code>import * as dotenv from 'dotenv'\nimport * as cron from 'node-cron'\nimport { Client } from \"@notionhq/client\";\n\ndotenv.config()\n\nasync function sleep(ms) {\n  return new Promise(resolve =&gt; setTimeout(resolve, ms));\n}\n\nasync function getLatestNote() {\n  let notion = new Client({\n    auth: process.env.NOTION_KEY\n  })\n\n  let params = {\n    database_id: process.env.NOTES_DBID,\n    sorts: [\n      {\n        property: 'Date',\n        direction: 'descending'\n      }\n    ]\n  }\n  let res = await notion.databases.query(params)\n\n  return res.results[0]\n}\n\nexport async function syncDailyNotes (event, context) {\n  try {\n    // get the latest note in the db\n    let latestNote = await getLatestNote()\n\n    let notion = new Client({\n      auth: process.env.NOTION_KEY\n    })\n\n    // get all the blocks on that page\n    let children = await notion.blocks.children.list({\n      block_id: latestNote.id,\n      page_size: 100\n    })\n\n    // flatten the list of blocks into only the to dos\n    let todos = children.results.filter(c =&gt; c.type === 'to_do')\n    let flattened = []\n    todos.forEach(t =&gt; {\n      if(t.to_do.rich_text[0].type !== \"mention\") {\n        let f = {\n          id: t.id,\n          text: t.to_do.rich_text[0].plain_text\n        }\n        flattened.push(f)\n      }\n    })\n\n    console.log(flattened.length)\n\n    // loop over the list and create the entry in the tasks database\n    if(flattened.length) {\n      for (let i = 0; i &lt; flattened.length; i++) {\n        let t = flattened[i]\n        console.log('processing', t.text)\n\n        // Create task\n        let page = await notion.pages.create({\n          parent: {\n            type: \"database_id\",\n            database_id: process.env.TASKS_DBID\n          },\n          \"properties\": {\n            \"Name\": {\n              \"title\": [\n                {\n                  \"text\": {\n                      \"content\": t.text\n                  }\n                }\n              ]\n            }\n          }\n        })\n\n        // also, update the block in the notes page to be a backlink to the todo entry\n        // this allows me to easily jump right to that task if I want to do anything about it.\n        await notion.blocks.update({\n          block_id: t.id,\n          to_do: {\n            type: \"to_do\",\n            rich_text: [\n                {\n                  \"type\": \"mention\",\n                  \"mention\": {\n                      \"type\": \"page\",\n                      \"page\": {\n                          \"id\": page.id\n                      }\n                  },\n                  \"annotations\": {\n                      \"bold\": false,\n                      \"italic\": false,\n                      \"strikethrough\": false,\n                      \"underline\": false,\n                      \"code\": false,\n                      \"color\": \"default\"\n                  },\n                  \"href\": \"https://www.notion.so/\" + page.id.replace(/-/g, \"\")\n              }\n            ]\n          }\n        })\n        await sleep(3000)\n      }\n    }\n  } catch (err) {\n    console.error('drstrange', err);\n  }\n}\n\ncron.schedule('0 0 * * 1-5', syncDailyNotes)\n\n</code></pre><p>The result is that each top-level todo in my notes page ends up having a backlink to the task in the Tasks database.</p><figure><img src=\"/img/n/my-notion-note-taking-automations/22e902a2-6945-4448-b8e8-7e5c583a1ba7-Untitled.png\" /><figcaption></figcaption></figure><p>This project is <a href=\"https://github.com/bmorrisondev/bmo-cloud\" target=\"_blank\">open source</a>, so feel free to use the code however you want for your use case. Next up I think I’m going to automatically update the “Done” property in the Tasks db entry if the to do on the notes page is checked. I love working with the Notion API because the options feel like they are endless!</p></div>","featuredImage":"/img/n/my-notion-note-taking-automations/cec39acc-e3cb-4b35-b7a7-820ace2ed79f-featured-images.001.jpeg","icon":"/img/n/my-notion-note-taking-automations/02d14448-4d5f-412d-b732-fb332f0ab682-Notion_app_logo.png","excerpt":"After attending THAT conference a few weeks ago, I sat in a group that discussed ongoing journalling. I decided to re...","cachedOn":1681394630},{"id":"854e38ce-32ad-4f47-bbcc-2fb7e6b59e77","notion_id":"854e38ce-32ad-4f47-bbcc-2fb7e6b59e77","status":"Published","slug":"deploy-to-netlify-with-github-actions","relation_series":[],"publishOn":"2023-04-04T00:00:00.000Z","codeURL":null,"seriesOrder":null,"category":"devops","youTubeURL":null,"title":"Deploy to Netlify with GitHub Actions","html":"<div><p>Some time ago, I decided to put a tools directory in the source repo for my website. I didn’t really want to let Netlify redeploy my site each time I modified a script in there, and I already knew how to deploy to Netlify manually, so I figure why not let GitHub Actions handle it for me? I could write my own workflows to publish on my own terms and have much more control over the process. Here’s how it works.</p><h2>Create PAT and get the Site ID</h2><p>Before you can build a workflow, you’ll need some info from Netlify. The first thing is a personal access token that is used to authenticate with the service. To get this, log into Netlify, click your <strong>profile</strong> in the upper right and go to <strong>User settings</strong>.</p><figure><img src=\"/img/n/deploy-to-netlify-with-github-actions/f91c9802-70e9-4543-ad8f-dca560ae2dc1-Untitled.png\" /><figcaption></figcaption></figure><p></p><p>Next go to <strong>Settings</strong>, then <strong>Applications</strong>. Click <strong>New access token</strong>.</p><figure><img src=\"/img/n/deploy-to-netlify-with-github-actions/cfaef7be-94c2-4211-9739-72545800d6b4-Untitled.png\" /><figcaption></figcaption></figure><p>Finally, give your token a name and click <strong>Generate token</strong>. You’ll be presented with the token. Copy it before you leave this screen as you wont be able to see it again.</p><figure><img src=\"/img/n/deploy-to-netlify-with-github-actions/fb54fb6d-d087-4129-a9eb-89ac54f3daa1-image.png\" /><figcaption></figcaption></figure><p>Next you’ll need to grab the Site ID of the site you want to deploy. This assumes you already have a site configured. To get it, open the site and find the <strong>Site settings</strong> tab. The Site ID will be on the first screen in the <strong>Site information</strong> section. Copy this value for later.</p><figure><img src=\"/img/n/deploy-to-netlify-with-github-actions/765654e9-740a-4cea-a9ef-befb96688b91-Untitled.png\" /><figcaption></figcaption></figure><h2>Setup actions secrets</h2><p>Next up, we need to add the two values above into the repositories Action secrets. This prevents you from having to save them into the code of the repo. To do this, open the repo in GitHub. Then click <strong>Settings</strong>, then <strong>Secrets and variables</strong>, then <strong>Actions</strong>.</p><p>In that view, click <strong>New repository secret</strong>.</p><figure><img src=\"/img/n/deploy-to-netlify-with-github-actions/cda6d7fa-62a2-4d87-8a83-d4d029c1aad8-Untitled.png\" /><figcaption></figcaption></figure><p>You’ll be adding two secrets:</p><ul><li><code>NETLIFY_AUTH_TOKEN</code> The personal access token you created earlier.</li><li><code>NETLIFY_SITE_ID</code> The Site ID from earlier.</li></ul><h2>Build the action</h2><p>To create the action, create a new file in your repo named <code>.github/workflows/deploy-prod.yaml</code> and add the following contents:</p><pre class=\"language-yaml\"><code>name: Deploy prod\n\non:\n  workflow_dispatch:\n  push:\n    branches:\n      - main\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-node@v3\n        with:\n          node-version: 18\n      - name: Install Netlify CLI\n        run: npm install -g netlify-cli\n      - name: Install deps\n        run: npm install\n      - name: Deploy\n        run: npm run deploy:prod\n        env:\n          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n          NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n</code></pre><p>This action will build and deploy the production version of your website whenever any commits are pushed to the <code>main</code>branch of your repo. This will occur on standard pushes or when pull requests are merged if you prefer that flow.</p><p>You can also configure a second action to publish to an alternate branch. For instance I also have <code>.github/workflows/deploy-dev.yaml</code> to deploy changes made to the <code>dev</code> branch into Netlify branch URL. This allows me to test my changes before I merge changes into the production branch:</p><pre class=\"language-yaml\"><code>name: Deploy dev\n\non:\n  workflow_dispatch:\n  push:\n    branches:\n      - dev\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-node@v3\n        with:\n          node-version: 18\n      - name: Install Netlify CLI\n        run: npm install -g netlify-cli\n      - name: Install deps\n        run: npm install\n      - name: Build\n        run: netlify build\n        env:\n          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n          NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n      - name: Deploy\n        run: netlify deploy --alias dev # this adds a 'dev--' prefix to my URL for testing\n        env:\n          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n          NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n</code></pre><p>Notice how much of the workflow is the same, with the exception of the branch trigger and the <code>--alias</code> set on the Deploy step.</p></div>","featuredImage":"/img/n/deploy-to-netlify-with-github-actions/b116b39a-a7ae-4991-9f50-63d5fbbce1f0-Thumbnails.001.jpeg","icon":"/img/n/deploy-to-netlify-with-github-actions/b81d712f-b022-4b58-aba0-153b3bcc11d5-github-icon-1.svg","excerpt":"Some time ago, I decided to put a tools directory in the source repo for my website. I didn’t really want to let Netl...","cachedOn":1681394631},{"id":"cb54a6b1-7cd5-4e89-bca1-75b56babb405","notion_id":"cb54a6b1-7cd5-4e89-bca1-75b56babb405","status":"Published","slug":"how-i-cross-post-to-hashnode-and-devto","relation_series":[],"publishOn":"2023-04-11T00:00:00.000Z","codeURL":null,"seriesOrder":null,"category":"automation","youTubeURL":null,"title":"How I cross-post to Hashnode and Dev.to","html":"<div><div class=\"callout\"><div class=\"callout-icon\">⚠️</div><div class=\"callout-content\">So I wrote this before I moved my content into Notion from WordPress. Expect an updated version at some point in the future once I sort out how I’ll do the same thing with Notion as the data source. That’s not to say it doesn’t work right now, it will just change a bit going forward.</div></div><p>So cross posting my content is something I’ve wanted to do for some time. I know Hashnode and <a href=\"http://Dev.to\" target=\"_blank\">Dev.to</a> are two of the best places for developers to write, but at the same time I like to own my content, which is why I store all of my content in my own WordPress instance.</p><p>Well today I took some time to play a bit with the code to post to Hashnode, and next thing I know I’ve got a fully working tool to publish content from my blog to to both of the above platforms. </p><p>As a TLDR on the process, here’s how it works:</p><ul><li>Grab the latest posts from WordPress from its API</li><li>Get the posts that published in the previous 24 hours</li><li>Convert the content from HTML to Markdown</li><li>Publish them to Hashnode & <a href=\"http://Dev.to\" target=\"_blank\">Dev.to</a> via their APIs</li></ul><h2>Fetching from WordPress</h2><p>The first step is pulling the posts from WordPress. I’m doing this through the REST API available with all WordPress installations. The <code>GetLatestPosts</code> hits the default <code>/posts</code> endpoint which returns 10 posts by default.</p><pre class=\"language-go\"><code>func GetLatestPosts() []Post {\n\t// Craft the request\n\turl := fmt.Sprintf(\"%v/wp-json/wp/v2/posts\", os.Getenv(\"WP_URL\"))\n\treq, err := http.NewRequest(\"GET\", url, nil)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Set the basic auth header\n\tauth := os.Getenv(\"WP_USERNAME\") + \":\" + os.Getenv(\"WP_PASSWORD\")\n\tauthHeader := b64.StdEncoding.EncodeToString([]byte(auth))\n\treq.Header.Add(\"Authorization\", fmt.Sprintf(\"Basic %v\", authHeader))\n\n\t// Execute the request\n\tc := http.Client{}\n\tres, err := c.Do(req)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer res.Body.Close()\n\n\t// Parse the JSON repsonse into a struct\n\tbbytes, err := ioutil.ReadAll(res.Body)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tvar posts []Post\n\terr = json.Unmarshal(bbytes, &posts)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\treturn posts\n}</code></pre><p></p><p>I also created a custom JSON parser for the <code>date_gmt</code> field on the <code>Post</code> type. This parses the response from the WordPress API into <code>time.Time</code> which I can work with easier in go, which I’ll demo in a sec.</p><pre class=\"language-go\"><code>func (t *CustomTime) UnmarshalJSON(b []byte) (err error) {\n\tdate, err := time.Parse(`\"2006-01-02T15:04:05\"`, string(b))\n\tif err != nil {\n\t\treturn err\n\t}\n\tt.Time = date\n\treturn\n}\n\ntype Post struct {\n\tID int `json:\"id\"`\n\tDateGmt CustomTime `json:\"date_gmt\"`\n\tSlug   string `json:\"slug\"`\n\tStatus string `json:\"status\"`\n\tTitle   Title   `json:\"title\"`\n\tContent Content `json:\"content\"`\n\tExcerpt Excerpt `json:\"excerpt\"`\n\tJetpackFeaturedMediaURL string `json:\"jetpack_featured_media_url\"`\n}</code></pre><p></p><p>This is all handled in the <code>main</code> function of <code>main.go</code>.</p><pre class=\"language-go\"><code>posts := wordpress.GetLatestPosts()\nfiltered := []wordpress.Post{}\npast24hrs := time.Now().Add(time.Hour * 24 * -1)\nfor _, el := range posts {\n\tif el.DateGmt.Time.Unix() &gt; past24hrs.Unix() {\n\t\tfiltered = append(filtered, el)\n\t}\n}</code></pre><h2>Converting to markdown</h2><p>Now the next thing that I needed to do is convert the posts <code>Content.Rendered</code> field into markdown. WordPress returns HTML in that property and both the Hashnode and <a href=\"http://dev.to\" target=\"_blank\">dev.to</a> API’s require markdown. I did this with a function on the <code>Post</code> struct:</p><pre class=\"language-go\"><code>func (p *Post) MarkdownBody() string {\n\tconverter := md.NewConverter(\"\", true, nil)\n\tconverter.Use(plugin.GitHubFlavored())\n\tmarkdown, err := converter.ConvertString(p.Content.Rendered)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tspl := strings.Split(markdown, \"\\n\\n\")\n\trebuilt := \"\"\n\tfor _, el := range spl {\n\t\tspl2 := strings.Split(el, \"\\n\")\n\t\tif len(spl2) &gt; 0 {\n\t\t\tfor _, el2 := range spl2 {\n\t\t\t\trebuilt += el2 + \"\\r\"\n\t\t\t}\n\t\t} else {\n\t\t\trebuilt += el\n\t\t\trebuilt += \"\\r\\r\"\n\t\t}\n\t}\n\treturn rebuilt\n}</code></pre><p>Now here’s the thing, I honestly have no idea why a simple <code>strings.Replace</code> function wouldn’t properly fix the newlines in the returned string from the markdown library I’m using. I had to split the string by newlines and loop over it to properly set it up. I really feel this function can be improved but it works so 🤷. </p><div class=\"callout\"><div class=\"callout-icon\">👉</div><div class=\"callout-content\"><strong>Feel free to reach out if you have a suggestion!</strong></div></div><h2>Publishing to Hashnode & Dev.to</h2><p>Finally, in the <code>main</code> function, I can call functions setup for each service to publish my WordPress post to their platforms. Here’s the Hashnode version which uses a GraphQL mutation:</p><pre class=\"language-go\"><code>func PublishPost(post wordpress.Post) {\n\tbaseQuery := `\n\t\tmutation {\n\t\t\tcreateStory(input: {\n\t\t\t\ttitle: \"%v\"\n\t\t\t\tisPartOfPublication:{\n\t\t\t\t\tpublicationId: \"%v\"\n\t\t\t\t}\n\t\t\t\tcontentMarkdown: \"%v\"\n\t\t\t\ttags:[]\n\t\t\t\tcoverImageURL: \"%v\"\n\t\t\t\tisRepublished: {\n\t\t\t\t\toriginalArticleURL: \"%v\"\n\t\t\t\t}\n\t\t\t}) {\n\t\t\t\tpost {\n\t\t\t\t\ttitle\n\t\t\t\t\tdateAdded\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t`\n\n\toriginalUrl := fmt.Sprintf(\"https://brianmorrison.me/blog/%v\", post.Slug)\n\tquery := fmt.Sprintf(baseQuery,\n\t\tpost.Title.Rendered,\n\t\tos.Getenv(\"HASHNODE_PUB_ID\"),\n\t\tpost.MarkdownBody(),\n\t\tpost.JetpackFeaturedMediaURL,\n\t\toriginalUrl)\n\n\tclient := graphql.NewClient(\"https://api.hashnode.com\")\n\trequest := graphql.NewRequest(query)\n\trequest.Header.Add(\"Authorization\", os.Getenv(\"HASHNODE_KEY\"))\n\tvar response interface{}\n\terr := client.Run(context.Background(), request, &response)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n}</code></pre><p>The <code>HASHNODE_PUB_ID</code> is the ID of my blog on Hashnode, and the <code>HASHNODE_KEY</code> was the API key generated in my settings. Here is the same code for Dev.to’s REST API:</p><pre class=\"language-go\"><code>func PublishPost(post wordpress.Post) {\n\tbody := PublishPostRequestBody{\n\t\tArticle: Article{\n\t\t\tTitle:        post.Title.Rendered,\n\t\t\tPublished:    true,\n\t\t\tBodyMarkdown: post.MarkdownBody(),\n\t\t\tMainImage:    post.JetpackFeaturedMediaURL,\n\t\t\tCanonicalUrl: post.CanonicalUrl(),\n\t\t},\n\t}\n\tjsonData, err := json.Marshal(body)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\treq, err := http.NewRequest(\"POST\", \"https://dev.to/api/articles\", bytes.NewBuffer(jsonData))\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\treq.Header.Add(\"api-key\", os.Getenv(\"DEV_TO_KEY\"))\n\treq.Header.Set(\"Content-Type\", \"application/json; charset=UTF-8\")\n\n\tc := http.Client{}\n\tres, err := c.Do(req)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer res.Body.Close()\n\n\tbbytes, err := ioutil.ReadAll(res.Body)\n\tif err != nil {\n\t\tlog.Panic(err)\n\t}\n\tlog.Println(string(bbytes))\n\n\tlog.Println(\"done!\")\n}</code></pre><h2>What’s next</h2><p>Next thing I want to do with this is deploy it to a container on my Kubernetes server and configure it to run every hour or so to look for posts published in the previous hour. This should automatically deploy my newest posts without needing me to do it manually. I also might configure it as an endpoint for WordPress webhooks to handle things like updates if they are ever needed!</p><p></p></div>","featuredImage":"/img/n/how-i-cross-post-to-hashnode-and-devto/5fd5393d-f2bd-438c-a8c6-d961e2b4174e-Thumbnails.001.jpeg","icon":"/img/n/how-i-cross-post-to-hashnode-and-devto/b774e590-e398-4e58-aede-3a23e38fab3f-264-2641184_111-kb-png-golang-logo.png","excerpt":"So cross posting my content is something I’ve wanted to do for some time. I know Hashnode and  Dev.to  are two of the...","cachedOn":1681394634}]