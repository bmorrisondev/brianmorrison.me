[{"id":"23ad43db-82db-808b-889b-d98cabaaeb75","notion_id":"23ad43db-82db-808b-889b-d98cabaaeb75","status":"Published","date":"2023-05-17T00:00:00.000Z","relation_job":["0978e1d3-dc41-4f83-b574-2311046bde51"],"relation_skillsUsed":["9548736b-828b-4434-984c-d60e40f6ad38","23ad43db-82db-80a2-931d-c9e4b4a48db4","23ad43db-82db-80d0-b9df-c7bd17722856"],"tags":[],"excerpt":"Jumped in on short notice to support Cloudflare’s PlanetScale integration launch and built a content package to drive adoption.","title":"Rapid Launch Support for Cloudflare Integration","slug":"rapid-launch-support-for-cloudflare-integration","html":"<div><p>Due to a last-minute miscommunication across multiple teams, we discovered that <strong>Cloudflare</strong> had gone live with a public <strong>PlanetScale</strong> integration—without any supporting marketing assets prepared on our side to help promote or explain it to users.</p><p>To close the gap quickly, I collaborated with our engineering team to thoroughly understand and test the integration. I then created several user-facing resources in rapid succession, including a <a href=\"https://github.com/planetscale/cloudflare-workers-quickstart\" target=\"_blank\"><strong>demo app starter repo</strong></a> to help developers get started with <strong>Cloudflare Workers</strong> and <strong>PlanetScale</strong>, and a new <a href=\"https://planetscale.com/docs/vitess/integrations/cloudflare-workers\" target=\"_blank\"><strong>integration guide in our docs</strong></a> to ensure comprehensive developer onboarding.</p><p>To support awareness and adoption, I also wrote a detailed <a href=\"https://planetscale.com/blog/integrate-cloudflare-workers-with-planetscale\" target=\"_blank\"><strong>blog post</strong></a> explaining the integration’s use cases and benefits, and produced a <a href=\"https://www.youtube.com/watch?v=K21jb_yv33Y\" target=\"_blank\"><strong>companion YouTube video</strong></a> to engage developers who prefer visual learning.</p><p>Despite the short notice, these efforts enabled a coordinated content push that significantly improved the integration’s visibility, provided immediate value to developers, and positioned PlanetScale as a ready and reliable partner in the Cloudflare ecosystem.</p><p></p></div>","cachedOn":1753393036},{"id":"237d43db-82db-80f9-b263-c62fc9965c12","notion_id":"237d43db-82db-80f9-b263-c62fc9965c12","status":"Published","date":"2025-07-21T00:00:00.000Z","relation_job":["237d43db-82db-80aa-8a81-cec335b2c3a1"],"relation_skillsUsed":["d40e5fe1-48a6-44f0-9db4-b3014cc6d0be","869e0ad5-a39a-43b6-85c7-e51a46461961"],"tags":[],"excerpt":"Automate the production of our bi-monthly newsletter using GitHub Actions to generate artifacts to upload to our platform, Loops.","title":"Streamlining newsletter production","slug":"streamlining-newsletter-production","html":"<div><h2>Problem</h2><p>Our original newsletter workflow was time-consuming and error-prone. It involved drafting and reviewing content manually in <strong>Notion</strong>, reformatting it for <strong>Loops</strong>, generating custom attribution links for all <strong>Clerk</strong> URLs, and finally uploading the content to <a href=\"http://dev.to/\" target=\"_blank\"><strong>dev.to</strong></a> as a secondary distribution channel.</p><h2>Solution</h2><p>To eliminate these inefficiencies, I partnered with the design and engineering teams to develop a reusable <strong>MJML</strong> email template that could be programmatically populated using an <strong>MDX</strong> file. I also launched a new section on our <strong>Next.js</strong> website to host each newsletter as a standalone page, improving long-term <strong>SEO</strong> value and content discoverability.</p><p>To automate the publishing pipeline, I built a custom <strong>GitHub Action</strong> that compiles the newsletter, integrates with the <a href=\"http://dub.sh/\" target=\"_blank\"><strong>dub.sh</strong></a> API to generate and insert attribution links, and creates the final artifact ready for upload to <strong>Loops</strong>. As a final step, I brought newsletter reviews in line with our broader content workflow by enabling <strong>Vercel Preview Comments</strong>, allowing stakeholders to leave feedback directly on preview deployments.</p><p>This system significantly reduced the effort involved in publishing the newsletter, ensured consistency across platforms, and increased the visibility of our content through organic search.</p></div>","cachedOn":1753129886},{"id":"fbd2ef2c-7876-434c-8caf-aeb24b226327","notion_id":"fbd2ef2c-7876-434c-8caf-aeb24b226327","status":"Not started","relation_job":[],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"TCP integration server","slug":"tcp-integration-server","html":"<div></div>","cachedOn":1689879587},{"id":"0928a8bf-589d-4504-a6d5-a3b95a250b01","notion_id":"0928a8bf-589d-4504-a6d5-a3b95a250b01","status":"Not started","relation_job":[],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"Revamped reporting engine","slug":"revamped-reporting-engine","html":"<div></div>","cachedOn":1689879611},{"id":"41a6e0cc-cfac-4471-b0e1-1dbe354a226d","notion_id":"41a6e0cc-cfac-4471-b0e1-1dbe354a226d","status":"Not started","relation_job":[],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"Test suite and reporting","slug":"test-suite-and-reporting","html":"<div></div>","cachedOn":1689879646},{"id":"d5d7f6f3-079d-41d3-9986-b78838dfc5b7","notion_id":"d5d7f6f3-079d-41d3-9986-b78838dfc5b7","status":"Not started","relation_job":[],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"API logging optimizations","slug":"api-logging-optimizations","html":"<div></div>","cachedOn":1687890004},{"id":"6361f723-a40a-42c6-8d87-bead13f304a0","notion_id":"6361f723-a40a-42c6-8d87-bead13f304a0","status":"Not started","relation_job":[],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"Webhook system","slug":"webhook-system","html":"<div></div>","cachedOn":1687890009},{"id":"0ed8dcb3-2de4-4b1b-a757-f32fc423f243","notion_id":"0ed8dcb3-2de4-4b1b-a757-f32fc423f243","status":"Published","date":"2023-02-01T00:00:00.000Z","relation_job":["0978e1d3-dc41-4f83-b574-2311046bde51"],"relation_skillsUsed":["23bd43db-82db-80ba-b289-f925a01d39c2"],"tags":[],"excerpt":"Mapped PlanetScale features to each DevOps phase with docs, examples, and a narrative use case.Ask ChatGPT","title":"PlanetScale DevOps Documentation section","slug":"planetscale-devops-documentation-section","html":"<div><p>Although <strong>PlanetScale</strong> was built with <strong>CI/CD workflows</strong> in mind, we lacked clear guidance on how its features mapped to the standard eight phases of the <strong>DevOps lifecycle</strong>. Developers unfamiliar with our toolset needed help understanding where PlanetScale fit into their existing workflows.</p><p>To bridge this gap, I used my prior experience building CI/CD pipelines and created a new section of our documentation portal that introduced each of the eight DevOps phases—<strong>Plan, Develop, Build, Test, Release, Deploy, Operate, and Monitor</strong>. Each page explained the purpose of the phase, highlighted how PlanetScale could be used during that stage, and outlined the benefits of doing so.</p><p>To bring the concept to life, I also wrote a fictional narrative illustrating how a modern engineering team might use PlanetScale alongside other tools across the entire DevOps process. This helped contextualize technical features in a more engaging and relatable way.</p><p>While the original long-form content has since been condensed and migrated to the blog, the core ideas remain accessible in a more concise format: <a href=\"https://planetscale.com/blog/the-eight-phases-of-devops\" target=\"_blank\"><strong>The Eight Phases of DevOps</strong></a>.</p></div>","cachedOn":1753454689},{"id":"28731d13-9e43-4ecb-aaed-2bbaf59e8844","notion_id":"28731d13-9e43-4ecb-aaed-2bbaf59e8844","status":"Published","date":"2023-04-01T00:00:00.000Z","relation_job":["0978e1d3-dc41-4f83-b574-2311046bde51"],"relation_skillsUsed":["95183242-3bf2-4fa7-b724-4e97da6c7b3a","d40e5fe1-48a6-44f0-9db4-b3014cc6d0be","5a039124-c30c-4fcd-9c00-4befd1726f84"],"tags":[],"excerpt":"Created the first set of Official PlanetScale GitHub Actions using Docker and Bash.","title":"Official GitHub Actions","slug":"official-github-actions","html":"<div><p>As I went deep into how to use DevOps with PlanetScale, I really wanted to demonstrate how PlanetScale can be used in CI environments so I got the idea to create and publish the first wave of official Actions that our customers could use.</p><figure><img src=\"/img/n/official-github-actions/3aeec30b-5b37-45bb-9c6c-a380c959f2ee-CleanShot_2023-06-27_at_15.57.37.png\" /><figcaption></figcaption></figure><p>After researching <em>how</em> to create Actions that can be published to the marketplace, I opted to use Docker since already have the PlanetScale CLI published as a Docker image on Docker Hub. I used some previously created helper scripts built in Bash that have some helpful utilities and essentially merged the two together. </p><p>The <code>dockerfile</code> is built using the <code>planetscale/pscale</code> image as its base, and uses <code>curl</code> to pull in the pinned version of the helper scripts mentioned above.</p><pre class=\"language-plain text\"><code># Container image that runs your code\nFROM planetscale/pscale:v0.129.0\n\nRUN apk --no-cache add bash jq curl\n\n# Copies your code file from your action repository to the filesystem path `/` of the container\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x entrypoint.sh\n\n# Install the PlanetScale Actions helpers\nCOPY install-helpers.sh /install-helpers.sh\nRUN chmod +x /install-helpers.sh\nRUN /install-helpers.sh\n\n# Code file to execute when the docker container starts up (`entrypoint.sh`)\nENTRYPOINT [\"/entrypoint.sh\"]</code></pre><p></p><p>A service token and parameters are passed into the container for the Action step to perform whatever operation its intended to do. For instance, this will create a new branch of a PlanetScale database: </p><pre class=\"language-bash\"><code>#!/bin/bash\n\ncreate_branch=true\ncommand=\"pscale branch create $1 $2 --org $3\"\n\nif [ -n \"$4\" ];then\n  command=\"$command --from $4\"\nfi\n\nif [ -n \"$5\" ];then\n  command=\"$command --restore $5\"\nfi\n\nif [ -n \"$6\" ];then\n  command=\"$command --region $6\"\nfi\n\n# Check if branch already exists\nif [ \"true\" == \"$8\" ];then\n  output=$(eval \"pscale branch show $1 $2 --org $3\" 2&gt;&1)\n  exit_status=$?\n  if [ $exit_status -ne 0 ]; then\n    create_branch=true\n  else\n    create_branch=false\n  fi\nfi\n\nif $create_branch; then\n  eval $command\n\n  ret=$?\n  if [ $ret -ne 0 ]; then\n    exit $ret\n  fi\n\n  if [ \"true\" == \"$7\" ];then\n    . /.pscale/cli-helper-scripts/wait-for-branch-readiness.sh\n    wait_for_branch_readiness 40 \"$1\" \"$2\" \"$3\" 5\n  fi\nelse\n  echo \"Branch $2 already exists\"\nfi</code></pre><p></p><p>Additional resources:</p><ul><li><a href=\"https://github.com/planetscale/create-branch-action\" target=\"_blank\">https://github.com/planetscale/create-branch-action</a></li><li><a href=\"https://planetscale.com/blog/announcing-the-planetscale-github-actions\" target=\"_blank\">https://planetscale.com/blog/announcing-the-planetscale-github-actions</a></li></div>","cachedOn":1687896114},{"id":"5182b74b-a962-4c8a-8e25-394adb2dd064","notion_id":"5182b74b-a962-4c8a-8e25-394adb2dd064","status":"Not started","date":"2022-09-01T00:00:00.000Z","relation_job":["0978e1d3-dc41-4f83-b574-2311046bde51"],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"Revamped onboarding docs","slug":"revamped-onboarding-docs","html":"<div></div>","cachedOn":1687813102},{"id":"1dc9592d-1bfd-4663-a9ce-b7946db381c9","notion_id":"1dc9592d-1bfd-4663-a9ce-b7946db381c9","status":"Not started","date":"2019-01-01T00:00:00.000Z","relation_job":["0ba85558-e569-4bec-a3b3-0db8d502bb70"],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"Onboarding system","slug":"onboarding-system","html":"<div></div>","cachedOn":1687890015},{"id":"51b41b72-e5ae-488a-845b-ece712773014","notion_id":"51b41b72-e5ae-488a-845b-ece712773014","status":"Not started","date":"2018-01-01T00:00:00.000Z","relation_job":["0ba85558-e569-4bec-a3b3-0db8d502bb70"],"relation_skillsUsed":["1be18392-7698-48dd-a162-fe2a1cfc349b","7e6c1091-5d2a-4c38-9ae9-529265bc4b6d","33998813-f859-46dd-a7bb-8f53aa662b3f","f9d966c2-e161-4e01-93c4-89bdff554b14","f5da23b7-f4b8-4896-b1ae-59f48fc6098b","a75285d4-98a5-44fa-a20c-0cf4d04e919d"],"tags":[],"excerpt":"","title":"Customer portal","slug":"customer-portal","html":"<div></div>","cachedOn":1687813112},{"id":"0dea3ab8-bb37-460f-9a30-d2f011642623","notion_id":"0dea3ab8-bb37-460f-9a30-d2f011642623","status":"Published","date":"2016-01-01T00:00:00.000Z","relation_job":["0ba85558-e569-4bec-a3b3-0db8d502bb70"],"relation_skillsUsed":["17360d1c-70e8-4633-89ec-d4281d96adc2","d7587d2d-6903-45dc-ac90-27f0c23f38db","c167351a-ca27-45d2-a9e4-40cd6600d735","1be18392-7698-48dd-a162-fe2a1cfc349b","7e6c1091-5d2a-4c38-9ae9-529265bc4b6d","f5da23b7-f4b8-4896-b1ae-59f48fc6098b","a75285d4-98a5-44fa-a20c-0cf4d04e919d"],"tags":[],"excerpt":"A Windows OS update service with a command & control server for scheduling maintenance windows.","title":"Windows Server upgrade utility","slug":"windows-server-upgrade-utility","html":"<div><p>When I joined LeSaint Logistics in 2015, one of the pain points of the IT Operations department was managing and deploying Windows Updates to all of the servers in a manageable way. I first attempted to deploy a WSUS server but was met with inconsistent results with minimal control for automating server update maintenance windows. So I designed a system where we could specify a maintenance window per server and an agent on each server would download updates, install them, and report back to a central control server on the progress of the updates. Here are what went into each component built in this system.</p><h3>The agent</h3><p>Each server was a Windows service built with C# that would check in with a central server and listen for maintenance windows configured on that server. During a configured maintenance window, the agent would do the following:</p><ol><li>Check for updates and download packages.</li><li>Install the update packages.</li><li>Reboot the servers as needed.</li><li>Repeat the above loop until no packages were available or the maintenance window would close.</li><li>Report the logs and status back to the central control server.</li></ol><p>Group Policy was configured for the servers to deploy the agent using a PowerShell script. </p><p></p><h3>The server</h3><p>There was a single central control server deployed to AWS EC2 running Ubuntu that the agents would communicate with. It was a simple website built with NodeJS, Vue, and MongoDB. The Vue front end would be used by the team to configure a maintenance window for the servers that would be a time range configured for each server. An API was available for the agents to check in with. </p><p></p><h2>The result</h2><p>Once this project was deployed, techs no longer had to manually install updates over a long 4-8 hour maintenance window and users within the organization would be able to resume operations since the updates would install across all servers at the same time with little manual intervention.</p><p></p></div>","cachedOn":1687813524},{"id":"b05f66c4-b037-4198-9b74-bc287c28acf9","notion_id":"b05f66c4-b037-4198-9b74-bc287c28acf9","status":"Not started","date":"2021-09-01T00:00:00.000Z","relation_job":["b6f6ed05-a879-43b2-b76c-e3f5df82dda4"],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"Docker DevOps pipeline","slug":"docker-devops-pipeline","html":"<div></div>","cachedOn":1687813117},{"id":"e2d61cde-df43-4ccf-8618-5ddd42fe8278","notion_id":"e2d61cde-df43-4ccf-8618-5ddd42fe8278","status":"Not started","date":"2021-03-01T00:00:00.000Z","relation_job":["b6f6ed05-a879-43b2-b76c-e3f5df82dda4"],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"Multi-account switching","slug":"multi-account-switching","html":"<div></div>","cachedOn":1687813117},{"id":"5eb72b62-92bf-4d80-9c8d-f047d5996f8f","notion_id":"5eb72b62-92bf-4d80-9c8d-f047d5996f8f","status":"Not started","date":"2019-12-01T00:00:00.000Z","relation_job":["b6f6ed05-a879-43b2-b76c-e3f5df82dda4"],"relation_skillsUsed":[],"tags":[],"excerpt":"","title":"Inspections","slug":"inspections","html":"<div></div>","cachedOn":1687813120},{"id":"8abb41f7-483e-4ef5-b839-a43d7dd35bc2","notion_id":"8abb41f7-483e-4ef5-b839-a43d7dd35bc2","status":"Published","date":"2021-09-01T00:00:00.000Z","relation_job":["b6f6ed05-a879-43b2-b76c-e3f5df82dda4"],"relation_skillsUsed":["fca0627e-d36b-4b04-a0ed-6c2914690f79","c51aefc2-10ef-4234-9f3a-a7d1356863fc","f9d966c2-e161-4e01-93c4-89bdff554b14","ef876a6c-cf7e-4f7e-9fd7-786e6427bb3a","0e65faed-1b45-47c3-84c6-7de4e6bbd61d","17360d1c-70e8-4633-89ec-d4281d96adc2","95183242-3bf2-4fa7-b724-4e97da6c7b3a","39268ffa-fc14-487f-85f5-815bedd217bf","5f17526f-7f0d-4501-8d86-20e5dd3fd0e9"],"tags":[],"excerpt":"A GraphQL API that enables the application to dynamically keep groups of assets updated on a set interval.","title":"GraphQL API & dynamic groups","slug":"graphql-api-and-dynamic-groups","html":"<div><h2>The problem</h2><p>At Temeda, there was the concept of groups of assets. An asset was essentially a device in the field that we tracked, and a group was a rigid, manually populated group of those assets. The request was to create a system to make these groups more dynamic. This way if a customer wanted to make a group based off of an attribute of an asset (think Make, Model, Year, etc.) they could do so. This would enable more dynamic reporting and decrease the manual effort of maintaining groups. These were aptly named “Dynamic groups.”</p><h2>The solution</h2><p>This was certainly one of the more complex projects I’ve worked on. Many parts of the application needed to be updated, and we introduced a brand new GraphQL API into the architecture.</p><h3>Go GraphQL API</h3><p>It was decided that a GraphQL API would be used to allow the solution to be agile. Usin<strong>g the introspection calls</strong>, we could surface only the fields we wanted to allow users to filter on since some were private fields used by the overall system.</p><p><strong>I built the GraphQL API in Go,</strong> but customized it quite a bit in order for it to match our system. I created a custom CLI that could analyze<strong> the schema of specific tables in SQL Server</strong> to generate the models needed for the GraphQL API. I also used attributes in the GraphQL models to add friendly names and a way to control what was displayed in the UI (more on that in a bit). </p><p>When a dynamic group was saved, the actual GraphQL query was stored alongside the group in a new field in the SQL Server database. This would be used by our scheduler to update the join tables at specific intervals.</p><h3>New CI pipeline</h3><p>This was the first Go project in the environment, and we had discussed utilizing containers more so I opted to make this the <strong>first fully containerized</strong> piece of our architecture. I set up an Ubuntu server running <strong>Docker</strong> and configured both Jenkins and Octopus to utilize it for both builds and artifacts. </p><h3>Hangfire scheduler</h3><p>We used Hangfire to schedule jobs as needed for various reasons. A new job was defined that would perform the following function every 10 minutes:</p><ol><li>Pull the list of dynamic groups, along with the stored query & list of assets.</li><li>Run the stored GraphQL query against the GraphQL API to get a list of assets.</li><li>If the list didn’t match, update the join tables in the database.</li></ol><p>This would ensure that all dynamic groups were current ever 10 minutes at most.</p><h3>Front end views</h3><p>To create a good experience for the user, the existing view to manage groups was updated. A checkbox was added to flag the group as dynamic. If true, the view would display a drag and drop UI that allowed user to specify what fields they wanted to filter on, as well as operators (equals, not equals, greater than, etc.) that were specific to the data type of that field. </p><p>Once the user had crafted their query, the could preview it to ensure that the correct list of assets was returned.</p><h2>The result</h2><p>Once this project was complete, users no longer had to manually keep their groups updated as things changed within their organization. Since groups were utilized heavily in other areas of the application (Ex: Reporting), this dynamism extended to those areas by default as well.</p><p></p><p></p></div>","cachedOn":1689879674},{"id":"cb245da1-fec4-4d96-9a70-6593b9c2d2f0","notion_id":"cb245da1-fec4-4d96-9a70-6593b9c2d2f0","status":"Published","date":"2019-09-01T00:00:00.000Z","relation_job":["b6f6ed05-a879-43b2-b76c-e3f5df82dda4"],"relation_skillsUsed":["17360d1c-70e8-4633-89ec-d4281d96adc2","0c334c2e-6eb4-4ecd-8a83-a67d8ba902a3","33998813-f859-46dd-a7bb-8f53aa662b3f","c51aefc2-10ef-4234-9f3a-a7d1356863fc","869e0ad5-a39a-43b6-85c7-e51a46461961"],"tags":[],"excerpt":"I optimized the load times of an IoT mapping dashboard by creating new API endpoints and client logic that introduced average lat/longs, pagination, and parallel API Calls.","title":"IoT dashboard optimizations","slug":"iot-dashboard-optimizations","html":"<div><p>When I joined Temeda in 2019, the first assignment I received was to optimize the initial load times of the application for a new, very large client. The core entity in the application was the Asset, and this customer had nearly 30,000 assets. These assets are dropped as points on a map using the Google Maps library, along with loading into several data tables. The application was not configured to handle this much data during the initialization process and it caused the application to take nearly 60 seconds to bootstrap. I was tasked with coming up with solutions to resolve this. Here are the steps I took to do so.</p><h2>Split the load process</h2><p>During initialization, the application loaded ALL data for that client before it was responsive. Until this point, it worked well and allowed the app to be snappy since all of the data was cached on the local device, but the application couldn’t load effectively with the amount of data the client had. I theorized that I could split the initialization process into two phases:</p><ol><li>The first phase would be to grab the average lat, long, and asset count for a given map segment.</li><li>The second phase would asynchronously load the remaining asset data until it was fully cached.</li></ol><p></p><h2>Phase 1: Loading average asset data</h2><p>Work on this phase involved making a number of changes to both the AngularJS client and ASP.NET WebAPI backend. In the backend, I created appended one of the several initialization API endpoints to return the total number of assets that user had access to. If it was over a given number, the client would load differently. Instead of making the necessary API calls to return ALL asset data, it instead accessed a new endpoint that would return average data for a given number of map segments. </p><p></p><pre class=\"language-go\"><code>POST /api/clusterpoints\nContent-Type: application/json\n\n{\n\t\"lat1\": 0000,\n\t\"lon1\": 0000,\n\t\"lat2\": 0000,\n\t\"lon2\": 0000,\n\t\"rows\": 4,\n\t\"cols\": 4\n}</code></pre><p></p><p>The request above (albeit faked) would be used to calculate 16 segments (4x4) on the map between the two sets of coordinates given. I would use these to craft a SQL query that would return the ONLY the lat/long between the overall coordinates. Then using the coordinates of each of the 16 map segments, I could sum the total number of assets in a given segment and calculate the average lat/long, returning a total of 16 records in a JSON structure that the client would use to render on a map.</p><div class=\"callout\"><div class=\"callout-icon\">⭐</div><div class=\"callout-content\">Even after the app finished initializing, this same logic would be used when the map was zoomed out beyond a given point to keep the application rendering data quickly.</div></div><p></p><h2>Phase 2: Asynchronously cache data</h2><p>The application would be responsive after the first phase, but since many other components of the application assumes the data is fully loaded before it would properly function, those components had loading indicators added to them while the loading process completed. The init logic was updated to take the total number of assets and load them in batches if 5000 in parallel using JavaScript promises. This also required a new API endpoint to be built that would support pagination to properly request the data from SQL Server.</p></div>","cachedOn":1687890025},{"id":"5feeee36-ceb9-4b86-b2ee-46da78a0e6e3","notion_id":"5feeee36-ceb9-4b86-b2ee-46da78a0e6e3","status":"Not started","date":"2021-06-01T00:00:00.000Z","relation_job":[],"relation_skillsUsed":[],"tags":["Saas","Go"],"excerpt":"","title":"GuardianForge","slug":"guardianforge","html":"<div></div>","cachedOn":1687813125},{"id":"120e2133-e6bd-4338-8a0e-27d63ee8861f","notion_id":"120e2133-e6bd-4338-8a0e-27d63ee8861f","status":"Published","date":"2013-01-01T00:00:00.000Z","relation_job":["1df62840-3545-47c1-98b8-7addf339d2ac"],"relation_skillsUsed":["0c334c2e-6eb4-4ecd-8a83-a67d8ba902a3","0d5db230-18d9-4291-bc62-3f9f0c38210d"],"tags":["ASP.NET MVC","Bootstrap","C#","HTML/CSS","JavaScript","MySQL"],"excerpt":"A simple mobile website that displayed active alerts for the MSP I was working for.","title":"Mobile RMM Dashboard","slug":"mobile-rmm-dashboard","html":"<div><figure><img src=\"/img/n/mobile-rmm-dashboard/168f5c33-a3cb-4d23-9b94-a6fdc7ed041b-image.png\" /><figcaption></figcaption></figure><h2>The Problem</h2><p>The Remote Monitoring & Management (RMM) tool used by the company did not have an easy way of quickly viewing active alerts without launching the Windows desktop application. The tech lead wanted a way to simply view all active alerts, specifically from a mobile device.</p><h2>The Solution</h2><p>I designed a responsive web app using ASP.NET MVC that queried data from the MySQL database of the RMM tool and presented any active alerts on the page. JavaScript was used to keep the views up to date by refreshing the data in the background every 30 seconds. Authentication was handled by Active Directory, and once the credentials were issued, a cookie was used for persistent authentication.</p><h2>The Results</h2><p>Technicians no longer needed to continue to monitor their computers when on-call. It freed them up to spend more time doing the things they enjoyed when responsible for being the initial point of contact during outages or emergencies.</p></div>","cachedOn":1687813128},{"id":"eb681861-3884-4caf-8aed-e415f0a65150","notion_id":"eb681861-3884-4caf-8aed-e415f0a65150","status":"Published","date":"2021-01-01T00:00:00.000Z","relation_job":[],"relation_skillsUsed":["869e0ad5-a39a-43b6-85c7-e51a46461961","28562ded-6a7b-4a6b-9975-c2a2192aeeaf"],"tags":["Bot Development","Docker","NodeJS","Open Source"],"excerpt":"A custom Discord bot built for the fullstack.chat community.","title":"fsc.bot (Walter)","slug":"fscbot-walter","html":"<div><figure><img src=\"/img/n/fscbot-walter/b9d2a497-d377-462c-9eb0-e1d24bf6f69e-image.png\" /><figcaption></figcaption></figure><p>fsc.bot is the open source, custom Discord bot written for the <a href=\"http://fullstack.chat/\" target=\"_blank\">fullstack.chat</a> community.</p><h2>Features</h2><p>Here are some key features of the bot at the time of this writing.</p><h3>XP Tracking</h3><p>I wrote an XP tracking system to automatically determine which users can be considered ‘active’. I wanted to be able to provide perks to users who actively participated in the conversation, one of which is a regular giveaway of some kind.</p><p>Each time a user sends a message to the server, they are granted XP. Continued activity will increase an XP multiplier to help active users gain XP faster. Likewise, inactive users are penalized after several days and XP will slowly start being decremented from their totals. This keeps a balance where active users, regardless of how recently they join, gain the biggest benefits of being on the server.</p><h3>Portfolios</h3><p>The <code>portfolio</code> command allows members to register their portfolio URL with the bot, and the bot will update a single message on the server with all member portfolios. It provides a nice way for everyone to connect with each other and see who works on what.</p><figure><img src=\"/img/n/fscbot-walter/e2d916ea-d3c6-459b-987a-74b4350895f6-image.png\" /><figcaption></figcaption></figure><h3>Help System</h3><p>I also built in a help system. When a user issues the command <code>!fsc help</code>, a private message will be sent to them with details on each command available to them. They can also append the name of the command if they are only interested in help for one command.</p><figure><img src=\"/img/n/fscbot-walter/e862d1df-0779-45de-bb24-a1f7c9d2c4a8-image.png\" /><figcaption></figcaption></figure><h2>Behind the Scenes</h2><p>Below are a few design considerations I took when building the bot.</p><h3>Dynamic Command Parsing</h3><p>When the bot starts, it will scan the <strong>commands</strong> directory and parse in JavaScript files dynamically provided the following fields are exported;</p><ul><li><code>command</code> - The actual command that is registered with the file.</li><li><code>fn</code> - The function that the message object is passed into for processing.</li></ul><p>The following fields are optional, but provide additional functionality;</p><ul><li><code>helpText</code> - The YAML formatted text that is associated with that command. I chose YAML because I could use syntax highlighting when DMing the user.</li><li><code>isEnabled</code> - If set to false, the init process will skip parsing that command file.</li><li><code>shouldCleanup</code> - If set to true, the bot will remove the message that the user sends to trigger the bot. This helps keep the server clean.</li></ul><h3>Security</h3><p>Certain commands should only be issues by moderators, so I implemented a method that checks the requester and confirms they are part of the moderator role before executing the code. If the user is not in the role, they will receive a DM from the bot stating they don't have access to that command.</p><h3>Scheduled Scripts</h3><p>A scheduler system exists within the bot where certain JavaScript functions are executed on a schedule based on cron expressions. One example of this is the XP system.</p><p>Gaining XP is event driven, specifically when the user sends a message to the server. But penalizing users for inactivity cannot be handled in the same way, so there is a script that runs every 24 hours to decrement XP from inactive users.</p><h3>Data Caching</h3><p>During the initialization process for the bot, all data is pulled into memory for quick access. Changes to each dataset are committed back to the database on an individual basis.</p><h2>Infrastructure</h2><p>Code for the bot is hosted on GitHub. The pipeline however managed by Azure DevOps. I inject environment variables & configuration flags into the build process and want to keep those private. The bot is run in a Docker container on my personal server at the moment, and data is stored in an Azure Storage Table.</p><p>Source for the bot can be found at <a href=\"https://github.com/bmorrisondev/fsc.bot\" target=\"_blank\">https://github.com/bmorrisondev/fsc.bot</a>.</p><p></p></div>","cachedOn":1687813527},{"id":"36afe066-973d-4e3e-9c86-2151e8af81e4","notion_id":"36afe066-973d-4e3e-9c86-2151e8af81e4","status":"Published","date":"2011-01-01T00:00:00.000Z","relation_job":["5e703996-2e08-4cfc-a56d-082087007754"],"relation_skillsUsed":["d8812a2a-0657-4830-8001-ac56e87f392b","f3b1c9d3-5957-4a4f-a2f6-4a0013c75e4d"],"tags":["Android SDK","Eclipse","Java"],"excerpt":"An Android application I built and launched into the Play store in order to learn professional development.","title":"Contact Notebooks","slug":"contact-notebooks","html":"<div><figure><img src=\"/img/n/contact-notebooks/70f3cce4-627f-4b80-8473-bd1cddbd3db0-9d0bca83-9b32-417b-b651-cdded5319913.png\" /><figcaption></figcaption></figure><p>Contact Notebooks was my first attempt at creating a mobile app, and using a serious language. It was a fairly simple Android app that hooked into the phones contact database and created a \"notebook\" for each contact. The main view was simply a list of contacts and when the user clicked on one, it would bring them to a sub-view that contained a list of notes. Users were able to add a note for a contact to reference later. Notes were added via a custom dialog view. Once the app was in a functional state, it was published into the Android App Market (now the Google Play Store).</p><p>While the app was nothing spectacular, it was my first attempt building app and bringing a product to market. I essentially decided one day I wanted to build a mobile app since I could see it was definitely the future of computing. Once my normal responsibilities were done for the day, I would go to a local 24 hour chain restaurant and study/code until the early hours of the morning. I would generally start around 10pm and, depending on my schedule the following day, I'd stay as late (?) as 7am if I was on a roll. Since I was running my own business at the time, I had the luxury of creating my own schedule which helped tremendously.</p></div>","cachedOn":1687813131},{"id":"fd4782ee-c24c-4195-b254-32827291bb0d","notion_id":"fd4782ee-c24c-4195-b254-32827291bb0d","status":"Published","date":"2014-01-01T00:00:00.000Z","relation_job":["1df62840-3545-47c1-98b8-7addf339d2ac"],"relation_skillsUsed":["17360d1c-70e8-4633-89ec-d4281d96adc2","ad7b2e6c-c07f-461b-970f-79aed1078b81","66bc0036-1ffb-4d8c-8db2-03b5ccb89bae"],"tags":["C#","WPF/XAML","Zebra Printers","ODBC"],"excerpt":"A label printing application that hooked into Sage100 for a manufacturer in Chicago.","title":"Sage 100 Label Printer","slug":"sage-100-label-printer","html":"<div><p>While working at Systech, we had a client that was mass upgrading their systems from Windows XP to Windows 8. We discovered that at some point many years ago, a custom label printing app was built that integrated into their Sage ERP to allow them to quickly print container labels for orders after they were packed and ready to ship. These labels listed what the contents of the package were and was critical to their operations. The app was built in 16 bit code and would not run on Windows 8, so I was asked to build a new version of the application and replicate the functionality.</p><p>I worked directly with the owner of the company, as well as warehouse personnel, to built a list of features that needed to be implemented in the new version. It was a simple app that allowed a user to click into a terminal field, enter an order number, list the order items, and print a label with those items on it. The app integrated directly into the ERP database (which I believe was Pervasive SQL) to pull the data to display.</p><p>I implemented the same functionality in the upgraded version of the app, but also enhanced it by redesigning the labels to work with industrial Zebra thermal printers, and implemented a number of keybindings as shortcuts to quickly navigate the app. This not only allowed them to continue with their upgrade, but also increased the speed at which they were able to generate labels.</p></div>","cachedOn":1687813131},{"id":"776703c6-aac9-428d-9a56-f6e866c3bc14","notion_id":"776703c6-aac9-428d-9a56-f6e866c3bc14","status":"Published","date":"2014-01-01T00:00:00.000Z","relation_job":["1df62840-3545-47c1-98b8-7addf339d2ac"],"relation_skillsUsed":["17360d1c-70e8-4633-89ec-d4281d96adc2","ad7b2e6c-c07f-461b-970f-79aed1078b81","66bc0036-1ffb-4d8c-8db2-03b5ccb89bae"],"tags":["C#","SQL Server","SSIS","Reporting","WPF/XAML"],"excerpt":"A desktop application built to streamline the quoting process for a roofing company.","title":"Quoting Pro","slug":"quoting-pro","html":"<div><h2>The Problem</h2><p>A roofing company required a very customized quote to be created for their clients. The original quoting process was executed as follows;</p><ul><li>An admin employee would utilize an Access application that was built in-house to lookup customers and create a quote. While the customer was stored in the database, all of the line items on the quote were manually inputted. They would then use an Access Report to print a professional looking quote. They would then scan the quote to their email in PDF format.</li><li>The employee would generate a specifications document using a PDF form and export that to be part of the overall quote.</li><li>Any number of CAD drawings and text files provided by the engineers would then need to be combined along with the line item quote and specs document into a single PDF which would be the final quote.</li><li>Finally, this quote would be sent to the client via email.</li></ul><p>This overall process was very manual and time consuming. Much of this process was automated in a series of phases detailed below.</p><h2>The Solution</h2><h3>Phase 1</h3><p>The first thing we did is prevent the user from having to print and combine the PDFs. We setup the Access Report with a small amount of VBA to automate the Print to PDF process. An Accessory Command Line App was built using an open source PDF library to combine the PDFs. The VBA would call the accessory app, which would prompt the user to select any number of files to combine. Once the files were combined, the would be attached to an email template through Outlook. The specs document was still required to be completed manually at this point.</p><p>The result of Phase 1 was a dramatic decrease of time spent in the quoting process. One issue that remained however is that the Access App was not originally designed to be used by multiple users. An attempt was made to break out the backend database, but it was unsuccessful.</p><h3>Phase 2</h3><p>The next phase of this project involved a custom designed WPF application that was tailored to their specific quoting process. SQL Server was used as the back end datastore. In order to preserve historical data, I created an SSIS package which imported data from the existing Access database and mapped it into a table structure I designed for the application.</p><p>Custom modules were created to track customers, contacts, product info, quotes, and spec documents. During the quoting process, the application would prompt the user to import any number of CAD drawings or text files submitted by the engineers. The application would use the open source PDF library to combine all documents into a single PDF and attach it to an email.</p><p>I also designed a process by which the user could export a list of products from their primary Line of Business app into an Excel documents, and import it into the Quoting App so as to prevent manual entry. (There was no public API for the LOB App.)</p><p>The result of Phase 2 was a another dramatic increase in productivity as the application was more streamlined and more intuitively designed. I also designed it with multiple users in mind, which eliminated the restriction that the app could only by used by one person at a time.</p></div>","cachedOn":1687813141},{"id":"5ca1e211-c17a-4878-9af2-b0fd3a645e13","notion_id":"5ca1e211-c17a-4878-9af2-b0fd3a645e13","status":"Published","date":"2013-06-01T00:00:00.000Z","relation_job":["1df62840-3545-47c1-98b8-7addf339d2ac"],"relation_skillsUsed":["e3f8fb06-f286-458a-a157-a7f620d0d5fe","17360d1c-70e8-4633-89ec-d4281d96adc2"],"tags":["C#","JavaScript","SharePoint SDK"],"excerpt":"A SharePoint extension that saved over 100k of billable work hours for an MSP I was employed with.","title":"SharePoint Backup Tracker","slug":"sharepoint-backup-tracker","html":"<div><figure><img src=\"/img/n/sharepoint-backup-tracker/7ef1daeb-568d-48ab-a35e-a17596dfd844-14895539-3a65-48c4-b205-da438cc5eb68.png\" /><figcaption></figcaption></figure><h2>The Problem</h2><p>I built this while working for a Managed Services Provider. As part of our contractual agreement with our customers, we checked all customers' backups daily to ensure they were being successfully completed. Every day, an administrator would spend upwards of 3 hours manually inputting a list of backup sets that needed to be reviewed by a technician. Once the list was populated, the admin would notify the designated technician that the list was ready to be reviewed. The tech would then manually review the backups for all customers using a number of methods including; remoting directly to a server, checking email reports, or checking a cloud based panel.</p><p>Each backup to be checked was a SharePoint list item. This required the technician to open the item, change the necessary attributes, and save the item to return to the list. Any backup that had failed would require the tech to manually create a ticket in the ticketing system and add the ticket number as a reference to the backup item in SharePoint.</p><p>This entire process took a minimum of 8 hours of labor due to the volume of backups that needed to be tracked and the speed at which the process flowed.</p><h2>The Solution</h2><p>There were several improvements put in place to solve this problem.</p><h3>The Template List</h3><p>The first thing that was change is a new list that defined the following attributes;</p><ul><li>The customer name</li><li>The backup name</li><li>The set number (used to assign a set to a technician)</li><li>The method required for checking the backup</li><li>The days which the backup needed to be checked (as checkboxes)</li></ul><p>This list served as the template for backups to be checked and allowed us to create a programmatic way to populate the daily list of backups to check.</p><h3>The Daily List</h3><p>This list was used by the technicians to know which backups needed to be checked, and which were assigned to them. Every day, the techs would log into the SharePoint site and complete the list that was assigned to them. Each item would be populated as follows;</p><ul><li>The customer name</li><li>The backup name</li><li>The set number</li><li>The methods required for checking the backup</li><li>The status of the backup (success/failure)</li><li>Notes that pertain to the failure status</li></ul><h3>The Technician List</h3><p>A third list was created with the tech name and the backup set assigned as referenced in the Daily List. This list was used as a reference so that each tech would know which set to check for that day.</p><h3>Timer Jobs</h3><p>To automate the process of creating the daily list of backups to check, I created a SharePoint Timer Job that ran every day at 3am and referenced items from the template list to populate the daily list. This completely automate the manual entry required in the original process and free up the time of the admin.</p><p>Per the requirements of the lead tech, each tech needed to be assigned a different set on a daily basis. A second Timer Job was created for the purpose of randomizing the sets assigned based on the number of technicians in the Technician List.</p><h3>JavaScript Multiple Close Button</h3><p>To resolve the issue with closing multiple Daily List items simultaneously, I created a custom JavaScript button that would update the Status field in any number of selected backup items. This button was added to the ribbon of SharePoint.</p><h2>The Results</h2><p>At this point, a process that generally took a minimum of 8 hours between the admin and a single tech was distributed among 4-5 technicians taking a total of 30 minutes of total time. This ultimately saved the company over $100,000 of billable hours that the tech could use to resolve customer issues as opposed to handling this administrative task.</p><ul><li>* <em>This project was ultimately rolled into a multi-tiered custom application we named Unify.</em></li></div>","cachedOn":1687813142},{"id":"419a33e0-98ed-44b3-a817-62ab52e5fcdb","notion_id":"419a33e0-98ed-44b3-a817-62ab52e5fcdb","status":"Published","date":"2021-01-01T00:00:00.000Z","relation_job":[],"relation_skillsUsed":["1be18392-7698-48dd-a162-fe2a1cfc349b","d8b6df75-9b91-4d51-b97b-44c2c530071d"],"tags":["Gridsome","Netlify","Open Source","VueJS"],"excerpt":"A custom website built for my Discord server.","title":"fullstack.chat website","slug":"fullstackchat-website","html":"<div><p></p><figure><img src=\"/img/n/fullstackchat-website/11e53a72-fd65-4f57-b3dc-144deb62fb4e-812dca98-8ee2-42fe-854d-a525166e5102.png\" /><figcaption></figcaption></figure><h2>Design</h2><p>As most sites, I started with a basic color scheme using coolors.co. Below are the colors that I settled on with input from the rest of the community.</p><p>I also utilized <a href=\"http://fontpairs.co/\" target=\"_blank\">fontpairs.co</a> to select a set of fonts that were tested to work well together. Lato is used for the headers and Open Sans for body type.</p><h2>Implementation</h2><p>The site is built with Gridsome, a static site generator using VueJS. Much of the code on the landing page is hand written, but the Rules section references a markdown file that is stored along with the project. This makes for quick updating of the rules without modifying the code.</p><p>Bootstrap was used to quickly get some responsiveness, but no other components from that framework were used.</p><p>The site is hosted on Netlify and their automated deployment tools are leveraged by connecting the site to the GitHub repository.</p><p>The code for the site is open source and can be found at <a href=\"https://github.com/bmorrisondev/fullstack.chat\" target=\"_blank\">https://github.com/bmorrisondev/fullstack.chat</a>.</p></div>","cachedOn":1687813528},{"id":"6dc2d67c-0bf5-4b7d-aa19-80bafa7599fe","notion_id":"6dc2d67c-0bf5-4b7d-aa19-80bafa7599fe","status":"Published","date":"2014-01-01T00:00:00.000Z","relation_job":["1df62840-3545-47c1-98b8-7addf339d2ac"],"relation_skillsUsed":["ad7b2e6c-c07f-461b-970f-79aed1078b81","f9d966c2-e161-4e01-93c4-89bdff554b14","17360d1c-70e8-4633-89ec-d4281d96adc2","992ebb81-581a-43b1-9359-446c50db9ae5"],"tags":["C#","SQL Server","Reporting","WCF","WPF/XAML","XML API"],"excerpt":"The internal tool designed to consolidate many disparate functions into a single application for an MSP I worked for.","title":"Project Unify","slug":"project-unify","html":"<div><h2>The Problem</h2><p>Unify was built to become a platform to bridge multiple, manual processes into a single point of access to manage those processes in an automated fashion. That said, this project is broken into multiple sections. This section documents the general design of the overall solution.</p><h2>Front End Application</h2><p>The front end of Unify was a Windows WPF application that utilized the Telerik WPF controls. The overall feel of the desktop application was designed to mimic other Microsoft Offices with a contextually aware ribbon and an Outlook-style navigation on the left. I used ClickOnce for distributing any updates to the desktop application, and the installed client would check for updates upon launch.</p><h2>Reporting</h2><p>Report templates were stored in the Front End Application. This allowed the user to batch export reports per customer to a location on their computer. They would select the reports they would want to export, select a folder on their computer, and the application would export the requested reports to PDFs. These reports were previously built manually using Excel spreadsheets with connections to various datastores.</p><h2>Back End Service</h2><p>I created a WCF Service that contained all of the business logic of Unify. This included executing certain actions (specifically for the accessory applications specified below) and any CRUD operations. I designed and built out a SQL Server database to store any structured data. Unstructured data such as reports and firewall backups were stored on the file system of the server with a direct link stored in the database for quick reference. The WCF service was presented to the client using IIS. Updated were performed by a simple Deploy to a network path that was mapped through IIS.</p><h2>Accessory Applications</h2><p>While the business logic was stored in the WCF Service, I created several Windows Command Line Applications that could be used to execute those tasks. These were used in conjunction with the Windows Task Scheduler to execute any operations required on a regular basis.</p><h2>Authentication</h2><p>Authentication was tied into Active Directory via the WCF service. Usernames & passwords were hashed and sent to the WCF service using HTTPS where they were validated against Active Directory. The front end also checked the currently logged on Windows user and if it were a member of the domain, it would authenticate automatically.</p><h2>Backup Tracker</h2><p>The first problem that Unify solved was to further streamline the Backup Tracking process described in <a href=\"notion://www.notion.so/portfolio/sharepoint-backup-tracker\" target=\"_blank\">SharePoint Backup Tracker</a>. Additional functionality was included to further streamline the process of quarterly customer reporting.</p><p>One improvement gained by implementing a custom application was a significant performance gain. The responsiveness of a native application exceeded accessing SharePoint through a web app. This allowed the technicians to complete their backup review in under 15 minutes under most circumstances.</p><p>A second improvement that was made was deeper integration into our ticketing system, ConnectWise. I was able to directly access a list of customers to reference within Unify, and this was used to automatically create tickets based on the reported status of a backup. Upon creating a ticket using ConnectWise’s XML API, the API would respond with the generated ticket number. That was stored along with the backup for reporting purposes. Further integration as it pertains to the firewall backups is detailed below.</p><h2>Firewall Backup Automation</h2><p>The second problem was the automation of firewall backups. In order to preserve the configuration of our managed customers’ Fortinet firewalls, technicians were required to manually offload monthly backups. This process took between 4-5 hours to perform on a monthly basis.</p><p>Using the Renci.SSH library, logic was built into the Unify Service to send commands to the firewalls of our clients and issue commands. The firewalls had a command built in to send a configuration backup to an FTP server. We setup a dedicated FTP server for these backups and used these commands to execute weekly backups of these firewalls. The success or failures of the backup process, along with a direct link to the backup for quick access, was stored in the database. This was used for quarterly reporting between the client executives and the customer.</p><p>Considering the access URL and credentials were stored in the CMDB of ConnectWise, and the onboarding process had a task to add this information, it was decided that this would be the best source of truth for creating the firewall backups. We used the ConnectWise API to pull this data in at the start of the routine. This allowed integration of the firewall backups procedure into the existing business processes, reducing the possibility of error or oversight.</p><h2>Warranty Reporting</h2><p>The final component that was completed for Unify was the process of automatically storing warranty information on all endpoints covered under contract. As part of the onboarding process, a Remote Monitoring & Management (RMM) tool was deployed to endpoints. This allowed for remote support and management, but also gathered data regarding the system, notably the system serial number.</p><p>I utilized these values stored in the MySQL database of the RMM server, in conjunction with a third party warranty lookup service, to parse the warranty info of all 1000+ managed endpoints into Unify for reporting and tracking. A custom report was created to allow client executives to export reports for the quarterly review and plan with the customer to ensure their hardware was under warranty and compliance.</p></div>","cachedOn":1687813145},{"id":"89791dee-d1aa-4683-a66e-a6abeb2cbfbe","notion_id":"89791dee-d1aa-4683-a66e-a6abeb2cbfbe","status":"Published","date":"2020-09-01T00:00:00.000Z","relation_job":[],"relation_skillsUsed":["172d0de7-9af3-4f87-b016-ccb75c1e1999","845fab36-9ea1-4487-82bb-7f6fd0c02598","5b643036-71e1-4706-b8a5-d92d7079aa95","b8037eac-3266-477c-8fd7-fa76ac45aace"],"tags":["After Effects","Premier Pro","Python","YouTube"],"excerpt":"A YouTube video I created for the team at MessengerX.io.","title":"MessengerX.io Python Bot YouTube Video","slug":"messengerxio-python-bot-youtube-video","html":"<div><figure><img src=\"/img/n/messengerxio-python-bot-youtube-video/4f3f329c-4447-47ea-897a-860c09fbe2c1-img_62b5f592f0dda.jpg\" /><figcaption></figcaption></figure><div class=\"callout\"><div class=\"callout-icon\">📽️</div><div class=\"callout-content\">You can view the video here: <a href=\"https://www.youtube.com/watch?v=nIp7QRNci2U\" target=\"_blank\">https://www.youtube.com/watch?v=nIp7QRNci2U</a></div></div><p><a href=\"http://messengerx.io/\" target=\"_blank\">MessengerX.io</a> is a developer marketplace for everyday chat apps also known as chatbots.</p><p>When I was in the process of releasing my <a href=\"https://www.youtube.com/playlist?list=PLwpjN-4DtVRbaxSY_dM0Ag0Y5SG6-RLu-\" target=\"_blank\">Coding Discord Bots</a> series on YouTube, I was advertising on a NodeJS Facebook group. The owner of the <a href=\"http://messengerx.io/\" target=\"_blank\">MessengerX.io</a> reached out to me and expressed interest in producing a video for their YouTube channel.</p><p>He sent me an existing article that someone on their internal team created outlining how to create a simple python bot using RASA, an open source ML framework. This was the foundation for the video I produced. I worked with their internal team over the course of a few weeks to better understand their platform, and to make revisions to the content as needed.</p><p>The video was recorded using Streamlabs OBS and edited using Adobe Premiere Pro. The intro animation was created using Adobe After Effects, and Photoshop was used to create the outro card. I leveraged Envato Elements for the music used in the intro & outro.</p><p><a href=\"http://messengerx.io/\" target=\"_blank\">MessengerX.io</a> is a developer marketplace for everyday chat apps also known as chatbots.</p><p>When I was in the process of releasing my <a href=\"https://www.youtube.com/playlist?list=PLwpjN-4DtVRbaxSY_dM0Ag0Y5SG6-RLu-\" target=\"_blank\">Coding Discord Bots</a> series on YouTube, I was advertising on a NodeJS Facebook group. The owner of the <a href=\"http://messengerx.io/\" target=\"_blank\">MessengerX.io</a> reached out to me and expressed interest in producing a video for their YouTube channel.</p><p>He sent me an existing article that someone on their internal team created outlining how to create a simple python bot using RASA, an open source ML framework. This was the foundation for the video I produced. I worked with their internal team over the course of a few weeks to better understand their platform, and to make revisions to the content as needed.</p><p>The video was recorded using Streamlabs OBS and edited using Adobe Premiere Pro. The intro animation was created using Adobe After Effects, and Photoshop was used to create the outro card. I leveraged Envato Elements for the music used in the intro & outro.</p><p></p></div>","cachedOn":1687813529},{"id":"b9ec4188-4abf-49b9-9a51-ea7ceaa442c6","notion_id":"b9ec4188-4abf-49b9-9a51-ea7ceaa442c6","status":"Published","date":"2010-01-01T00:00:00.000Z","relation_job":["5e703996-2e08-4cfc-a56d-082087007754"],"relation_skillsUsed":["ad189126-9b9f-4260-ad1b-8c3c35399079"],"tags":["MS Access","VBA"],"excerpt":"A Microsoft Access application that streamlined the yearly renewals for a landscaping client of mine.","title":"Proposals App","slug":"proposals-app","html":"<div><h2>The Problem</h2><p>A client of mine in the landscaping industry had a manual process defined to send out maintenance contract renewals every year. There were over 1000 customers that required the office employees to manually reference last year’s contracts and write out a proposal to mail to a customer. There were three possible forms that needed to be filled out for each of their customers, with certain customers requiring multiple forms to be completed. My client researched multiple canned solutions, but nothing existed that could be integrated into their workflow at the time.</p><h2>The Solution</h2><p>I worked with the client to build an application using Microsoft Access that automated much of this process. The three forms required for their proposals were digitized into Access Forms. The employees could fill out the form, save the data, and use a custom report that mirrored the previous physical form to perform one of the following operations;</p><ul><li>Print the document to mail if required. Each user is also able to customize the printer they wanted to send to.</li><li>Generate a PDF and attach to an Outlook Email using VBA integration. An email template was also configured to pre-populate the recipient and a standard message into the body. The message was then presented to the user to add or remove detail as necessary.</li></ul><p>The first year this was implemented required the employees to manually enter the information referenced from the previous year’s hand written proposals. Copy and Archive were added so that creating new proposals that referenced the previous year required a single button click, and historical data was kept for reference.</p><p>The Access Application was broken into a front end and back end so multiple users could use it at the same time.</p><h2>The Result</h2><p>We were able to take a process that originally required several weeks of manual work and reduce it to several days.</p></div>","cachedOn":1687890028}]